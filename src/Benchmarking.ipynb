{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 23:54:47.056173: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 23:54:47.320984: I tensorflow/core/platform/cpu_feature_guard.cc:211] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# How to create the docker run directly in Lambda2\n",
    "# docker run --gpus all -it --mount type=bind,source=/home/jacob-delgado/Documents/CAPSTONE,target=/workspace/CAPSTONE nvcr.io/nvidia/tensorflow:24.10-tf2-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 0)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "print(trt.trt_utils._pywrap_py_utils.get_linked_tensorrt_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 228 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_root = pathlib.Path(\"../data/Monkeypox_Data/Original_Images\")    # points to the folder containing the images that will be used for training\n",
    "saved_model_dir = '../best_model/model1/best_f1score_fold'\n",
    "optimized_model_dir = '../tensorRT_model/test'\n",
    "# optimized_model_dir = '../tensorRT_model/test_INT8'\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32         # size of the batch that will be fed to model\n",
    "img_height = 224        # input image height\n",
    "img_width = 224         # input image width\n",
    "test_size = 0.2\n",
    "\n",
    "# Load dataset without splitting\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_root,                                  # loads images from the data_root directory\n",
    "    image_size=(img_height, img_width),         # resizes all images to (224, 224) pixels\n",
    "    batch_size=batch_size,                      # set the batch size\n",
    "    shuffle=True                                # shufle data when loaded\n",
    ")\n",
    "\n",
    "# normalization_layer = layers.Rescaling(1./255)\n",
    "# dataset = dataset.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Metrics Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 23:45:54.492110: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 228 \n",
      "Total Labels: 228\n"
     ]
    }
   ],
   "source": [
    "image_batches, labels = [], []\n",
    "for image_batch, label_batch in dataset:\n",
    "    image_batches.append(image_batch)\n",
    "    labels.append(label_batch)\n",
    "\n",
    "image_batches = np.concatenate(image_batches) # Flatten batches to get all images\n",
    "labels = np.concatenate(labels)               # Flatten batches to get all labels  \n",
    "print(f\"Total Images: {image_batches.shape[0]} \\nTotal Labels: {labels.shape[0]}\")\n",
    "\n",
    "# Split the data into test subset for benchmarking\n",
    "_, X_test, _, Y_test = train_test_split(image_batches, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "\n",
    "X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Size Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model_dir):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(model_dir):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# original_size = get_model_size(saved_model_dir)\n",
    "# optimized_size = get_model_size(optimized_model_dir)\n",
    "# print(f\"Original Model Size: {original_size:.2f} MB\")\n",
    "# print(f\"Optimized Model Size: {optimized_size:.2f} MB\")\n",
    "# print(f\"Compression Ratio: {original_size / optimized_size:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on original model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 500 inference trials on 46 test images...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Binding inputs to tf.function failed due to `too many positional arguments`. Received args: (array([[[[0.68235296, 0.53333336, 0.49019608],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.85490197, 0.72156864, 0.68235296],\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ]],\n\n        [[0.68235296, 0.53333336, 0.49019608],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ]],\n\n        [[0.6862745 , 0.5372549 , 0.49411765],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.8627451 , 0.7294118 , 0.6901961 ],\n         [0.8627451 , 0.7294118 , 0.6901961 ],\n         [0.8627451 , 0.7294118 , 0.6901961 ]],\n\n        ...,\n\n        [[0.6666667 , 0.5019608 , 0.41568628],\n         [0.67058825, 0.5058824 , 0.41960785],\n         [0.67058825, 0.5058824 , 0.41960785],\n         ...,\n         [0.8156863 , 0.627451  , 0.54901963],\n         [0.827451  , 0.627451  , 0.5529412 ],\n         [0.827451  , 0.627451  , 0.5529412 ]],\n\n        [[0.6745098 , 0.50980395, 0.42352942],\n         [0.6745098 , 0.50980395, 0.42352942],\n         [0.6745098 , 0.50980395, 0.42352942],\n         ...,\n         [0.8117647 , 0.62352943, 0.54509807],\n         [0.81960785, 0.61960787, 0.54509807],\n         [0.8235294 , 0.62352943, 0.54901963]],\n\n        [[0.6784314 , 0.5137255 , 0.42745098],\n         [0.6784314 , 0.5137255 , 0.42745098],\n         [0.6745098 , 0.50980395, 0.42352942],\n         ...,\n         [0.80784315, 0.61960787, 0.5411765 ],\n         [0.81960785, 0.61960787, 0.54509807],\n         [0.81960785, 0.61960787, 0.54509807]]],\n\n\n       [[[0.7176471 , 0.4509804 , 0.3764706 ],\n         [0.7176471 , 0.4509804 , 0.3764706 ],\n         [0.72156864, 0.45490196, 0.38039216],\n         ...,\n         [0.9764706 , 0.77254903, 0.7294118 ],\n         [0.9647059 , 0.7607843 , 0.7176471 ],\n         [0.9490196 , 0.74509805, 0.7019608 ]],\n\n        [[0.7137255 , 0.44705883, 0.37254903],\n         [0.7137255 , 0.44705883, 0.37254903],\n         [0.7137255 , 0.44705883, 0.37254903],\n         ...,\n         [0.972549  , 0.76862746, 0.7254902 ],\n         [0.9607843 , 0.75686276, 0.7137255 ],\n         [0.9490196 , 0.74509805, 0.7019608 ]],\n\n        [[0.7137255 , 0.44705883, 0.3647059 ],\n         [0.7137255 , 0.44705883, 0.3647059 ],\n         [0.70980394, 0.44313726, 0.36078432],\n         ...,\n         [0.9607843 , 0.75686276, 0.7137255 ],\n         [0.9529412 , 0.7490196 , 0.7058824 ],\n         [0.94509804, 0.7411765 , 0.69803923]],\n\n        ...,\n\n        [[0.8235294 , 0.5411765 , 0.44705883],\n         [0.827451  , 0.54509807, 0.4509804 ],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [0.98039216, 0.87058824, 0.85882354],\n         [0.98039216, 0.87058824, 0.85882354],\n         [0.9764706 , 0.8666667 , 0.85490197]],\n\n        [[0.8392157 , 0.5568628 , 0.4627451 ],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [0.99607843, 0.8862745 , 0.8745098 ],\n         [0.99607843, 0.8862745 , 0.8745098 ],\n         [0.9882353 , 0.8784314 , 0.8666667 ]],\n\n        [[0.85490197, 0.57254905, 0.47843137],\n         [0.84313726, 0.56078434, 0.46666667],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [1.        , 0.8980392 , 0.8862745 ],\n         [1.        , 0.89411765, 0.88235295],\n         [0.99607843, 0.8862745 , 0.8745098 ]]],\n\n\n       [[[0.77254903, 0.6509804 , 0.6313726 ],\n         [0.78039217, 0.65882355, 0.6392157 ],\n         [0.8039216 , 0.6745098 , 0.654902  ],\n         ...,\n         [0.8666667 , 0.74509805, 0.6313726 ],\n         [0.8745098 , 0.7529412 , 0.6392157 ],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        [[0.77254903, 0.6431373 , 0.62352943],\n         [0.7921569 , 0.6627451 , 0.6431373 ],\n         [0.8117647 , 0.68235296, 0.6627451 ],\n         ...,\n         [0.8666667 , 0.74509805, 0.6313726 ],\n         [0.8745098 , 0.7529412 , 0.6392157 ],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        [[0.8039216 , 0.6666667 , 0.65882355],\n         [0.827451  , 0.6901961 , 0.68235296],\n         [0.85882354, 0.70980394, 0.7058824 ],\n         ...,\n         [0.8627451 , 0.7411765 , 0.627451  ],\n         [0.87058824, 0.7490196 , 0.63529414],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        ...,\n\n        [[0.9529412 , 0.88235295, 0.84313726],\n         [0.95686275, 0.8862745 , 0.84705883],\n         [0.94509804, 0.8745098 , 0.8352941 ],\n         ...,\n         [0.6901961 , 0.50980395, 0.3764706 ],\n         [0.70980394, 0.5294118 , 0.3882353 ],\n         [0.7254902 , 0.54509807, 0.40392157]],\n\n        [[0.95686275, 0.8784314 , 0.84313726],\n         [0.95686275, 0.8784314 , 0.84313726],\n         [0.94509804, 0.8666667 , 0.83137256],\n         ...,\n         [0.69411767, 0.5137255 , 0.38039216],\n         [0.70980394, 0.5294118 , 0.3882353 ],\n         [0.7176471 , 0.5372549 , 0.39607844]],\n\n        [[0.95686275, 0.8784314 , 0.84313726],\n         [0.95686275, 0.8784314 , 0.84313726],\n         [0.94509804, 0.8666667 , 0.83137256],\n         ...,\n         [0.69411767, 0.5137255 , 0.38039216],\n         [0.7058824 , 0.5254902 , 0.38431373],\n         [0.7137255 , 0.53333336, 0.39215687]]],\n\n\n       ...,\n\n\n       [[[0.5882353 , 0.5372549 , 0.5137255 ],\n         [0.5882353 , 0.5372549 , 0.5137255 ],\n         [0.5882353 , 0.5372549 , 0.5137255 ],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        [[0.58431375, 0.53333336, 0.50980395],\n         [0.58431375, 0.53333336, 0.50980395],\n         [0.58431375, 0.53333336, 0.50980395],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        [[0.5803922 , 0.52156866, 0.5019608 ],\n         [0.58431375, 0.5254902 , 0.5058824 ],\n         [0.5803922 , 0.5294118 , 0.5058824 ],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        ...,\n\n        [[0.47058824, 0.40392157, 0.3647059 ],\n         [0.47058824, 0.40784314, 0.35686275],\n         [0.47058824, 0.40392157, 0.3647059 ],\n         ...,\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667]],\n\n        [[0.4627451 , 0.4       , 0.34901962],\n         [0.46666667, 0.40392157, 0.34509805],\n         [0.47058824, 0.40784314, 0.35686275],\n         ...,\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667]],\n\n        [[0.45882353, 0.39607844, 0.3372549 ],\n         [0.4627451 , 0.4       , 0.34117648],\n         [0.47058824, 0.40784314, 0.34901962],\n         ...,\n         [0.54509807, 0.5137255 , 0.47058824],\n         [0.54509807, 0.5137255 , 0.47058824],\n         [0.54509807, 0.5137255 , 0.47058824]]],\n\n\n       [[[0.49411765, 0.36078432, 0.22352941],\n         [0.5019608 , 0.36862746, 0.23137255],\n         [0.5058824 , 0.37254903, 0.23529412],\n         ...,\n         [0.47058824, 0.35686275, 0.22352941],\n         [0.46666667, 0.3529412 , 0.22745098],\n         [0.5647059 , 0.4627451 , 0.33333334]],\n\n        [[0.49019608, 0.35686275, 0.21960784],\n         [0.49803922, 0.3647059 , 0.22745098],\n         [0.5019608 , 0.36862746, 0.23137255],\n         ...,\n         [0.46666667, 0.3529412 , 0.21960784],\n         [0.4627451 , 0.34901962, 0.22352941],\n         [0.56078434, 0.45882353, 0.32941177]],\n\n        [[0.4862745 , 0.3529412 , 0.21568628],\n         [0.49019608, 0.35686275, 0.21960784],\n         [0.49803922, 0.3647059 , 0.22745098],\n         ...,\n         [0.4627451 , 0.34901962, 0.21568628],\n         [0.45882353, 0.34509805, 0.21960784],\n         [0.5568628 , 0.45490196, 0.3254902 ]],\n\n        ...,\n\n        [[0.6117647 , 0.4627451 , 0.30980393],\n         [0.6117647 , 0.4627451 , 0.30980393],\n         [0.6156863 , 0.46666667, 0.3137255 ],\n         ...,\n         [0.5921569 , 0.4745098 , 0.33333334],\n         [0.60784316, 0.49411765, 0.36862746],\n         [0.68235296, 0.5686275 , 0.44313726]],\n\n        [[0.6039216 , 0.45490196, 0.3019608 ],\n         [0.6039216 , 0.45490196, 0.3019608 ],\n         [0.60784316, 0.45882353, 0.30588236],\n         ...,\n         [0.5882353 , 0.47058824, 0.32941177],\n         [0.6039216 , 0.49019608, 0.3647059 ],\n         [0.6784314 , 0.5647059 , 0.4392157 ]],\n\n        [[0.5921569 , 0.44313726, 0.2901961 ],\n         [0.59607846, 0.44705883, 0.29411766],\n         [0.6039216 , 0.45490196, 0.3019608 ],\n         ...,\n         [0.5882353 , 0.47058824, 0.32941177],\n         [0.6       , 0.4862745 , 0.36078432],\n         [0.6745098 , 0.56078434, 0.43529412]]],\n\n\n       [[[0.8117647 , 0.5294118 , 0.5764706 ],\n         [0.827451  , 0.54509807, 0.5921569 ],\n         [0.8509804 , 0.5686275 , 0.6156863 ],\n         ...,\n         [0.28235295, 0.35686275, 0.38431373],\n         [0.27450982, 0.3647059 , 0.39607844],\n         [0.26666668, 0.3647059 , 0.39215687]],\n\n        [[0.84313726, 0.56078434, 0.60784316],\n         [0.84705883, 0.5647059 , 0.6117647 ],\n         [0.85490197, 0.57254905, 0.61960787],\n         ...,\n         [0.27058825, 0.3372549 , 0.36862746],\n         [0.2627451 , 0.34117648, 0.3764706 ],\n         [0.24705882, 0.3372549 , 0.36862746]],\n\n        [[0.8666667 , 0.5921569 , 0.63529414],\n         [0.8627451 , 0.5882353 , 0.6313726 ],\n         [0.8627451 , 0.5803922 , 0.627451  ],\n         ...,\n         [0.3019608 , 0.34117648, 0.3764706 ],\n         [0.2784314 , 0.33333334, 0.36862746],\n         [0.2627451 , 0.32941177, 0.36078432]],\n\n        ...,\n\n        [[0.87058824, 0.654902  , 0.6431373 ],\n         [0.87058824, 0.654902  , 0.6431373 ],\n         [0.8745098 , 0.65882355, 0.64705884],\n         ...,\n         [0.90588236, 0.627451  , 0.61960787],\n         [0.90588236, 0.6156863 , 0.6117647 ],\n         [0.9098039 , 0.6117647 , 0.6117647 ]],\n\n        [[0.8745098 , 0.64705884, 0.6392157 ],\n         [0.88235295, 0.654902  , 0.64705884],\n         [0.8901961 , 0.6627451 , 0.654902  ],\n         ...,\n         [0.9098039 , 0.6313726 , 0.62352943],\n         [0.8980392 , 0.60784316, 0.6039216 ],\n         [0.89411765, 0.6039216 , 0.6       ]],\n\n        [[0.8745098 , 0.64705884, 0.6392157 ],\n         [0.8862745 , 0.65882355, 0.6509804 ],\n         [0.8980392 , 0.67058825, 0.6627451 ],\n         ...,\n         [0.90588236, 0.6313726 , 0.62352943],\n         [0.89411765, 0.6039216 , 0.6       ],\n         [0.8862745 , 0.59607846, 0.5921569 ]]]], dtype=float32),) and kwargs: {} for signature: (*, keras_tensor_312: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_312')) -> Dict[['output_0', TensorSpec(shape=(None, 1), dtype=tf.float32, name='output_0')]].\nFallback to flat signature also failed due to: signature_wrapper___call__(keras_tensor_312): expected argument #0(zero-based) to be a Tensor; got ndarray ([[[[0.68235296 0.53333336 0.49019608]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.85490197 0.72156864 0.68235296]\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]]\n\n  [[0.68235296 0.53333336 0.49019608]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]]\n\n  [[0.6862745  0.5372549  0.49411765]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.8627451  0.7294118  0.6901961 ]\n   [0.8627451  0.7294118  0.6901961 ]\n   [0.8627451  0.7294118  0.6901961 ]]\n\n  ...\n\n  [[0.6666667  0.5019608  0.41568628]\n   [0.67058825 0.5058824  0.41960785]\n   [0.67058825 0.5058824  0.41960785]\n   ...\n   [0.8156863  0.627451   0.54901963]\n   [0.827451   0.627451   0.5529412 ]\n   [0.827451   0.627451   0.5529412 ]]\n\n  [[0.6745098  0.50980395 0.42352942]\n   [0.6745098  0.50980395 0.42352942]\n   [0.6745098  0.50980395 0.42352942]\n   ...\n   [0.8117647  0.62352943 0.54509807]\n   [0.81960785 0.61960787 0.54509807]\n   [0.8235294  0.62352943 0.54901963]]\n\n  [[0.6784314  0.5137255  0.42745098]\n   [0.6784314  0.5137255  0.42745098]\n   [0.6745098  0.50980395 0.42352942]\n   ...\n   [0.80784315 0.61960787 0.5411765 ]\n   [0.81960785 0.61960787 0.54509807]\n   [0.81960785 0.61960787 0.54509807]]]\n\n\n [[[0.7176471  0.4509804  0.3764706 ]\n   [0.7176471  0.4509804  0.3764706 ]\n   [0.72156864 0.45490196 0.38039216]\n   ...\n   [0.9764706  0.77254903 0.7294118 ]\n   [0.9647059  0.7607843  0.7176471 ]\n   [0.9490196  0.74509805 0.7019608 ]]\n\n  [[0.7137255  0.44705883 0.37254903]\n   [0.7137255  0.44705883 0.37254903]\n   [0.7137255  0.44705883 0.37254903]\n   ...\n   [0.972549   0.76862746 0.7254902 ]\n   [0.9607843  0.75686276 0.7137255 ]\n   [0.9490196  0.74509805 0.7019608 ]]\n\n  [[0.7137255  0.44705883 0.3647059 ]\n   [0.7137255  0.44705883 0.3647059 ]\n   [0.70980394 0.44313726 0.36078432]\n   ...\n   [0.9607843  0.75686276 0.7137255 ]\n   [0.9529412  0.7490196  0.7058824 ]\n   [0.94509804 0.7411765  0.69803923]]\n\n  ...\n\n  [[0.8235294  0.5411765  0.44705883]\n   [0.827451   0.54509807 0.4509804 ]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [0.98039216 0.87058824 0.85882354]\n   [0.98039216 0.87058824 0.85882354]\n   [0.9764706  0.8666667  0.85490197]]\n\n  [[0.8392157  0.5568628  0.4627451 ]\n   [0.8352941  0.5529412  0.45882353]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [0.99607843 0.8862745  0.8745098 ]\n   [0.99607843 0.8862745  0.8745098 ]\n   [0.9882353  0.8784314  0.8666667 ]]\n\n  [[0.85490197 0.57254905 0.47843137]\n   [0.84313726 0.56078434 0.46666667]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [1.         0.8980392  0.8862745 ]\n   [1.         0.89411765 0.88235295]\n   [0.99607843 0.8862745  0.8745098 ]]]\n\n\n [[[0.77254903 0.6509804  0.6313726 ]\n   [0.78039217 0.65882355 0.6392157 ]\n   [0.8039216  0.6745098  0.654902  ]\n   ...\n   [0.8666667  0.74509805 0.6313726 ]\n   [0.8745098  0.7529412  0.6392157 ]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  [[0.77254903 0.6431373  0.62352943]\n   [0.7921569  0.6627451  0.6431373 ]\n   [0.8117647  0.68235296 0.6627451 ]\n   ...\n   [0.8666667  0.74509805 0.6313726 ]\n   [0.8745098  0.7529412  0.6392157 ]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  [[0.8039216  0.6666667  0.65882355]\n   [0.827451   0.6901961  0.68235296]\n   [0.85882354 0.70980394 0.7058824 ]\n   ...\n   [0.8627451  0.7411765  0.627451  ]\n   [0.87058824 0.7490196  0.63529414]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  ...\n\n  [[0.9529412  0.88235295 0.84313726]\n   [0.95686275 0.8862745  0.84705883]\n   [0.94509804 0.8745098  0.8352941 ]\n   ...\n   [0.6901961  0.50980395 0.3764706 ]\n   [0.70980394 0.5294118  0.3882353 ]\n   [0.7254902  0.54509807 0.40392157]]\n\n  [[0.95686275 0.8784314  0.84313726]\n   [0.95686275 0.8784314  0.84313726]\n   [0.94509804 0.8666667  0.83137256]\n   ...\n   [0.69411767 0.5137255  0.38039216]\n   [0.70980394 0.5294118  0.3882353 ]\n   [0.7176471  0.5372549  0.39607844]]\n\n  [[0.95686275 0.8784314  0.84313726]\n   [0.95686275 0.8784314  0.84313726]\n   [0.94509804 0.8666667  0.83137256]\n   ...\n   [0.69411767 0.5137255  0.38039216]\n   [0.7058824  0.5254902  0.38431373]\n   [0.7137255  0.53333336 0.39215687]]]\n\n\n ...\n\n\n [[[0.5882353  0.5372549  0.5137255 ]\n   [0.5882353  0.5372549  0.5137255 ]\n   [0.5882353  0.5372549  0.5137255 ]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  [[0.58431375 0.53333336 0.50980395]\n   [0.58431375 0.53333336 0.50980395]\n   [0.58431375 0.53333336 0.50980395]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  [[0.5803922  0.52156866 0.5019608 ]\n   [0.58431375 0.5254902  0.5058824 ]\n   [0.5803922  0.5294118  0.5058824 ]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  ...\n\n  [[0.47058824 0.40392157 0.3647059 ]\n   [0.47058824 0.40784314 0.35686275]\n   [0.47058824 0.40392157 0.3647059 ]\n   ...\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]]\n\n  [[0.4627451  0.4        0.34901962]\n   [0.46666667 0.40392157 0.34509805]\n   [0.47058824 0.40784314 0.35686275]\n   ...\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]]\n\n  [[0.45882353 0.39607844 0.3372549 ]\n   [0.4627451  0.4        0.34117648]\n   [0.47058824 0.40784314 0.34901962]\n   ...\n   [0.54509807 0.5137255  0.47058824]\n   [0.54509807 0.5137255  0.47058824]\n   [0.54509807 0.5137255  0.47058824]]]\n\n\n [[[0.49411765 0.36078432 0.22352941]\n   [0.5019608  0.36862746 0.23137255]\n   [0.5058824  0.37254903 0.23529412]\n   ...\n   [0.47058824 0.35686275 0.22352941]\n   [0.46666667 0.3529412  0.22745098]\n   [0.5647059  0.4627451  0.33333334]]\n\n  [[0.49019608 0.35686275 0.21960784]\n   [0.49803922 0.3647059  0.22745098]\n   [0.5019608  0.36862746 0.23137255]\n   ...\n   [0.46666667 0.3529412  0.21960784]\n   [0.4627451  0.34901962 0.22352941]\n   [0.56078434 0.45882353 0.32941177]]\n\n  [[0.4862745  0.3529412  0.21568628]\n   [0.49019608 0.35686275 0.21960784]\n   [0.49803922 0.3647059  0.22745098]\n   ...\n   [0.4627451  0.34901962 0.21568628]\n   [0.45882353 0.34509805 0.21960784]\n   [0.5568628  0.45490196 0.3254902 ]]\n\n  ...\n\n  [[0.6117647  0.4627451  0.30980393]\n   [0.6117647  0.4627451  0.30980393]\n   [0.6156863  0.46666667 0.3137255 ]\n   ...\n   [0.5921569  0.4745098  0.33333334]\n   [0.60784316 0.49411765 0.36862746]\n   [0.68235296 0.5686275  0.44313726]]\n\n  [[0.6039216  0.45490196 0.3019608 ]\n   [0.6039216  0.45490196 0.3019608 ]\n   [0.60784316 0.45882353 0.30588236]\n   ...\n   [0.5882353  0.47058824 0.32941177]\n   [0.6039216  0.49019608 0.3647059 ]\n   [0.6784314  0.5647059  0.4392157 ]]\n\n  [[0.5921569  0.44313726 0.2901961 ]\n   [0.59607846 0.44705883 0.29411766]\n   [0.6039216  0.45490196 0.3019608 ]\n   ...\n   [0.5882353  0.47058824 0.32941177]\n   [0.6        0.4862745  0.36078432]\n   [0.6745098  0.56078434 0.43529412]]]\n\n\n [[[0.8117647  0.5294118  0.5764706 ]\n   [0.827451   0.54509807 0.5921569 ]\n   [0.8509804  0.5686275  0.6156863 ]\n   ...\n   [0.28235295 0.35686275 0.38431373]\n   [0.27450982 0.3647059  0.39607844]\n   [0.26666668 0.3647059  0.39215687]]\n\n  [[0.84313726 0.56078434 0.60784316]\n   [0.84705883 0.5647059  0.6117647 ]\n   [0.85490197 0.57254905 0.61960787]\n   ...\n   [0.27058825 0.3372549  0.36862746]\n   [0.2627451  0.34117648 0.3764706 ]\n   [0.24705882 0.3372549  0.36862746]]\n\n  [[0.8666667  0.5921569  0.63529414]\n   [0.8627451  0.5882353  0.6313726 ]\n   [0.8627451  0.5803922  0.627451  ]\n   ...\n   [0.3019608  0.34117648 0.3764706 ]\n   [0.2784314  0.33333334 0.36862746]\n   [0.2627451  0.32941177 0.36078432]]\n\n  ...\n\n  [[0.87058824 0.654902   0.6431373 ]\n   [0.87058824 0.654902   0.6431373 ]\n   [0.8745098  0.65882355 0.64705884]\n   ...\n   [0.90588236 0.627451   0.61960787]\n   [0.90588236 0.6156863  0.6117647 ]\n   [0.9098039  0.6117647  0.6117647 ]]\n\n  [[0.8745098  0.64705884 0.6392157 ]\n   [0.88235295 0.654902   0.64705884]\n   [0.8901961  0.6627451  0.654902  ]\n   ...\n   [0.9098039  0.6313726  0.62352943]\n   [0.8980392  0.60784316 0.6039216 ]\n   [0.89411765 0.6039216  0.6       ]]\n\n  [[0.8745098  0.64705884 0.6392157 ]\n   [0.8862745  0.65882355 0.6509804 ]\n   [0.8980392  0.67058825 0.6627451 ]\n   ...\n   [0.90588236 0.6313726  0.62352943]\n   [0.89411765 0.6039216  0.6       ]\n   [0.8862745  0.59607846 0.5921569 ]]]]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:442\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py:264\u001b[0m, in \u001b[0;36mFunctionType.bind_with_defaults\u001b[0;34m(self, args, kwargs, default_values)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns BoundArguments with default values filled in.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03mand `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03mif the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3112\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   3110\u001b[0m     \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m     \u001b[38;5;66;03m# argument\u001b[39;00m\n\u001b[0;32m-> 3112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _VAR_POSITIONAL:\n\u001b[1;32m   3116\u001b[0m     \u001b[38;5;66;03m# We have an '*args'-like argument, let's fill it with\u001b[39;00m\n\u001b[1;32m   3117\u001b[0m     \u001b[38;5;66;03m# all positional arguments we have left and move on to\u001b[39;00m\n\u001b[1;32m   3118\u001b[0m     \u001b[38;5;66;03m# the next phase\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: too many positional arguments",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1179\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1179\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_structured_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m structured_err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1259\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_structured_signature\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function with the structured signature.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;124;03m    of this `ConcreteFunction`.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1259\u001b[0m     \u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m )\n\u001b[1;32m   1262\u001b[0m filtered_flat_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:422\u001b[0m, in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values, is_pure)\u001b[0m\n\u001b[1;32m    421\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[0;32m--> 422\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mbind_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:446\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 446\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `too many positional arguments`. Received args: (array([[[[0.68235296, 0.53333336, 0.49019608],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.85490197, 0.72156864, 0.68235296],\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ]],\n\n        [[0.68235296, 0.53333336, 0.49019608],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ]],\n\n        [[0.6862745 , 0.5372549 , 0.49411765],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.8627451 , 0.7294118 , 0.6901961 ],\n         [0.8627451 , 0.7294118 , 0.6901961 ],\n         [0.8627451 , 0.7294118 , 0.6901961 ]],\n\n        ...,\n\n        [[0.6666667 , 0.5019608 , 0.41568628],\n         [0.67058825, 0.5058824 , 0.41960785],\n         [0.67058825, 0.5058824 , 0.41960785],\n         ...,\n         [0.8156863 , 0.627451  , 0.54901963],\n         [0.827451  , 0.627451  , 0.5529412 ],\n         [0.827451  , 0.627451  , 0.5529412 ]],\n\n        [[0.6745098 , 0.50980395, 0.42352942],\n         [0.6745098 , 0.50980395, 0.42352942],\n         [0.6745098 , 0.50980395, 0.42352942],\n         ...,\n         [0.8117647 , 0.62352943, 0.54509807],\n         [0.81960785, 0.61960787, 0.54509807],\n         [0.8235294 , 0.62352943, 0.54901963]],\n\n        [[0.6784314 , 0.5137255 , 0.42745098],\n         [0.6784314 , 0.5137255 , 0.42745098],\n         [0.6745098 , 0.50980395, 0.42352942],\n         ...,\n         [0.80784315, 0.61960787, 0.5411765 ],\n         [0.81960785, 0.61960787, 0.54509807],\n         [0.81960785, 0.61960787, 0.54509807]]],\n\n\n       [[[0.7176471 , 0.4509804 , 0.3764706 ],\n         [0.7176471 , 0.4509804 , 0.3764706 ],\n         [0.72156864, 0.45490196, 0.38039216],\n         ...,\n         [0.9764706 , 0.77254903, 0.7294118 ],\n         [0.9647059 , 0.7607843 , 0.7176471 ],\n         [0.9490196 , 0.74509805, 0.7019608 ]],\n\n        [[0.7137255 , 0.44705883, 0.37254903],\n         [0.7137255 , 0.44705883, 0.37254903],\n         [0.7137255 , 0.44705883, 0.37254903],\n         ...,\n         [0.972549  , 0.76862746, 0.7254902 ],\n         [0.9607843 , 0.75686276, 0.7137255 ],\n         [0.9490196 , 0.74509805, 0.7019608 ]],\n\n        [[0.7137255 , 0.44705883, 0.3647059 ],\n         [0.7137255 , 0.44705883, 0.3647059 ],\n         [0.70980394, 0.44313726, 0.36078432],\n         ...,\n         [0.9607843 , 0.75686276, 0.7137255 ],\n         [0.9529412 , 0.7490196 , 0.7058824 ],\n         [0.94509804, 0.7411765 , 0.69803923]],\n\n        ...,\n\n        [[0.8235294 , 0.5411765 , 0.44705883],\n         [0.827451  , 0.54509807, 0.4509804 ],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [0.98039216, 0.87058824, 0.85882354],\n         [0.98039216, 0.87058824, 0.85882354],\n         [0.9764706 , 0.8666667 , 0.85490197]],\n\n        [[0.8392157 , 0.5568628 , 0.4627451 ],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [0.99607843, 0.8862745 , 0.8745098 ],\n         [0.99607843, 0.8862745 , 0.8745098 ],\n         [0.9882353 , 0.8784314 , 0.8666667 ]],\n\n        [[0.85490197, 0.57254905, 0.47843137],\n         [0.84313726, 0.56078434, 0.46666667],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [1.        , 0.8980392 , 0.8862745 ],\n         [1.        , 0.89411765, 0.88235295],\n         [0.99607843, 0.8862745 , 0.8745098 ]]],\n\n\n       [[[0.77254903, 0.6509804 , 0.6313726 ],\n         [0.78039217, 0.65882355, 0.6392157 ],\n         [0.8039216 , 0.6745098 , 0.654902  ],\n         ...,\n         [0.8666667 , 0.74509805, 0.6313726 ],\n         [0.8745098 , 0.7529412 , 0.6392157 ],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        [[0.77254903, 0.6431373 , 0.62352943],\n         [0.7921569 , 0.6627451 , 0.6431373 ],\n         [0.8117647 , 0.68235296, 0.6627451 ],\n         ...,\n         [0.8666667 , 0.74509805, 0.6313726 ],\n         [0.8745098 , 0.7529412 , 0.6392157 ],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        [[0.8039216 , 0.6666667 , 0.65882355],\n         [0.827451  , 0.6901961 , 0.68235296],\n         [0.85882354, 0.70980394, 0.7058824 ],\n         ...,\n         [0.8627451 , 0.7411765 , 0.627451  ],\n         [0.87058824, 0.7490196 , 0.63529414],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        ...,\n\n        [[0.9529412 , 0.88235295, 0.84313726],\n         [0.95686275, 0.8862745 , 0.84705883],\n         [0.94509804, 0.8745098 , 0.8352941 ],\n         ...,\n         [0.6901961 , 0.50980395, 0.3764706 ],\n         [0.70980394, 0.5294118 , 0.3882353 ],\n         [0.7254902 , 0.54509807, 0.40392157]],\n\n        [[0.95686275, 0.8784314 , 0.84313726],\n         [0.95686275, 0.8784314 , 0.84313726],\n         [0.94509804, 0.8666667 , 0.83137256],\n         ...,\n         [0.69411767, 0.5137255 , 0.38039216],\n         [0.70980394, 0.5294118 , 0.3882353 ],\n         [0.7176471 , 0.5372549 , 0.39607844]],\n\n        [[0.95686275, 0.8784314 , 0.84313726],\n         [0.95686275, 0.8784314 , 0.84313726],\n         [0.94509804, 0.8666667 , 0.83137256],\n         ...,\n         [0.69411767, 0.5137255 , 0.38039216],\n         [0.7058824 , 0.5254902 , 0.38431373],\n         [0.7137255 , 0.53333336, 0.39215687]]],\n\n\n       ...,\n\n\n       [[[0.5882353 , 0.5372549 , 0.5137255 ],\n         [0.5882353 , 0.5372549 , 0.5137255 ],\n         [0.5882353 , 0.5372549 , 0.5137255 ],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        [[0.58431375, 0.53333336, 0.50980395],\n         [0.58431375, 0.53333336, 0.50980395],\n         [0.58431375, 0.53333336, 0.50980395],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        [[0.5803922 , 0.52156866, 0.5019608 ],\n         [0.58431375, 0.5254902 , 0.5058824 ],\n         [0.5803922 , 0.5294118 , 0.5058824 ],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        ...,\n\n        [[0.47058824, 0.40392157, 0.3647059 ],\n         [0.47058824, 0.40784314, 0.35686275],\n         [0.47058824, 0.40392157, 0.3647059 ],\n         ...,\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667]],\n\n        [[0.4627451 , 0.4       , 0.34901962],\n         [0.46666667, 0.40392157, 0.34509805],\n         [0.47058824, 0.40784314, 0.35686275],\n         ...,\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667]],\n\n        [[0.45882353, 0.39607844, 0.3372549 ],\n         [0.4627451 , 0.4       , 0.34117648],\n         [0.47058824, 0.40784314, 0.34901962],\n         ...,\n         [0.54509807, 0.5137255 , 0.47058824],\n         [0.54509807, 0.5137255 , 0.47058824],\n         [0.54509807, 0.5137255 , 0.47058824]]],\n\n\n       [[[0.49411765, 0.36078432, 0.22352941],\n         [0.5019608 , 0.36862746, 0.23137255],\n         [0.5058824 , 0.37254903, 0.23529412],\n         ...,\n         [0.47058824, 0.35686275, 0.22352941],\n         [0.46666667, 0.3529412 , 0.22745098],\n         [0.5647059 , 0.4627451 , 0.33333334]],\n\n        [[0.49019608, 0.35686275, 0.21960784],\n         [0.49803922, 0.3647059 , 0.22745098],\n         [0.5019608 , 0.36862746, 0.23137255],\n         ...,\n         [0.46666667, 0.3529412 , 0.21960784],\n         [0.4627451 , 0.34901962, 0.22352941],\n         [0.56078434, 0.45882353, 0.32941177]],\n\n        [[0.4862745 , 0.3529412 , 0.21568628],\n         [0.49019608, 0.35686275, 0.21960784],\n         [0.49803922, 0.3647059 , 0.22745098],\n         ...,\n         [0.4627451 , 0.34901962, 0.21568628],\n         [0.45882353, 0.34509805, 0.21960784],\n         [0.5568628 , 0.45490196, 0.3254902 ]],\n\n        ...,\n\n        [[0.6117647 , 0.4627451 , 0.30980393],\n         [0.6117647 , 0.4627451 , 0.30980393],\n         [0.6156863 , 0.46666667, 0.3137255 ],\n         ...,\n         [0.5921569 , 0.4745098 , 0.33333334],\n         [0.60784316, 0.49411765, 0.36862746],\n         [0.68235296, 0.5686275 , 0.44313726]],\n\n        [[0.6039216 , 0.45490196, 0.3019608 ],\n         [0.6039216 , 0.45490196, 0.3019608 ],\n         [0.60784316, 0.45882353, 0.30588236],\n         ...,\n         [0.5882353 , 0.47058824, 0.32941177],\n         [0.6039216 , 0.49019608, 0.3647059 ],\n         [0.6784314 , 0.5647059 , 0.4392157 ]],\n\n        [[0.5921569 , 0.44313726, 0.2901961 ],\n         [0.59607846, 0.44705883, 0.29411766],\n         [0.6039216 , 0.45490196, 0.3019608 ],\n         ...,\n         [0.5882353 , 0.47058824, 0.32941177],\n         [0.6       , 0.4862745 , 0.36078432],\n         [0.6745098 , 0.56078434, 0.43529412]]],\n\n\n       [[[0.8117647 , 0.5294118 , 0.5764706 ],\n         [0.827451  , 0.54509807, 0.5921569 ],\n         [0.8509804 , 0.5686275 , 0.6156863 ],\n         ...,\n         [0.28235295, 0.35686275, 0.38431373],\n         [0.27450982, 0.3647059 , 0.39607844],\n         [0.26666668, 0.3647059 , 0.39215687]],\n\n        [[0.84313726, 0.56078434, 0.60784316],\n         [0.84705883, 0.5647059 , 0.6117647 ],\n         [0.85490197, 0.57254905, 0.61960787],\n         ...,\n         [0.27058825, 0.3372549 , 0.36862746],\n         [0.2627451 , 0.34117648, 0.3764706 ],\n         [0.24705882, 0.3372549 , 0.36862746]],\n\n        [[0.8666667 , 0.5921569 , 0.63529414],\n         [0.8627451 , 0.5882353 , 0.6313726 ],\n         [0.8627451 , 0.5803922 , 0.627451  ],\n         ...,\n         [0.3019608 , 0.34117648, 0.3764706 ],\n         [0.2784314 , 0.33333334, 0.36862746],\n         [0.2627451 , 0.32941177, 0.36078432]],\n\n        ...,\n\n        [[0.87058824, 0.654902  , 0.6431373 ],\n         [0.87058824, 0.654902  , 0.6431373 ],\n         [0.8745098 , 0.65882355, 0.64705884],\n         ...,\n         [0.90588236, 0.627451  , 0.61960787],\n         [0.90588236, 0.6156863 , 0.6117647 ],\n         [0.9098039 , 0.6117647 , 0.6117647 ]],\n\n        [[0.8745098 , 0.64705884, 0.6392157 ],\n         [0.88235295, 0.654902  , 0.64705884],\n         [0.8901961 , 0.6627451 , 0.654902  ],\n         ...,\n         [0.9098039 , 0.6313726 , 0.62352943],\n         [0.8980392 , 0.60784316, 0.6039216 ],\n         [0.89411765, 0.6039216 , 0.6       ]],\n\n        [[0.8745098 , 0.64705884, 0.6392157 ],\n         [0.8862745 , 0.65882355, 0.6509804 ],\n         [0.8980392 , 0.67058825, 0.6627451 ],\n         ...,\n         [0.90588236, 0.6313726 , 0.62352943],\n         [0.89411765, 0.6039216 , 0.6       ],\n         [0.8862745 , 0.59607846, 0.5921569 ]]]], dtype=float32),) and kwargs: {} for signature: (*, keras_tensor_312: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_312')) -> Dict[['output_0', TensorSpec(shape=(None, 1), dtype=tf.float32, name='output_0')]].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1182\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_flat_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m flat_err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1239\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_flat_signature\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1238\u001b[0m       arg, (tensor_lib\u001b[38;5;241m.\u001b[39mTensor, resource_variable_ops\u001b[38;5;241m.\u001b[39mBaseResourceVariable)):\n\u001b[0;32m-> 1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_signature_summary()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: expected argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1240\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(zero-based) to be a Tensor; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_flat(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "\u001b[0;31mTypeError\u001b[0m: signature_wrapper___call__(keras_tensor_312): expected argument #0(zero-based) to be a Tensor; got ndarray ([[[[0.68235296 0.53333336 0.49019608]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.85490197 0.72156864 0.68235296]\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]]\n\n  [[0.68235296 0.53333336 0.49019608]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]]\n\n  [[0.6862745  0.5372549  0.49411765]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.8627451  0.7294118  0.6901961 ]\n   [0.8627451  0.7294118  0.6901961 ]\n   [0.8627451  0.7294118  0.6901961 ]]\n\n  ...\n\n  [[0.6666667  0.5019608  0.41568628]\n   [0.67058825 0.5058824  0.41960785]\n   [0.67058825 0.5058824  0.41960785]\n   ...\n   [0.8156863  0.627451   0.54901963]\n   [0.827451   0.627451   0.5529412 ]\n   [0.827451   0.627451   0.5529412 ]]\n\n  [[0.6745098  0.50980395 0.42352942]\n   [0.6745098  0.50980395 0.42352942]\n   [0.6745098  0.50980395 0.42352942]\n   ...\n   [0.8117647  0.62352943 0.54509807]\n   [0.81960785 0.61960787 0.54509807]\n   [0.8235294  0.62352943 0.54901963]]\n\n  [[0.6784314  0.5137255  0.42745098]\n   [0.6784314  0.5137255  0.42745098]\n   [0.6745098  0.50980395 0.42352942]\n   ...\n   [0.80784315 0.61960787 0.5411765 ]\n   [0.81960785 0.61960787 0.54509807]\n   [0.81960785 0.61960787 0.54509807]]]\n\n\n [[[0.7176471  0.4509804  0.3764706 ]\n   [0.7176471  0.4509804  0.3764706 ]\n   [0.72156864 0.45490196 0.38039216]\n   ...\n   [0.9764706  0.77254903 0.7294118 ]\n   [0.9647059  0.7607843  0.7176471 ]\n   [0.9490196  0.74509805 0.7019608 ]]\n\n  [[0.7137255  0.44705883 0.37254903]\n   [0.7137255  0.44705883 0.37254903]\n   [0.7137255  0.44705883 0.37254903]\n   ...\n   [0.972549   0.76862746 0.7254902 ]\n   [0.9607843  0.75686276 0.7137255 ]\n   [0.9490196  0.74509805 0.7019608 ]]\n\n  [[0.7137255  0.44705883 0.3647059 ]\n   [0.7137255  0.44705883 0.3647059 ]\n   [0.70980394 0.44313726 0.36078432]\n   ...\n   [0.9607843  0.75686276 0.7137255 ]\n   [0.9529412  0.7490196  0.7058824 ]\n   [0.94509804 0.7411765  0.69803923]]\n\n  ...\n\n  [[0.8235294  0.5411765  0.44705883]\n   [0.827451   0.54509807 0.4509804 ]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [0.98039216 0.87058824 0.85882354]\n   [0.98039216 0.87058824 0.85882354]\n   [0.9764706  0.8666667  0.85490197]]\n\n  [[0.8392157  0.5568628  0.4627451 ]\n   [0.8352941  0.5529412  0.45882353]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [0.99607843 0.8862745  0.8745098 ]\n   [0.99607843 0.8862745  0.8745098 ]\n   [0.9882353  0.8784314  0.8666667 ]]\n\n  [[0.85490197 0.57254905 0.47843137]\n   [0.84313726 0.56078434 0.46666667]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [1.         0.8980392  0.8862745 ]\n   [1.         0.89411765 0.88235295]\n   [0.99607843 0.8862745  0.8745098 ]]]\n\n\n [[[0.77254903 0.6509804  0.6313726 ]\n   [0.78039217 0.65882355 0.6392157 ]\n   [0.8039216  0.6745098  0.654902  ]\n   ...\n   [0.8666667  0.74509805 0.6313726 ]\n   [0.8745098  0.7529412  0.6392157 ]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  [[0.77254903 0.6431373  0.62352943]\n   [0.7921569  0.6627451  0.6431373 ]\n   [0.8117647  0.68235296 0.6627451 ]\n   ...\n   [0.8666667  0.74509805 0.6313726 ]\n   [0.8745098  0.7529412  0.6392157 ]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  [[0.8039216  0.6666667  0.65882355]\n   [0.827451   0.6901961  0.68235296]\n   [0.85882354 0.70980394 0.7058824 ]\n   ...\n   [0.8627451  0.7411765  0.627451  ]\n   [0.87058824 0.7490196  0.63529414]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  ...\n\n  [[0.9529412  0.88235295 0.84313726]\n   [0.95686275 0.8862745  0.84705883]\n   [0.94509804 0.8745098  0.8352941 ]\n   ...\n   [0.6901961  0.50980395 0.3764706 ]\n   [0.70980394 0.5294118  0.3882353 ]\n   [0.7254902  0.54509807 0.40392157]]\n\n  [[0.95686275 0.8784314  0.84313726]\n   [0.95686275 0.8784314  0.84313726]\n   [0.94509804 0.8666667  0.83137256]\n   ...\n   [0.69411767 0.5137255  0.38039216]\n   [0.70980394 0.5294118  0.3882353 ]\n   [0.7176471  0.5372549  0.39607844]]\n\n  [[0.95686275 0.8784314  0.84313726]\n   [0.95686275 0.8784314  0.84313726]\n   [0.94509804 0.8666667  0.83137256]\n   ...\n   [0.69411767 0.5137255  0.38039216]\n   [0.7058824  0.5254902  0.38431373]\n   [0.7137255  0.53333336 0.39215687]]]\n\n\n ...\n\n\n [[[0.5882353  0.5372549  0.5137255 ]\n   [0.5882353  0.5372549  0.5137255 ]\n   [0.5882353  0.5372549  0.5137255 ]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  [[0.58431375 0.53333336 0.50980395]\n   [0.58431375 0.53333336 0.50980395]\n   [0.58431375 0.53333336 0.50980395]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  [[0.5803922  0.52156866 0.5019608 ]\n   [0.58431375 0.5254902  0.5058824 ]\n   [0.5803922  0.5294118  0.5058824 ]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  ...\n\n  [[0.47058824 0.40392157 0.3647059 ]\n   [0.47058824 0.40784314 0.35686275]\n   [0.47058824 0.40392157 0.3647059 ]\n   ...\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]]\n\n  [[0.4627451  0.4        0.34901962]\n   [0.46666667 0.40392157 0.34509805]\n   [0.47058824 0.40784314 0.35686275]\n   ...\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]]\n\n  [[0.45882353 0.39607844 0.3372549 ]\n   [0.4627451  0.4        0.34117648]\n   [0.47058824 0.40784314 0.34901962]\n   ...\n   [0.54509807 0.5137255  0.47058824]\n   [0.54509807 0.5137255  0.47058824]\n   [0.54509807 0.5137255  0.47058824]]]\n\n\n [[[0.49411765 0.36078432 0.22352941]\n   [0.5019608  0.36862746 0.23137255]\n   [0.5058824  0.37254903 0.23529412]\n   ...\n   [0.47058824 0.35686275 0.22352941]\n   [0.46666667 0.3529412  0.22745098]\n   [0.5647059  0.4627451  0.33333334]]\n\n  [[0.49019608 0.35686275 0.21960784]\n   [0.49803922 0.3647059  0.22745098]\n   [0.5019608  0.36862746 0.23137255]\n   ...\n   [0.46666667 0.3529412  0.21960784]\n   [0.4627451  0.34901962 0.22352941]\n   [0.56078434 0.45882353 0.32941177]]\n\n  [[0.4862745  0.3529412  0.21568628]\n   [0.49019608 0.35686275 0.21960784]\n   [0.49803922 0.3647059  0.22745098]\n   ...\n   [0.4627451  0.34901962 0.21568628]\n   [0.45882353 0.34509805 0.21960784]\n   [0.5568628  0.45490196 0.3254902 ]]\n\n  ...\n\n  [[0.6117647  0.4627451  0.30980393]\n   [0.6117647  0.4627451  0.30980393]\n   [0.6156863  0.46666667 0.3137255 ]\n   ...\n   [0.5921569  0.4745098  0.33333334]\n   [0.60784316 0.49411765 0.36862746]\n   [0.68235296 0.5686275  0.44313726]]\n\n  [[0.6039216  0.45490196 0.3019608 ]\n   [0.6039216  0.45490196 0.3019608 ]\n   [0.60784316 0.45882353 0.30588236]\n   ...\n   [0.5882353  0.47058824 0.32941177]\n   [0.6039216  0.49019608 0.3647059 ]\n   [0.6784314  0.5647059  0.4392157 ]]\n\n  [[0.5921569  0.44313726 0.2901961 ]\n   [0.59607846 0.44705883 0.29411766]\n   [0.6039216  0.45490196 0.3019608 ]\n   ...\n   [0.5882353  0.47058824 0.32941177]\n   [0.6        0.4862745  0.36078432]\n   [0.6745098  0.56078434 0.43529412]]]\n\n\n [[[0.8117647  0.5294118  0.5764706 ]\n   [0.827451   0.54509807 0.5921569 ]\n   [0.8509804  0.5686275  0.6156863 ]\n   ...\n   [0.28235295 0.35686275 0.38431373]\n   [0.27450982 0.3647059  0.39607844]\n   [0.26666668 0.3647059  0.39215687]]\n\n  [[0.84313726 0.56078434 0.60784316]\n   [0.84705883 0.5647059  0.6117647 ]\n   [0.85490197 0.57254905 0.61960787]\n   ...\n   [0.27058825 0.3372549  0.36862746]\n   [0.2627451  0.34117648 0.3764706 ]\n   [0.24705882 0.3372549  0.36862746]]\n\n  [[0.8666667  0.5921569  0.63529414]\n   [0.8627451  0.5882353  0.6313726 ]\n   [0.8627451  0.5803922  0.627451  ]\n   ...\n   [0.3019608  0.34117648 0.3764706 ]\n   [0.2784314  0.33333334 0.36862746]\n   [0.2627451  0.32941177 0.36078432]]\n\n  ...\n\n  [[0.87058824 0.654902   0.6431373 ]\n   [0.87058824 0.654902   0.6431373 ]\n   [0.8745098  0.65882355 0.64705884]\n   ...\n   [0.90588236 0.627451   0.61960787]\n   [0.90588236 0.6156863  0.6117647 ]\n   [0.9098039  0.6117647  0.6117647 ]]\n\n  [[0.8745098  0.64705884 0.6392157 ]\n   [0.88235295 0.654902   0.64705884]\n   [0.8901961  0.6627451  0.654902  ]\n   ...\n   [0.9098039  0.6313726  0.62352943]\n   [0.8980392  0.60784316 0.6039216 ]\n   [0.89411765 0.6039216  0.6       ]]\n\n  [[0.8745098  0.64705884 0.6392157 ]\n   [0.8862745  0.65882355 0.6509804 ]\n   [0.8980392  0.67058825 0.6627451 ]\n   ...\n   [0.90588236 0.6313726  0.62352943]\n   [0.89411765 0.6039216  0.6       ]\n   [0.8862745  0.59607846 0.5921569 ]]]]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Run inference for a batch of images\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# saved_model.predict(X_test, batch_size=batch_size, verbose=0)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserving_default\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform inference directly\u001b[39;00m\n\u001b[1;32m     32\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     33\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1170\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m  ConcreteFunctions have two signatures:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;124;03m    TypeError: If the arguments do not match the function's signature.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1184\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_flat_signature(args, kwargs)\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m flat_err:\n\u001b[0;32m-> 1184\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(  \u001b[38;5;66;03m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m           \u001b[38;5;28mstr\u001b[39m(structured_err)\n\u001b[1;32m   1186\u001b[0m           \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFallback to flat signature also failed due to: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m           \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(flat_err)\n\u001b[1;32m   1188\u001b[0m       )\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_flat_signature(args, kwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `too many positional arguments`. Received args: (array([[[[0.68235296, 0.53333336, 0.49019608],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.85490197, 0.72156864, 0.68235296],\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ]],\n\n        [[0.68235296, 0.53333336, 0.49019608],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ],\n         [0.85882354, 0.7254902 , 0.6862745 ]],\n\n        [[0.6862745 , 0.5372549 , 0.49411765],\n         [0.6862745 , 0.5372549 , 0.49411765],\n         [0.6901961 , 0.5411765 , 0.49803922],\n         ...,\n         [0.8627451 , 0.7294118 , 0.6901961 ],\n         [0.8627451 , 0.7294118 , 0.6901961 ],\n         [0.8627451 , 0.7294118 , 0.6901961 ]],\n\n        ...,\n\n        [[0.6666667 , 0.5019608 , 0.41568628],\n         [0.67058825, 0.5058824 , 0.41960785],\n         [0.67058825, 0.5058824 , 0.41960785],\n         ...,\n         [0.8156863 , 0.627451  , 0.54901963],\n         [0.827451  , 0.627451  , 0.5529412 ],\n         [0.827451  , 0.627451  , 0.5529412 ]],\n\n        [[0.6745098 , 0.50980395, 0.42352942],\n         [0.6745098 , 0.50980395, 0.42352942],\n         [0.6745098 , 0.50980395, 0.42352942],\n         ...,\n         [0.8117647 , 0.62352943, 0.54509807],\n         [0.81960785, 0.61960787, 0.54509807],\n         [0.8235294 , 0.62352943, 0.54901963]],\n\n        [[0.6784314 , 0.5137255 , 0.42745098],\n         [0.6784314 , 0.5137255 , 0.42745098],\n         [0.6745098 , 0.50980395, 0.42352942],\n         ...,\n         [0.80784315, 0.61960787, 0.5411765 ],\n         [0.81960785, 0.61960787, 0.54509807],\n         [0.81960785, 0.61960787, 0.54509807]]],\n\n\n       [[[0.7176471 , 0.4509804 , 0.3764706 ],\n         [0.7176471 , 0.4509804 , 0.3764706 ],\n         [0.72156864, 0.45490196, 0.38039216],\n         ...,\n         [0.9764706 , 0.77254903, 0.7294118 ],\n         [0.9647059 , 0.7607843 , 0.7176471 ],\n         [0.9490196 , 0.74509805, 0.7019608 ]],\n\n        [[0.7137255 , 0.44705883, 0.37254903],\n         [0.7137255 , 0.44705883, 0.37254903],\n         [0.7137255 , 0.44705883, 0.37254903],\n         ...,\n         [0.972549  , 0.76862746, 0.7254902 ],\n         [0.9607843 , 0.75686276, 0.7137255 ],\n         [0.9490196 , 0.74509805, 0.7019608 ]],\n\n        [[0.7137255 , 0.44705883, 0.3647059 ],\n         [0.7137255 , 0.44705883, 0.3647059 ],\n         [0.70980394, 0.44313726, 0.36078432],\n         ...,\n         [0.9607843 , 0.75686276, 0.7137255 ],\n         [0.9529412 , 0.7490196 , 0.7058824 ],\n         [0.94509804, 0.7411765 , 0.69803923]],\n\n        ...,\n\n        [[0.8235294 , 0.5411765 , 0.44705883],\n         [0.827451  , 0.54509807, 0.4509804 ],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [0.98039216, 0.87058824, 0.85882354],\n         [0.98039216, 0.87058824, 0.85882354],\n         [0.9764706 , 0.8666667 , 0.85490197]],\n\n        [[0.8392157 , 0.5568628 , 0.4627451 ],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [0.99607843, 0.8862745 , 0.8745098 ],\n         [0.99607843, 0.8862745 , 0.8745098 ],\n         [0.9882353 , 0.8784314 , 0.8666667 ]],\n\n        [[0.85490197, 0.57254905, 0.47843137],\n         [0.84313726, 0.56078434, 0.46666667],\n         [0.8352941 , 0.5529412 , 0.45882353],\n         ...,\n         [1.        , 0.8980392 , 0.8862745 ],\n         [1.        , 0.89411765, 0.88235295],\n         [0.99607843, 0.8862745 , 0.8745098 ]]],\n\n\n       [[[0.77254903, 0.6509804 , 0.6313726 ],\n         [0.78039217, 0.65882355, 0.6392157 ],\n         [0.8039216 , 0.6745098 , 0.654902  ],\n         ...,\n         [0.8666667 , 0.74509805, 0.6313726 ],\n         [0.8745098 , 0.7529412 , 0.6392157 ],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        [[0.77254903, 0.6431373 , 0.62352943],\n         [0.7921569 , 0.6627451 , 0.6431373 ],\n         [0.8117647 , 0.68235296, 0.6627451 ],\n         ...,\n         [0.8666667 , 0.74509805, 0.6313726 ],\n         [0.8745098 , 0.7529412 , 0.6392157 ],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        [[0.8039216 , 0.6666667 , 0.65882355],\n         [0.827451  , 0.6901961 , 0.68235296],\n         [0.85882354, 0.70980394, 0.7058824 ],\n         ...,\n         [0.8627451 , 0.7411765 , 0.627451  ],\n         [0.87058824, 0.7490196 , 0.63529414],\n         [0.8784314 , 0.75686276, 0.6431373 ]],\n\n        ...,\n\n        [[0.9529412 , 0.88235295, 0.84313726],\n         [0.95686275, 0.8862745 , 0.84705883],\n         [0.94509804, 0.8745098 , 0.8352941 ],\n         ...,\n         [0.6901961 , 0.50980395, 0.3764706 ],\n         [0.70980394, 0.5294118 , 0.3882353 ],\n         [0.7254902 , 0.54509807, 0.40392157]],\n\n        [[0.95686275, 0.8784314 , 0.84313726],\n         [0.95686275, 0.8784314 , 0.84313726],\n         [0.94509804, 0.8666667 , 0.83137256],\n         ...,\n         [0.69411767, 0.5137255 , 0.38039216],\n         [0.70980394, 0.5294118 , 0.3882353 ],\n         [0.7176471 , 0.5372549 , 0.39607844]],\n\n        [[0.95686275, 0.8784314 , 0.84313726],\n         [0.95686275, 0.8784314 , 0.84313726],\n         [0.94509804, 0.8666667 , 0.83137256],\n         ...,\n         [0.69411767, 0.5137255 , 0.38039216],\n         [0.7058824 , 0.5254902 , 0.38431373],\n         [0.7137255 , 0.53333336, 0.39215687]]],\n\n\n       ...,\n\n\n       [[[0.5882353 , 0.5372549 , 0.5137255 ],\n         [0.5882353 , 0.5372549 , 0.5137255 ],\n         [0.5882353 , 0.5372549 , 0.5137255 ],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        [[0.58431375, 0.53333336, 0.50980395],\n         [0.58431375, 0.53333336, 0.50980395],\n         [0.58431375, 0.53333336, 0.50980395],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        [[0.5803922 , 0.52156866, 0.5019608 ],\n         [0.58431375, 0.5254902 , 0.5058824 ],\n         [0.5803922 , 0.5294118 , 0.5058824 ],\n         ...,\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296],\n         [0.654902  , 0.67058825, 0.68235296]],\n\n        ...,\n\n        [[0.47058824, 0.40392157, 0.3647059 ],\n         [0.47058824, 0.40784314, 0.35686275],\n         [0.47058824, 0.40392157, 0.3647059 ],\n         ...,\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667]],\n\n        [[0.4627451 , 0.4       , 0.34901962],\n         [0.46666667, 0.40392157, 0.34509805],\n         [0.47058824, 0.40784314, 0.35686275],\n         ...,\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667],\n         [0.5411765 , 0.50980395, 0.46666667]],\n\n        [[0.45882353, 0.39607844, 0.3372549 ],\n         [0.4627451 , 0.4       , 0.34117648],\n         [0.47058824, 0.40784314, 0.34901962],\n         ...,\n         [0.54509807, 0.5137255 , 0.47058824],\n         [0.54509807, 0.5137255 , 0.47058824],\n         [0.54509807, 0.5137255 , 0.47058824]]],\n\n\n       [[[0.49411765, 0.36078432, 0.22352941],\n         [0.5019608 , 0.36862746, 0.23137255],\n         [0.5058824 , 0.37254903, 0.23529412],\n         ...,\n         [0.47058824, 0.35686275, 0.22352941],\n         [0.46666667, 0.3529412 , 0.22745098],\n         [0.5647059 , 0.4627451 , 0.33333334]],\n\n        [[0.49019608, 0.35686275, 0.21960784],\n         [0.49803922, 0.3647059 , 0.22745098],\n         [0.5019608 , 0.36862746, 0.23137255],\n         ...,\n         [0.46666667, 0.3529412 , 0.21960784],\n         [0.4627451 , 0.34901962, 0.22352941],\n         [0.56078434, 0.45882353, 0.32941177]],\n\n        [[0.4862745 , 0.3529412 , 0.21568628],\n         [0.49019608, 0.35686275, 0.21960784],\n         [0.49803922, 0.3647059 , 0.22745098],\n         ...,\n         [0.4627451 , 0.34901962, 0.21568628],\n         [0.45882353, 0.34509805, 0.21960784],\n         [0.5568628 , 0.45490196, 0.3254902 ]],\n\n        ...,\n\n        [[0.6117647 , 0.4627451 , 0.30980393],\n         [0.6117647 , 0.4627451 , 0.30980393],\n         [0.6156863 , 0.46666667, 0.3137255 ],\n         ...,\n         [0.5921569 , 0.4745098 , 0.33333334],\n         [0.60784316, 0.49411765, 0.36862746],\n         [0.68235296, 0.5686275 , 0.44313726]],\n\n        [[0.6039216 , 0.45490196, 0.3019608 ],\n         [0.6039216 , 0.45490196, 0.3019608 ],\n         [0.60784316, 0.45882353, 0.30588236],\n         ...,\n         [0.5882353 , 0.47058824, 0.32941177],\n         [0.6039216 , 0.49019608, 0.3647059 ],\n         [0.6784314 , 0.5647059 , 0.4392157 ]],\n\n        [[0.5921569 , 0.44313726, 0.2901961 ],\n         [0.59607846, 0.44705883, 0.29411766],\n         [0.6039216 , 0.45490196, 0.3019608 ],\n         ...,\n         [0.5882353 , 0.47058824, 0.32941177],\n         [0.6       , 0.4862745 , 0.36078432],\n         [0.6745098 , 0.56078434, 0.43529412]]],\n\n\n       [[[0.8117647 , 0.5294118 , 0.5764706 ],\n         [0.827451  , 0.54509807, 0.5921569 ],\n         [0.8509804 , 0.5686275 , 0.6156863 ],\n         ...,\n         [0.28235295, 0.35686275, 0.38431373],\n         [0.27450982, 0.3647059 , 0.39607844],\n         [0.26666668, 0.3647059 , 0.39215687]],\n\n        [[0.84313726, 0.56078434, 0.60784316],\n         [0.84705883, 0.5647059 , 0.6117647 ],\n         [0.85490197, 0.57254905, 0.61960787],\n         ...,\n         [0.27058825, 0.3372549 , 0.36862746],\n         [0.2627451 , 0.34117648, 0.3764706 ],\n         [0.24705882, 0.3372549 , 0.36862746]],\n\n        [[0.8666667 , 0.5921569 , 0.63529414],\n         [0.8627451 , 0.5882353 , 0.6313726 ],\n         [0.8627451 , 0.5803922 , 0.627451  ],\n         ...,\n         [0.3019608 , 0.34117648, 0.3764706 ],\n         [0.2784314 , 0.33333334, 0.36862746],\n         [0.2627451 , 0.32941177, 0.36078432]],\n\n        ...,\n\n        [[0.87058824, 0.654902  , 0.6431373 ],\n         [0.87058824, 0.654902  , 0.6431373 ],\n         [0.8745098 , 0.65882355, 0.64705884],\n         ...,\n         [0.90588236, 0.627451  , 0.61960787],\n         [0.90588236, 0.6156863 , 0.6117647 ],\n         [0.9098039 , 0.6117647 , 0.6117647 ]],\n\n        [[0.8745098 , 0.64705884, 0.6392157 ],\n         [0.88235295, 0.654902  , 0.64705884],\n         [0.8901961 , 0.6627451 , 0.654902  ],\n         ...,\n         [0.9098039 , 0.6313726 , 0.62352943],\n         [0.8980392 , 0.60784316, 0.6039216 ],\n         [0.89411765, 0.6039216 , 0.6       ]],\n\n        [[0.8745098 , 0.64705884, 0.6392157 ],\n         [0.8862745 , 0.65882355, 0.6509804 ],\n         [0.8980392 , 0.67058825, 0.6627451 ],\n         ...,\n         [0.90588236, 0.6313726 , 0.62352943],\n         [0.89411765, 0.6039216 , 0.6       ],\n         [0.8862745 , 0.59607846, 0.5921569 ]]]], dtype=float32),) and kwargs: {} for signature: (*, keras_tensor_312: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_312')) -> Dict[['output_0', TensorSpec(shape=(None, 1), dtype=tf.float32, name='output_0')]].\nFallback to flat signature also failed due to: signature_wrapper___call__(keras_tensor_312): expected argument #0(zero-based) to be a Tensor; got ndarray ([[[[0.68235296 0.53333336 0.49019608]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.85490197 0.72156864 0.68235296]\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]]\n\n  [[0.68235296 0.53333336 0.49019608]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]\n   [0.85882354 0.7254902  0.6862745 ]]\n\n  [[0.6862745  0.5372549  0.49411765]\n   [0.6862745  0.5372549  0.49411765]\n   [0.6901961  0.5411765  0.49803922]\n   ...\n   [0.8627451  0.7294118  0.6901961 ]\n   [0.8627451  0.7294118  0.6901961 ]\n   [0.8627451  0.7294118  0.6901961 ]]\n\n  ...\n\n  [[0.6666667  0.5019608  0.41568628]\n   [0.67058825 0.5058824  0.41960785]\n   [0.67058825 0.5058824  0.41960785]\n   ...\n   [0.8156863  0.627451   0.54901963]\n   [0.827451   0.627451   0.5529412 ]\n   [0.827451   0.627451   0.5529412 ]]\n\n  [[0.6745098  0.50980395 0.42352942]\n   [0.6745098  0.50980395 0.42352942]\n   [0.6745098  0.50980395 0.42352942]\n   ...\n   [0.8117647  0.62352943 0.54509807]\n   [0.81960785 0.61960787 0.54509807]\n   [0.8235294  0.62352943 0.54901963]]\n\n  [[0.6784314  0.5137255  0.42745098]\n   [0.6784314  0.5137255  0.42745098]\n   [0.6745098  0.50980395 0.42352942]\n   ...\n   [0.80784315 0.61960787 0.5411765 ]\n   [0.81960785 0.61960787 0.54509807]\n   [0.81960785 0.61960787 0.54509807]]]\n\n\n [[[0.7176471  0.4509804  0.3764706 ]\n   [0.7176471  0.4509804  0.3764706 ]\n   [0.72156864 0.45490196 0.38039216]\n   ...\n   [0.9764706  0.77254903 0.7294118 ]\n   [0.9647059  0.7607843  0.7176471 ]\n   [0.9490196  0.74509805 0.7019608 ]]\n\n  [[0.7137255  0.44705883 0.37254903]\n   [0.7137255  0.44705883 0.37254903]\n   [0.7137255  0.44705883 0.37254903]\n   ...\n   [0.972549   0.76862746 0.7254902 ]\n   [0.9607843  0.75686276 0.7137255 ]\n   [0.9490196  0.74509805 0.7019608 ]]\n\n  [[0.7137255  0.44705883 0.3647059 ]\n   [0.7137255  0.44705883 0.3647059 ]\n   [0.70980394 0.44313726 0.36078432]\n   ...\n   [0.9607843  0.75686276 0.7137255 ]\n   [0.9529412  0.7490196  0.7058824 ]\n   [0.94509804 0.7411765  0.69803923]]\n\n  ...\n\n  [[0.8235294  0.5411765  0.44705883]\n   [0.827451   0.54509807 0.4509804 ]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [0.98039216 0.87058824 0.85882354]\n   [0.98039216 0.87058824 0.85882354]\n   [0.9764706  0.8666667  0.85490197]]\n\n  [[0.8392157  0.5568628  0.4627451 ]\n   [0.8352941  0.5529412  0.45882353]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [0.99607843 0.8862745  0.8745098 ]\n   [0.99607843 0.8862745  0.8745098 ]\n   [0.9882353  0.8784314  0.8666667 ]]\n\n  [[0.85490197 0.57254905 0.47843137]\n   [0.84313726 0.56078434 0.46666667]\n   [0.8352941  0.5529412  0.45882353]\n   ...\n   [1.         0.8980392  0.8862745 ]\n   [1.         0.89411765 0.88235295]\n   [0.99607843 0.8862745  0.8745098 ]]]\n\n\n [[[0.77254903 0.6509804  0.6313726 ]\n   [0.78039217 0.65882355 0.6392157 ]\n   [0.8039216  0.6745098  0.654902  ]\n   ...\n   [0.8666667  0.74509805 0.6313726 ]\n   [0.8745098  0.7529412  0.6392157 ]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  [[0.77254903 0.6431373  0.62352943]\n   [0.7921569  0.6627451  0.6431373 ]\n   [0.8117647  0.68235296 0.6627451 ]\n   ...\n   [0.8666667  0.74509805 0.6313726 ]\n   [0.8745098  0.7529412  0.6392157 ]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  [[0.8039216  0.6666667  0.65882355]\n   [0.827451   0.6901961  0.68235296]\n   [0.85882354 0.70980394 0.7058824 ]\n   ...\n   [0.8627451  0.7411765  0.627451  ]\n   [0.87058824 0.7490196  0.63529414]\n   [0.8784314  0.75686276 0.6431373 ]]\n\n  ...\n\n  [[0.9529412  0.88235295 0.84313726]\n   [0.95686275 0.8862745  0.84705883]\n   [0.94509804 0.8745098  0.8352941 ]\n   ...\n   [0.6901961  0.50980395 0.3764706 ]\n   [0.70980394 0.5294118  0.3882353 ]\n   [0.7254902  0.54509807 0.40392157]]\n\n  [[0.95686275 0.8784314  0.84313726]\n   [0.95686275 0.8784314  0.84313726]\n   [0.94509804 0.8666667  0.83137256]\n   ...\n   [0.69411767 0.5137255  0.38039216]\n   [0.70980394 0.5294118  0.3882353 ]\n   [0.7176471  0.5372549  0.39607844]]\n\n  [[0.95686275 0.8784314  0.84313726]\n   [0.95686275 0.8784314  0.84313726]\n   [0.94509804 0.8666667  0.83137256]\n   ...\n   [0.69411767 0.5137255  0.38039216]\n   [0.7058824  0.5254902  0.38431373]\n   [0.7137255  0.53333336 0.39215687]]]\n\n\n ...\n\n\n [[[0.5882353  0.5372549  0.5137255 ]\n   [0.5882353  0.5372549  0.5137255 ]\n   [0.5882353  0.5372549  0.5137255 ]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  [[0.58431375 0.53333336 0.50980395]\n   [0.58431375 0.53333336 0.50980395]\n   [0.58431375 0.53333336 0.50980395]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  [[0.5803922  0.52156866 0.5019608 ]\n   [0.58431375 0.5254902  0.5058824 ]\n   [0.5803922  0.5294118  0.5058824 ]\n   ...\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]\n   [0.654902   0.67058825 0.68235296]]\n\n  ...\n\n  [[0.47058824 0.40392157 0.3647059 ]\n   [0.47058824 0.40784314 0.35686275]\n   [0.47058824 0.40392157 0.3647059 ]\n   ...\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]]\n\n  [[0.4627451  0.4        0.34901962]\n   [0.46666667 0.40392157 0.34509805]\n   [0.47058824 0.40784314 0.35686275]\n   ...\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]\n   [0.5411765  0.50980395 0.46666667]]\n\n  [[0.45882353 0.39607844 0.3372549 ]\n   [0.4627451  0.4        0.34117648]\n   [0.47058824 0.40784314 0.34901962]\n   ...\n   [0.54509807 0.5137255  0.47058824]\n   [0.54509807 0.5137255  0.47058824]\n   [0.54509807 0.5137255  0.47058824]]]\n\n\n [[[0.49411765 0.36078432 0.22352941]\n   [0.5019608  0.36862746 0.23137255]\n   [0.5058824  0.37254903 0.23529412]\n   ...\n   [0.47058824 0.35686275 0.22352941]\n   [0.46666667 0.3529412  0.22745098]\n   [0.5647059  0.4627451  0.33333334]]\n\n  [[0.49019608 0.35686275 0.21960784]\n   [0.49803922 0.3647059  0.22745098]\n   [0.5019608  0.36862746 0.23137255]\n   ...\n   [0.46666667 0.3529412  0.21960784]\n   [0.4627451  0.34901962 0.22352941]\n   [0.56078434 0.45882353 0.32941177]]\n\n  [[0.4862745  0.3529412  0.21568628]\n   [0.49019608 0.35686275 0.21960784]\n   [0.49803922 0.3647059  0.22745098]\n   ...\n   [0.4627451  0.34901962 0.21568628]\n   [0.45882353 0.34509805 0.21960784]\n   [0.5568628  0.45490196 0.3254902 ]]\n\n  ...\n\n  [[0.6117647  0.4627451  0.30980393]\n   [0.6117647  0.4627451  0.30980393]\n   [0.6156863  0.46666667 0.3137255 ]\n   ...\n   [0.5921569  0.4745098  0.33333334]\n   [0.60784316 0.49411765 0.36862746]\n   [0.68235296 0.5686275  0.44313726]]\n\n  [[0.6039216  0.45490196 0.3019608 ]\n   [0.6039216  0.45490196 0.3019608 ]\n   [0.60784316 0.45882353 0.30588236]\n   ...\n   [0.5882353  0.47058824 0.32941177]\n   [0.6039216  0.49019608 0.3647059 ]\n   [0.6784314  0.5647059  0.4392157 ]]\n\n  [[0.5921569  0.44313726 0.2901961 ]\n   [0.59607846 0.44705883 0.29411766]\n   [0.6039216  0.45490196 0.3019608 ]\n   ...\n   [0.5882353  0.47058824 0.32941177]\n   [0.6        0.4862745  0.36078432]\n   [0.6745098  0.56078434 0.43529412]]]\n\n\n [[[0.8117647  0.5294118  0.5764706 ]\n   [0.827451   0.54509807 0.5921569 ]\n   [0.8509804  0.5686275  0.6156863 ]\n   ...\n   [0.28235295 0.35686275 0.38431373]\n   [0.27450982 0.3647059  0.39607844]\n   [0.26666668 0.3647059  0.39215687]]\n\n  [[0.84313726 0.56078434 0.60784316]\n   [0.84705883 0.5647059  0.6117647 ]\n   [0.85490197 0.57254905 0.61960787]\n   ...\n   [0.27058825 0.3372549  0.36862746]\n   [0.2627451  0.34117648 0.3764706 ]\n   [0.24705882 0.3372549  0.36862746]]\n\n  [[0.8666667  0.5921569  0.63529414]\n   [0.8627451  0.5882353  0.6313726 ]\n   [0.8627451  0.5803922  0.627451  ]\n   ...\n   [0.3019608  0.34117648 0.3764706 ]\n   [0.2784314  0.33333334 0.36862746]\n   [0.2627451  0.32941177 0.36078432]]\n\n  ...\n\n  [[0.87058824 0.654902   0.6431373 ]\n   [0.87058824 0.654902   0.6431373 ]\n   [0.8745098  0.65882355 0.64705884]\n   ...\n   [0.90588236 0.627451   0.61960787]\n   [0.90588236 0.6156863  0.6117647 ]\n   [0.9098039  0.6117647  0.6117647 ]]\n\n  [[0.8745098  0.64705884 0.6392157 ]\n   [0.88235295 0.654902   0.64705884]\n   [0.8901961  0.6627451  0.654902  ]\n   ...\n   [0.9098039  0.6313726  0.62352943]\n   [0.8980392  0.60784316 0.6039216 ]\n   [0.89411765 0.6039216  0.6       ]]\n\n  [[0.8745098  0.64705884 0.6392157 ]\n   [0.8862745  0.65882355 0.6509804 ]\n   [0.8980392  0.67058825 0.6627451 ]\n   ...\n   [0.90588236 0.6313726  0.62352943]\n   [0.89411765 0.6039216  0.6       ]\n   [0.8862745  0.59607846 0.5921569 ]]]])."
     ]
    }
   ],
   "source": [
    "# run inference\n",
    "trials = 500\n",
    "inference_times = []\n",
    "\n",
    "# Load the SavedModel directly\n",
    "model = tf.saved_model.load('../best_model/model1/best_f1score_fold')\n",
    "\n",
    "# # Load the SavedModel using TFSMLayer, treating it as a Keras layer\n",
    "# model_layer = tf.keras.layers.TFSMLayer('../best_model/model1/best_f1score_fold', call_endpoint='serving_default')\n",
    "\n",
    "# # Wrap the TFSMLayer in a Sequential model for inference\n",
    "# model = tf.keras.Sequential([model_layer])\n",
    "\n",
    "# # Wrap the SavedModel for inference\n",
    "# def model_call(inputs):\n",
    "#     return model.signatures[\"serving_default\"](inputs)[\"output_0\"]\n",
    "\n",
    "# # Create a Keras Sequential model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),  # Replace with your model's input shape\n",
    "#     tf.keras.layers.Lambda(model_call)\n",
    "# ])\n",
    "\n",
    "print(f\"Running {trials} inference trials on {len(X_test)} test images...\")\n",
    "for i in range(trials):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Run inference for a batch of images\n",
    "    # saved_model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    model.signatures[\"serving_default\"](X_test)  # Perform inference directly\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    inference_time = end_time - start_time\n",
    "    inference_times.append(inference_time)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        avg_inference = np.mean(inference_times)  # Average inference time per trial\n",
    "        print(f\"Step {i}: average inference time = {avg_inference:.6f} seconds\")\n",
    "        \n",
    "    tf.keras.backend.clear_session()\n",
    "        \n",
    "# Compute throughput (images per second)\n",
    "# total_time = np.sum(inference_times)\n",
    "# throughput = (trials * len(X_test)) / total_time\n",
    "# print(f\"Throughput: {throughput:.2f} images/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference on Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, X_test, batch_size, trials=500):\n",
    "    inference_times = []\n",
    "\n",
    "    print(f\"Running {trials} inference trials on {len(X_test)} test images...\")\n",
    "    num_batches = len(X_test) // batch_size\n",
    "\n",
    "    for i in range(trials):\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Loop over the batches of X_test\n",
    "        for j in range(num_batches):\n",
    "            batch_start = j * batch_size\n",
    "            batch_end = (j + 1) * batch_size\n",
    "            batch_images = X_test[batch_start:batch_end]  # Get a batch of images\n",
    "            inputs = tf.convert_to_tensor(batch_images) # Ensure the batch is in tensor format\n",
    "\n",
    "            # Run inference for the batch\n",
    "            model.signatures[\"serving_default\"](inputs)  # Perform inference directly\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        inference_time = end_time - start_time\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            avg_inference = np.mean(inference_times)  # Average inference time per trial\n",
    "            print(f\"Step {i}: average inference time = {avg_inference:.6f} seconds\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    # Compute throughput (images per second)\n",
    "    total_time = np.sum(inference_times)\n",
    "    throughput = (trials * len(X_test)) / total_time\n",
    "    return np.mean(inference_times), throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Metrics on Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load models\n",
    "original_model = tf.saved_model.load('../best_model/model1/best_f1score_fold')\n",
    "# optimized_model = tf.saved_model.load('../tensorRT_model/test_INT8')\n",
    "\n",
    "def measure_metrics(model, X_test, Y_test, batch_size):\n",
    "    print(f\"Evaluating metrics on {len(X_test)} test images...\")\n",
    "    \n",
    "    num_batches = len(X_test) // batch_size\n",
    "    all_predicted_classes = []\n",
    "\n",
    "    # Loop over the test dataset in batches\n",
    "    for j in range(num_batches):\n",
    "        batch_start = j * batch_size\n",
    "        batch_end = (j + 1) * batch_size\n",
    "        batch_images = X_test[batch_start:batch_end]\n",
    "        inputs = tf.convert_to_tensor(batch_images)  # Convert batch to tensor\n",
    "\n",
    "        # Run predictions\n",
    "        result = model.signatures[\"serving_default\"](inputs)  # Inference\n",
    "        prediction_logits = result[\"output_0\"].numpy()  # Extract logits\n",
    "        probabilities = tf.nn.sigmoid(prediction_logits).numpy()  # Apply sigmoid\n",
    "        predicted_classes = (probabilities > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "        # Collect predictions\n",
    "        all_predicted_classes.extend(predicted_classes)\n",
    "\n",
    "    # Handle any remaining images that don't fit evenly in batches\n",
    "    remaining_samples = len(X_test) % batch_size\n",
    "    if remaining_samples > 0:\n",
    "        batch_images = X_test[-remaining_samples:]\n",
    "        inputs = tf.convert_to_tensor(batch_images)\n",
    "        result = model.signatures[\"serving_default\"](inputs)\n",
    "        prediction_logits = result[\"output_0\"].numpy()\n",
    "        probabilities = tf.nn.sigmoid(prediction_logits).numpy()\n",
    "        predicted_classes = (probabilities > 0.5).astype(int)\n",
    "        all_predicted_classes.extend(predicted_classes)\n",
    "\n",
    "    # Flatten predictions and labels to ensure they are 1D arrays\n",
    "    all_predicted_classes = np.array(all_predicted_classes).flatten()\n",
    "    Y_test = np.array(Y_test).flatten()\n",
    "\n",
    "    # Ensure the number of predictions matches the number of ground truth labels\n",
    "    if len(all_predicted_classes) != len(Y_test):\n",
    "        raise ValueError(f\"Number of predicted classes ({len(all_predicted_classes)}) \"\n",
    "                         f\"does not match the number of ground truth labels ({len(Y_test)}).\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(Y_test, all_predicted_classes)\n",
    "    precision = precision_score(Y_test, all_predicted_classes, average=\"binary\")\n",
    "    recall = recall_score(Y_test, all_predicted_classes, average=\"binary\")\n",
    "    f1 = f1_score(Y_test, all_predicted_classes, average=\"binary\")\n",
    "\n",
    "    # Output metrics\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "\n",
    "    print(f\"Metrics: {metrics_dict}\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Size: 18.90 MB\n",
      "Optimized Model Size: 9.01 MB\n",
      "Compression Ratio: 2.10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "original_size = get_model_size(saved_model_dir)\n",
    "optimized_size = get_model_size(optimized_model_dir)\n",
    "print(f\"Original Model Size: {original_size:.2f} MB\")\n",
    "print(f\"Optimized Model Size: {optimized_size:.2f} MB\")\n",
    "print(f\"Compression Ratio: {original_size / optimized_size:.2f}\")\n",
    "\n",
    "# Int8 compression ratio: 1.50, speedup factor: 2.50\n",
    "# Load models\n",
    "original_model = tf.saved_model.load(saved_model_dir)\n",
    "optimized_model = tf.saved_model.load(optimized_model_dir)\n",
    "\n",
    "# # Measure inference time for both models\n",
    "# original_avg_time, original_throughput = measure_inference_time(\n",
    "#     original_model, X_test, batch_size=batch_size\n",
    "# )\n",
    "# optimized_avg_time, optimized_throughput = measure_inference_time(\n",
    "#     optimized_model, X_test, batch_size=batch_size\n",
    "# )\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nResults:\")\n",
    "# print(f\"Original Model - Average Inference Time: {original_avg_time:.6f} seconds\")\n",
    "# print(f\"Original Model - Throughput: {original_throughput:.2f} images/second\")\n",
    "# print(f\"Optimized Model - Average Inference Time: {optimized_avg_time:.6f} seconds\")\n",
    "# print(f\"Optimized Model - Throughput: {optimized_throughput:.2f} images/second\")\n",
    "\n",
    "# # Compute speedup\n",
    "# speedup_factor = original_avg_time / optimized_avg_time\n",
    "# print(f\"\\nSpeedup Factor: {speedup_factor:.2f}\")\n",
    "\n",
    "# Measure metrics for both models\n",
    "original_metrics = measure_metrics(\n",
    "    model=original_model, \n",
    "    X_test=X_test, \n",
    "    Y_test=Y_test, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "optimized_metrics = measure_metrics(\n",
    "    model=optimized_model, \n",
    "    X_test=X_test, \n",
    "    Y_test=Y_test, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"\\nComparison of Metrics:\")\n",
    "print(f\"{'Metric':<12} {'Original Model':<15} {'Optimized Model':<15}\")\n",
    "print(\"-\" * 42)\n",
    "for metric in original_metrics.keys():\n",
    "    print(f\"{metric:<12} {original_metrics[metric]:<15.4f} {optimized_metrics[metric]:<15.4f}\")\n",
    "\n",
    "# Highlighting model differences\n",
    "print(\"\\nSummary of Changes:\")\n",
    "for metric in original_metrics.keys():\n",
    "    change = optimized_metrics[metric] - original_metrics[metric]\n",
    "    print(f\"{metric:<12} Change: {change:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the optimized TensorRT model\n",
    "# saved_model_loaded = tf.saved_model.load('path/to/save/tensorrt_model')\n",
    "# infer = saved_model_loaded.signatures['serving_default']\n",
    "\n",
    "# # Example input data (adjust as per your model's input requirements)\n",
    "# input_tensor = tf.convert_to_tensor(your_input_data)\n",
    "\n",
    "# # Run inference\n",
    "# output = infer(input_tensor)\n",
    "\n",
    "# # Process output as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
