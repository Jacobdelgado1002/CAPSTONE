{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from numpy import concatenate, array, expand_dims, mean, sum\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"../data/Monkeypox_Data/Original_Images\")    # points to the folder containing the images that will be used for training\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32         # size of the batch that will be fed to model\n",
    "img_height = 224        # input image height\n",
    "img_width = 224         # input image width\n",
    "test_size = 0.2\n",
    "\n",
    "# Load dataset without splitting\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_root,                                  # loads images from the data_root directory\n",
    "    image_size=(img_height, img_width),         # resizes all images to (224, 224) pixels\n",
    "    batch_size=batch_size,                      # set the batch size\n",
    "    shuffle=False                                # shufle data when loaded\n",
    ")\n",
    "\n",
    "# normalization_layer = layers.Rescaling(1./255)\n",
    "# dataset = dataset.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batches, labels = [], []\n",
    "for image_batch, label_batch in dataset:\n",
    "    image_batches.append(image_batch)\n",
    "    labels.append(label_batch)\n",
    "\n",
    "image_batches = concatenate(image_batches) # Flatten batches to get all images\n",
    "labels = concatenate(labels)               # Flatten batches to get all labels  \n",
    "print(f\"Total Images: {image_batches.shape[0]} \\nTotal Labels: {labels.shape[0]}\")\n",
    "\n",
    "# Split the data into test subset for benchmarking\n",
    "_, X_test, _, Y_test = train_test_split(image_batches, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "\n",
    "X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model_dir):\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(model_dir):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, X_test, batch_size, trials=50):\n",
    "    inference_times = []\n",
    "\n",
    "    num_batches = len(X_test) // batch_size\n",
    "\n",
    "    # Warm-up phase (Run a few trials to initialize the model)\n",
    "    print(f\"Running {10} warm-up trials to initialize the model...\")\n",
    "    for _ in range(10):\n",
    "        for j in range(num_batches):\n",
    "            batch_start = j * batch_size\n",
    "            batch_end = (j + 1) * batch_size\n",
    "            batch_images = X_test[batch_start:batch_end]  # Get a batch of images\n",
    "            dummy_input = tf.convert_to_tensor(batch_images) # Ensure the batch is in tensor format\n",
    "            model.signatures[\"serving_default\"](tf.convert_to_tensor(dummy_input))\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    print(f\"Running {trials} inference trials on {len(X_test)} test images...\")\n",
    "    for i in range(trials):\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Loop over the batches of X_test\n",
    "        for j in range(num_batches):\n",
    "            batch_start = j * batch_size\n",
    "            batch_end = (j + 1) * batch_size\n",
    "            batch_images = X_test[batch_start:batch_end]  # Get a batch of images\n",
    "            inputs = tf.convert_to_tensor(batch_images) # Ensure the batch is in tensor format\n",
    "\n",
    "            # Run inference for the batch\n",
    "            model.signatures[\"serving_default\"](inputs)  # Perform inference directly\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        inference_time = end_time - start_time\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            avg_inference = mean(inference_times)  # Average inference time per trial\n",
    "            print(f\"Step {i}: average inference time = {avg_inference:.6f} seconds\")\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Compute throughput (images per second)\n",
    "    total_time = sum(inference_times)\n",
    "    throughput = (trials * len(X_test)) / total_time\n",
    "    return mean(inference_times), throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def measure_metrics(model, X_test, Y_test, batch_size):\n",
    "    print(f\"Evaluating metrics on {len(X_test)} test images...\")\n",
    "    \n",
    "    num_batches = len(X_test) // batch_size\n",
    "    all_predicted_classes = []\n",
    "\n",
    "    # Loop over the test dataset in batches\n",
    "    for j in range(num_batches):\n",
    "        batch_start = j * batch_size\n",
    "        batch_end = (j + 1) * batch_size\n",
    "        batch_images = X_test[batch_start:batch_end]\n",
    "        inputs = tf.convert_to_tensor(batch_images)  # Convert batch to tensor\n",
    "\n",
    "        # Run predictions\n",
    "        result = model.signatures[\"serving_default\"](inputs)  # Inference\n",
    "        prediction_logits = result[\"output_0\"].numpy()  # Extract logits\n",
    "        probabilities = tf.nn.sigmoid(prediction_logits).numpy()  # Apply sigmoid\n",
    "        predicted_classes = (probabilities > 0.5).astype(int)  # Threshold for binary classification\n",
    "\n",
    "        # Collect predictions\n",
    "        all_predicted_classes.extend(predicted_classes)\n",
    "\n",
    "    # Handle any remaining images that don't fit evenly in batches\n",
    "    remaining_samples = len(X_test) % batch_size\n",
    "    if remaining_samples > 0:\n",
    "        batch_images = X_test[-remaining_samples:]\n",
    "        inputs = tf.convert_to_tensor(batch_images)\n",
    "        result = model.signatures[\"serving_default\"](inputs)\n",
    "        prediction_logits = result[\"output_0\"].numpy()\n",
    "        probabilities = tf.nn.sigmoid(prediction_logits).numpy()\n",
    "        predicted_classes = (probabilities > 0.5).astype(int)\n",
    "        all_predicted_classes.extend(predicted_classes)\n",
    "\n",
    "    # Flatten predictions and labels to ensure they are 1D arrays\n",
    "    all_predicted_classes = array(all_predicted_classes).flatten()\n",
    "    Y_test = array(Y_test).flatten()\n",
    "\n",
    "    # Ensure the number of predictions matches the number of ground truth labels\n",
    "    if len(all_predicted_classes) != len(Y_test):\n",
    "        raise ValueError(f\"Number of predicted classes ({len(all_predicted_classes)}) \"\n",
    "                         f\"does not match the number of ground truth labels ({len(Y_test)}).\")\n",
    "\n",
    "    # Calculate metricsimport numpy as np\n",
    "\n",
    "    accuracy = accuracy_score(Y_test, all_predicted_classes)\n",
    "    precision = precision_score(Y_test, all_predicted_classes, average=\"binary\")\n",
    "    recall = recall_score(Y_test, all_predicted_classes, average=\"binary\")\n",
    "    f1 = f1_score(Y_test, all_predicted_classes, average=\"binary\")\n",
    "\n",
    "    # Output metrics\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "    }\n",
    "\n",
    "    print(f\"Metrics: {metrics_dict}\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for results\n",
    "model_sizes = {}\n",
    "average_times = {}\n",
    "throughputs = {}\n",
    "metrics = {}\n",
    "\n",
    "file_path = '../tensorRT_model/saved_values/values.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load, dump\n",
    "\n",
    "def save_or_update_json(new_data, file_path):\n",
    "    \"\"\"\n",
    "    Save new data to a JSON file. If the file exists, load its content,\n",
    "    update it with the new data, and save it back.\n",
    "    \"\"\"\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            existing_data = load(json_file)  # Load existing data\n",
    "    else:\n",
    "        existing_data = {}  # Start with an empty dictionary if file doesn't exist\n",
    "\n",
    "    # Update existing data with the new data\n",
    "    for key, value in new_data.items():\n",
    "        if key in existing_data:\n",
    "            existing_data[key].update(value)  # Merge dictionaries\n",
    "        else:\n",
    "            existing_data[key] = value  # Add new key\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        dump(existing_data, json_file, indent=4)\n",
    "    print(f\"Data updated and saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Original\"\n",
    "model = tf.saved_model.load('../best_model/model1/best_f1score_fold')\n",
    "model_sizes[model_name] = get_model_size('../best_model/model1/best_f1score_fold')\n",
    "\n",
    "# Measure inference time and throughput\n",
    "print(f'Checking inference time for {model_name} model')\n",
    "avg_time, throughput = measure_inference_time(model, X_test, batch_size=batch_size)\n",
    "average_times[model_name] = avg_time\n",
    "throughputs[model_name] = throughput\n",
    "\n",
    "# Measure metrics\n",
    "metrics[model_name] = measure_metrics(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    Y_test=Y_test,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "new_data = {\n",
    "    \"model_sizes\": {model_name: model_sizes[model_name]},\n",
    "    \"average_times\": {model_name: average_times[model_name]},\n",
    "    \"throughputs\": {model_name: throughputs[model_name]},\n",
    "    \"metrics\": {model_name: metrics[model_name]},\n",
    "}\n",
    "\n",
    "save_or_update_json(new_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FP32\"\n",
    "model = tf.saved_model.load('../tensorRT_model/fp32')\n",
    "model_sizes[model_name] = get_model_size('../tensorRT_model/fp32')\n",
    "\n",
    "# Measure inference time and throughput\n",
    "print(f'Checking inference time for {model_name} model')\n",
    "avg_time, throughput = measure_inference_time(model, X_test, batch_size=batch_size)\n",
    "average_times[model_name] = avg_time\n",
    "throughputs[model_name] = throughput\n",
    "\n",
    "# Measure metrics\n",
    "metrics[model_name] = measure_metrics(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    Y_test=Y_test,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "new_data = {\n",
    "    \"model_sizes\": {model_name: model_sizes[model_name]},\n",
    "    \"average_times\": {model_name: average_times[model_name]},\n",
    "    \"throughputs\": {model_name: throughputs[model_name]},\n",
    "    \"metrics\": {model_name: metrics[model_name]},\n",
    "}\n",
    "\n",
    "save_or_update_json(new_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FP16\"\n",
    "model = tf.saved_model.load('../tensorRT_model/fp16')\n",
    "model_sizes[model_name] = get_model_size('../tensorRT_model/fp16')\n",
    "\n",
    "# Measure inference time and throughput\n",
    "print(f'Checking inference time for {model_name} model')\n",
    "avg_time, throughput = measure_inference_time(model, X_test, batch_size=batch_size)\n",
    "average_times[model_name] = avg_time\n",
    "throughputs[model_name] = throughput\n",
    "\n",
    "# Measure metrics\n",
    "metrics[model_name] = measure_metrics(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    Y_test=Y_test,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "new_data = {\n",
    "    \"model_sizes\": {model_name: model_sizes[model_name]},\n",
    "    \"average_times\": {model_name: average_times[model_name]},\n",
    "    \"throughputs\": {model_name: throughputs[model_name]},\n",
    "    \"metrics\": {model_name: metrics[model_name]},\n",
    "}\n",
    "\n",
    "save_or_update_json(new_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"INT8\"\n",
    "model = tf.saved_model.load('../tensorRT_model/int8')\n",
    "model_sizes[model_name] = get_model_size('../tensorRT_model/int8')\n",
    "\n",
    "# Measure inference time and throughput\n",
    "print(f'Checking inference time for {model_name} model')\n",
    "avg_time, throughput = measure_inference_time(model, X_test, batch_size=batch_size)\n",
    "average_times[model_name] = avg_time\n",
    "throughputs[model_name] = throughput\n",
    "\n",
    "# Measure metrics\n",
    "metrics[model_name] = measure_metrics(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    Y_test=Y_test,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "new_data = {\n",
    "    \"model_sizes\": {model_name: model_sizes[model_name]},\n",
    "    \"average_times\": {model_name: average_times[model_name]},\n",
    "    \"throughputs\": {model_name: throughputs[model_name]},\n",
    "    \"metrics\": {model_name: metrics[model_name]},\n",
    "}\n",
    "\n",
    "save_or_update_json(new_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate compression ratios and speedup factors\n",
    "compression_ratios = {\n",
    "    model: model_sizes[\"Original\"] / size\n",
    "    for model, size in model_sizes.items() if model != \"Original\"\n",
    "}\n",
    "\n",
    "speedup_factors = {\n",
    "    model: average_times[\"Original\"] / time\n",
    "    for model, time in average_times.items() if model != \"Original\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, melt\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    combined_data = load(json_file)\n",
    "\n",
    "# Access individual components\n",
    "model_sizes = combined_data.get(\"model_sizes\", {})\n",
    "average_times = combined_data.get(\"average_times\", {})\n",
    "throughputs = combined_data.get(\"throughputs\", {})\n",
    "metrics = combined_data.get(\"metrics\", {})\n",
    "\n",
    "# Example Data (Replace with your actual data)\n",
    "models = [\"Original\", \"FP32\", \"FP16\", \"INT8\"]\n",
    "model_sizes_data = [model_sizes[model] for model in models]\n",
    "throughputs_data = [throughputs[model] for model in models]\n",
    "average_times_data = [average_times[model] for model in models]\n",
    "\n",
    "# Dictionary to store metrics for all models\n",
    "metrics_data = {model: metrics[model] for model in models}\n",
    "\n",
    "# Prepare DataFrames for sizes and throughputs\n",
    "model_sizes_df = DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Size (MB)\": model_sizes_data\n",
    "})\n",
    "\n",
    "throughputs_df = DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Throughput (images/second)\": throughputs_data\n",
    "})\n",
    "\n",
    "average_times_df = DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Average Inference Time (Seconds)\": average_times_data\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot model sizes\n",
    "def graph_model_size(model_sizes):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.barplot(data=model_sizes, x=\"Model\", y=\"Size (MB)\", palette=\"muted\")\n",
    "    plt.title(\"Model Sizes Comparison\")\n",
    "    plt.ylabel(\"Size (MB)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    # Annotate each bar with its height\n",
    "    for p in ax.patches:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2,\n",
    "            p.get_height(),\n",
    "            f'{p.get_height():.2f}',\n",
    "            ha='center', va='bottom', fontsize=10\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../tensorRT_model/metrics/model_sizes.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot throughputs\n",
    "def graph_throughput(throughputs):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.barplot(data=throughputs, x=\"Model\", y=\"Throughput (images/second)\", palette=\"muted\")\n",
    "    plt.title(\"Model Throughput Comparison\")\n",
    "    plt.ylabel(\"Throughput (images/second)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    # Annotate each bar with its height\n",
    "    for p in ax.patches:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2,\n",
    "            p.get_height(),\n",
    "            f'{p.get_height():.2f}',\n",
    "            ha='center', va='bottom', fontsize=10\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../tensorRT_model/metrics/throughput_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_inference_times(average_times_df):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.barplot(data=average_times_df, x=\"Model\", y=\"Average Inference Time (Seconds)\", palette=\"muted\")\n",
    "    plt.title(\"Average Inference Time Comparison\")\n",
    "    plt.ylabel(\"Inference Time (Seconds)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    # Annotate each bar with its height\n",
    "    for p in ax.patches:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2,\n",
    "            p.get_height(),\n",
    "            f'{p.get_height():.2f}',\n",
    "            ha='center', va='bottom', fontsize=10\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../tensorRT_model/metrics/average_inference_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to compare metrics across models\n",
    "def graph_metrics(metrics_data):\n",
    "    # Convert metrics data into a DataFrame\n",
    "    metrics_df = DataFrame(metrics_data).T.reset_index()\n",
    "    metrics_df = metrics_df.rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "    # Melt for plotting\n",
    "    metrics_long = melt(\n",
    "        metrics_df, id_vars=[\"Model\"], \n",
    "        var_name=\"Metric\", value_name=\"Value\"\n",
    "    )\n",
    "\n",
    "    # Grouped Bar Chart for Metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=metrics_long, x=\"Metric\", y=\"Value\", hue=\"Model\", palette=\"muted\")\n",
    "    plt.title(\"Metrics Comparison\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.xlabel(\"Metric\")\n",
    "    # Annotate each bar with its height\n",
    "    for p in ax.patches:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2,\n",
    "            p.get_height(),\n",
    "            f'{p.get_height():.2f}',\n",
    "            ha='center', va='bottom', fontsize=10\n",
    "        )\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../tensorRT_model/metrics/metrics_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nDetailed Metric Changes:\")\n",
    "    print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model_size(model_sizes_df)  # Pass the DataFrame, not the dictionary\n",
    "graph_throughput(throughputs_df)  # Pass the DataFrame, not the dictionary\n",
    "graph_inference_times(average_times_df)\n",
    "graph_metrics(metrics_data)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
