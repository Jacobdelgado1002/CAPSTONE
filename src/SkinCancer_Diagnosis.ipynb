{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Skin Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(\"../data/Augmented_Images\")    # points to the folder containing the images that will be used for training\n",
    "\n",
    "# hyperparameters\n",
    "img_height = 224        # input image height\n",
    "img_width = 224         # input image width\n",
    "batch_size = 32         # size of the batch that will be fed to model\n",
    "\n",
    "# folds = the amount of folds that will be created for cross-validation\n",
    "# fine_tune_epochs = number of epochs after which we start fine-tuning\n",
    "# fine_tune_at = layer number where we start unfreezing layers\n",
    "\n",
    "# configurations that will be used in training\n",
    "configs = [\n",
    "    {\"learning_rate\": 0.001, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    # {\"learning_rate\": 0.001, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 3, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    # {\"learning_rate\": 0.001, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": True, \"fine_tune_epochs\": 25, \"fine_tune_at\": 152},\n",
    "    # {\"learning_rate\": 0.001, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 3, \"fine_tune\": True, \"fine_tune_epochs\": 25, \"fine_tune_at\": 152},\n",
    "]\n",
    "\n",
    "# Define the base path for saving models\n",
    "save_dir = \"../saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset without splitting\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_root,                                  # loads images from the data_root directory\n",
    "    image_size=(img_height, img_width),         # resizes all images to (224, 224) pixels\n",
    "    batch_size=batch_size,                      # set the batch size\n",
    "    shuffle=True                                # shufle data when loaded\n",
    ")\n",
    "\n",
    "class_names = np.array(dataset.class_names)     # get the class names for the data\n",
    "num_classes = len(class_names)                  # get the number of classes in the dataset\n",
    "\n",
    "# convert the dataset to a list of (image, label) pairs. This makes it easier to perform cross-validation\n",
    "image_paths, labels = [], []\n",
    "for image_batch, label_batch in dataset:\n",
    "    image_paths.extend(image_batch.numpy())\n",
    "    labels.extend(label_batch.numpy())\n",
    "\n",
    "image_paths = np.array(image_paths)             # convert to numpy array to facilitate training\n",
    "labels = np.array(labels)                       # convert to numpy array to facilitate training\n",
    "\n",
    "# Split the dataset into training/validation and test sets\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.10, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "def callbacks_setup(checkpoint_filepath):\n",
    "    # EarlyStopping callback configuration\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # monitor validation loss\n",
    "        patience=6,                # number of epochs with no improvement to stop training\n",
    "        mode = 'min',              # want to minimize what it being monitored \n",
    "        restore_best_weights=False # don't restore in EarlyStopping, handled by ModelCheckpoint\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,   # path to save weights\n",
    "        save_weights_only=True,         # only save weights instead of full model\n",
    "        monitor='val_loss',             # monitor validation loss\n",
    "        mode='min',                     # want to maximize what is being monitored\n",
    "        save_best_only=True             # save the best weights\n",
    "    )            \n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',      # monitor validation loss \n",
    "        factor=0.5,              # factor by which the learning rate will be reduced \n",
    "        patience=4,              # number of epochs with no improvement to stop training \n",
    "        mode='min',              # want to minimize what it being monitored \n",
    "        min_lr=1e-6              # lower bound on the learning rate \n",
    "    )            \n",
    "\n",
    "    return early_stopping, model_checkpoint, reduce_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def save_confusion_matrix(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                          class_names: List[str], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the confusion matrix for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels.\n",
    "        class_names (List[str]): List of class names corresponding to class indices.\n",
    "        save_path (str): Path to save the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix using sklearn\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    # Plot with adjustments\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Adjust figure size\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=12)\n",
    "    ax.set_ylabel(\"True label\", fontsize=12)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # Prevent labels from being cut off\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and close plot\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_loss_curve(history: Dict[str, Any], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the training and validation loss curves.\n",
    "\n",
    "    Args:\n",
    "        history (Dict[str, Any]): Dictionary containing training history (loss values).\n",
    "        save_path (str): Path to save the loss curve plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_roc_auc(true_labels: np.ndarray, predicted_probs: np.ndarray, class_names: list, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Plots and saves the ROC AUC curve for multi-class classification.\n",
    "    \n",
    "    Args:\n",
    "        true_labels (np.ndarray): True class labels.\n",
    "        predicted_probs (np.ndarray): Predicted class probabilities.\n",
    "        class_names (list): List of class names.\n",
    "        save_path (str, optional): Path to save the ROC curve plot. Defaults to None.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        fpr, tpr, _ = roc_curve(true_labels == i, predicted_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC AUC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def save_evaluation_metrics(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                            predicted_probs: np.ndarray, save_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for multi-class classification and saves a bar chart.\n",
    "    The metrics include accuracy, precision, recall, F1 score, and ROC AUC.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels.\n",
    "        predicted_probs (np.ndarray): Array of predicted probabilities (shape: [n_samples, n_classes]).\n",
    "        save_path (str): Path to save the evaluation metrics bar chart.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing computed metrics.\n",
    "    \"\"\"\n",
    "    # Calculate accuracy by comparing predicted and true labels\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    # Compute macro-averaged metrics for multi-class classification\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    # For ROC AUC, first binarize the true labels to one-hot encoding\n",
    "    n_classes = predicted_probs.shape[1]\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=list(range(n_classes)))\n",
    "    # Compute ROC AUC with a one-vs-rest approach and macro average\n",
    "    roc_auc = roc_auc_score(true_labels_binarized, predicted_probs, multi_class='ovr', average='macro')\n",
    "\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Sensitivity (Recall)\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "    # Plot metrics as a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(metrics.keys(), metrics.values(), \n",
    "                   color=['darkturquoise', 'sandybrown', 'hotpink', 'limegreen', 'mediumpurple'])\n",
    "    # Annotate each bar with its value\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.4f}', ha='center', va='bottom')\n",
    "    plt.title(\"Model Evaluation Metrics\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_classification_report(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                               class_names: List[str], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the classification report to a text file for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels.\n",
    "        class_names (List[str]): List of class names.\n",
    "        save_path (str): Path to save the classification report.\n",
    "    \"\"\"\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=class_names, digits=4)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "def calculate_metrics(true_labels: np.ndarray, predictions: np.ndarray) -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculates evaluation metrics for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predictions (np.ndarray): Array of predicted probabilities (shape: [n_samples, n_classes]).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float, float]: A tuple containing accuracy, precision, recall, \n",
    "            F1 score, and ROC AUC score.\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to predicted class labels using argmax\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    # Binarize true labels for ROC AUC calculation\n",
    "    n_classes = predictions.shape[1]\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=list(range(n_classes)))\n",
    "    auc = roc_auc_score(true_labels_binarized, predictions, multi_class='ovr', average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "def save_best_model_visuals(history: tf.keras.callbacks.History, model: tf.keras.Model, \n",
    "                              val_ds: tf.data.Dataset, class_names: List[str], \n",
    "                              weights_path: str, fold: int) -> None:\n",
    "    \"\"\"\n",
    "    Generates and saves evaluation visuals including confusion matrix, loss curve, evaluation \n",
    "    metrics bar chart, and classification report for the best performing model in a given fold.\n",
    "\n",
    "    Args:\n",
    "        history (tf.keras.callbacks.History): Training history object.\n",
    "        model (tf.keras.Model): Trained model.\n",
    "        val_ds (tf.data.Dataset): Validation dataset.\n",
    "        class_names (List[str]): List of class names.\n",
    "        weights_path (str): Directory path to save visuals.\n",
    "        fold (int): Current fold number.\n",
    "    \"\"\"\n",
    "    # Generate predictions (predicted probabilities) for the validation set\n",
    "    val_predictions = model.predict(val_ds)\n",
    "    # Convert predicted probabilities to class labels using argmax\n",
    "    val_predicted_ids = np.argmax(val_predictions, axis=1)\n",
    "    # Concatenate true labels from the validation dataset\n",
    "    true_labels = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    confusion_matrix_path = os.path.join(weights_path, f\"confusion_matrix_fold{fold}.png\")\n",
    "    save_confusion_matrix(true_labels, val_predicted_ids, class_names, confusion_matrix_path)\n",
    "\n",
    "    # Save the loss curve using the training history\n",
    "    loss_curve_path = os.path.join(weights_path, f\"loss_curve_fold{fold}.png\")\n",
    "    save_loss_curve(history.history, loss_curve_path)\n",
    "\n",
    "    # Save the roc auc curve using the training history\n",
    "    roc_auc_curve_path = os.path.join(weights_path, f\"roc_auc_curve_fold{fold}.png\")\n",
    "    save_roc_auc(true_labels, val_predictions, class_names, roc_auc_curve_path)\n",
    "\n",
    "    # Save evaluation metrics bar chart (passing predicted probabilities for ROC AUC calculation)\n",
    "    metrics_bar_chart_path = os.path.join(weights_path, f\"evaluation_metrics_fold{fold}.png\")\n",
    "    save_evaluation_metrics(true_labels, val_predicted_ids, val_predictions, metrics_bar_chart_path)\n",
    "\n",
    "    # Save the classification report as a text file\n",
    "    classification_report_path = os.path.join(weights_path, f\"classification_report_fold{fold}.txt\")\n",
    "    save_classification_report(true_labels, val_predicted_ids, class_names, classification_report_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training/validation set for hyperparameter tuning\n",
    "# train_images_tuning, val_images_tuning, train_labels_tuning, val_labels_tuning = train_test_split(\n",
    "#     image_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "# )\n",
    "\n",
    "# # Define the hypermodel for hyperparameter tuning\n",
    "# def build_model(hp):\n",
    "#     base_model = tf.keras.applications.MobileNetV2(\n",
    "#         input_shape=(img_height, img_width, 3),\n",
    "#         include_top=False,\n",
    "#         weights='imagenet'\n",
    "#     )\n",
    "#     base_model.trainable = False  # Freeze layers initially\n",
    "    \n",
    "#     model = Sequential([\n",
    "#         base_model,\n",
    "#         layers.GlobalAveragePooling2D(),\n",
    "#         layers.Dense(num_classes)\n",
    "#     ])\n",
    "\n",
    "#     # Tune hyperparameters\n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "#     optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "\n",
    "#     if optimizer == 'adam':\n",
    "#         opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     else:\n",
    "#         opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=opt,\n",
    "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Set up the tuner for hyperparameter tuning\n",
    "# tuner = kt.RandomSearch(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',  # Optimize for validation accuracy\n",
    "#     max_trials=10,             # Try 10 different hyperparameter combinations\n",
    "#     executions_per_trial=1,    # Run each combination once\n",
    "#     directory='hyperparameter_tuning',\n",
    "#     project_name='best_hyperparams_tuning'\n",
    "# )\n",
    "\n",
    "# # Prepare TensorFlow datasets for training and validation\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images_tuning, train_labels_tuning)).batch(batch_size)\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((val_images_tuning, val_labels_tuning)).batch(batch_size)\n",
    "\n",
    "# # Perform the hyperparameter search on the validation set\n",
    "# tuner.search(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "# # Get the best hyperparameters after the search\n",
    "# best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(f\"Best Hyperparameters: {best_hyperparams.values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and compile the model\n",
    "def create_model(num_classes, config, fine_tune=None):\n",
    "    # if you are not fine tuning the model, instantiate a new model \n",
    "    if(fine_tune == False):         \n",
    "        # instantiate mobilenet (contains 154 layers)\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=(img_height, img_width, 3),     # set the input it will receive\n",
    "            include_top=False,                          # do not include top layer to perform transfer learning\n",
    "            weights='imagenet'                          # load weights from imagenet dataset\n",
    "        )\n",
    "        base_model.trainable = False                    # Freeze the base model\n",
    "        \n",
    "        # add a layer in order to perform classification on our dataset\n",
    "        model = Sequential([\n",
    "            base_model,                         # use base_model as the start of your model\n",
    "            layers.GlobalAveragePooling2D(),    # add a final layer to perform classification\n",
    "            layers.Dense(num_classes)           # set the number of possible prediction to the num of classes in dataset\n",
    "        ])\n",
    "        \n",
    "    # select optimizer and learning rate based on configuration\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {config['optimizer']}\")\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fine tune model by unfreezing the layers after the first fine_tune_at layers\n",
    "def fine_tune_model(base_model, fine_tune_at):\n",
    "    # Unfreeze the layers starting from fine_tune_at index\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/1 with config: {'learning_rate': 0.001, 'optimizer': 'adam', 'epochs': 50, 'save_metrics': True, 'folds': 5, 'fine_tune': False, 'fine_tune_epochs': 25, 'fine_tune_at': 150}\n",
      "\n",
      "Fold 1/5...\n",
      "Training with frozen base layers for 50 epochs...\n",
      "Epoch 1/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 243ms/step - accuracy: 0.5861 - loss: 1.1096 - val_accuracy: 0.7651 - val_loss: 0.6243 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 229ms/step - accuracy: 0.8055 - loss: 0.5389 - val_accuracy: 0.8129 - val_loss: 0.5182 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 224ms/step - accuracy: 0.8503 - loss: 0.4265 - val_accuracy: 0.8266 - val_loss: 0.4678 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 221ms/step - accuracy: 0.8768 - loss: 0.3641 - val_accuracy: 0.8350 - val_loss: 0.4380 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 217ms/step - accuracy: 0.8921 - loss: 0.3226 - val_accuracy: 0.8455 - val_loss: 0.4182 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 221ms/step - accuracy: 0.9067 - loss: 0.2919 - val_accuracy: 0.8529 - val_loss: 0.4042 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 228ms/step - accuracy: 0.9134 - loss: 0.2678 - val_accuracy: 0.8607 - val_loss: 0.3940 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 227ms/step - accuracy: 0.9226 - loss: 0.2480 - val_accuracy: 0.8639 - val_loss: 0.3862 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 222ms/step - accuracy: 0.9281 - loss: 0.2313 - val_accuracy: 0.8686 - val_loss: 0.3802 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 220ms/step - accuracy: 0.9354 - loss: 0.2169 - val_accuracy: 0.8718 - val_loss: 0.3755 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 221ms/step - accuracy: 0.9399 - loss: 0.2042 - val_accuracy: 0.8760 - val_loss: 0.3719 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 221ms/step - accuracy: 0.9441 - loss: 0.1928 - val_accuracy: 0.8786 - val_loss: 0.3690 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 260ms/step - accuracy: 0.9482 - loss: 0.1827 - val_accuracy: 0.8770 - val_loss: 0.3668 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 385ms/step - accuracy: 0.9511 - loss: 0.1734 - val_accuracy: 0.8807 - val_loss: 0.3653 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 376ms/step - accuracy: 0.9538 - loss: 0.1650 - val_accuracy: 0.8823 - val_loss: 0.3642 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 376ms/step - accuracy: 0.9580 - loss: 0.1573 - val_accuracy: 0.8828 - val_loss: 0.3636 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 375ms/step - accuracy: 0.9603 - loss: 0.1502 - val_accuracy: 0.8823 - val_loss: 0.3634 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 379ms/step - accuracy: 0.9623 - loss: 0.1436 - val_accuracy: 0.8833 - val_loss: 0.3635 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 428ms/step - accuracy: 0.9647 - loss: 0.1374 - val_accuracy: 0.8828 - val_loss: 0.3640 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 379ms/step - accuracy: 0.9655 - loss: 0.1317 - val_accuracy: 0.8828 - val_loss: 0.3648 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 380ms/step - accuracy: 0.9667 - loss: 0.1264 - val_accuracy: 0.8828 - val_loss: 0.3658 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 388ms/step - accuracy: 0.9681 - loss: 0.1202 - val_accuracy: 0.8849 - val_loss: 0.3657 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 384ms/step - accuracy: 0.9709 - loss: 0.1153 - val_accuracy: 0.8854 - val_loss: 0.3660 - learning_rate: 5.0000e-04\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 338ms/step\n",
      "\n",
      "Validation: \tFold 1 - Loss: 0.3634, Accuracy: 0.8823, Precision: 0.8764, Recall: 0.8630, F1 Score: 0.8693, AUC Score: 0.9648\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 279ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 119\u001b[0m\n\u001b[0;32m    115\u001b[0m avg_test_f1 \u001b[38;5;241m=\u001b[39m f1_score(true_labels, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# -------------------- Optional: Evaluation on Test Dataset --------------------\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# If this fold produces the best F1 score so far, save the model and visuals\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_test_f1 \u001b[38;5;241m>\u001b[39m \u001b[43mbest_f1_score\u001b[49m:\n\u001b[0;32m    120\u001b[0m     best_f1_score \u001b[38;5;241m=\u001b[39m avg_test_f1\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# Save the best model (using model.export for TensorFlow SavedModel format)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "train_metrics = []      # list to save training metrics\n",
    "val_metrics = []        # list to save validation metrics\n",
    "normalization_layer = layers.Rescaling(1.0 / 255)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).batch(32)\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"Training model {i + 1}/{len(configs)} with config: {config}\")\n",
    "\n",
    "    # K-fold Cross Validation\n",
    "    kfold = StratifiedKFold(n_splits=config['folds'], shuffle=True, random_state=42)\n",
    "    best_val_f1score = -float('inf')            # Initialize best F1 score with a very low value\n",
    "\n",
    "    # Define the base path for saving models\n",
    "    model_subdir = os.path.join(save_dir, f'model{i + 1}')\n",
    "    os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "    # Define the base path for saving checkpoints for model\n",
    "    checkpoint_folder = os.path.join(model_subdir, 'checkpoints')\n",
    "    os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "    # Define the base path for saving cthe model with the best f1-score\n",
    "    best_f1_dir = os.path.join(model_subdir, 'best_f1score_fold')\n",
    "    os.makedirs(best_f1_dir, exist_ok=True)\n",
    "    \n",
    "    # Training and validation loop for each fold\n",
    "    fold = 1\n",
    "    best_f1_score = 0\n",
    "    for train_idx, val_idx in kfold.split(train_val_images, train_val_labels):\n",
    "        print(f\"\\nFold {fold}/{config['folds']}...\")\n",
    "\n",
    "        checkpoint_filepath = os.path.join(checkpoint_folder, f'checkpoint_fold{fold}.weights.h5')\n",
    "\n",
    "        # Create subset datasets for training and validation\n",
    "        train_images, train_labels = train_val_images[train_idx], train_val_labels[train_idx]\n",
    "        val_images, val_labels = train_val_images[val_idx], train_val_labels[val_idx]\n",
    "\n",
    "        # Convert NumPy arrays back to TensorFlow datasets\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "\n",
    "        # Normalize datasets \n",
    "        normalization_layer = layers.Rescaling(1./255)\n",
    "        train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "        # prefetch data to improve performance by overlapping data preprocessing and model execution and cache the dataset in memory and batch\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        train_ds = train_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        val_ds = val_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        # Step 1: Train model with frozen layers\n",
    "        print(f\"Training with frozen base layers for {config['epochs']} epochs...\")\n",
    "\n",
    "        # Create and compile model for each fold\n",
    "        model = create_model(num_classes, config, fine_tune=False) \n",
    "\n",
    "        # setup callbacks \n",
    "        early_stopping, model_checkpoint, reduce_lr = callbacks_setup(checkpoint_filepath)\n",
    "\n",
    "        # train the model on the training set until the epochs specified\n",
    "        history_frozen = model.fit(\n",
    "            train_ds,                                       # dataset used for training\n",
    "            validation_data=val_ds,                         # dataset used for validation\n",
    "            epochs=config['epochs'],                        # epochs used for training\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],   # set early stopping to avoid overfitting\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # load the best weights from ModelCheckpoint after training\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "\n",
    "        if(config[\"fine_tune\"] == True):\n",
    "            # Step 2: Unfreeze layers and fine-tune\n",
    "            print(f\"Unfreezing layers starting from layer {config['fine_tune_at']} for fine-tuning...\")\n",
    "            fine_tune_model(model.layers[0], config['fine_tune_at'])      # fine tune model\n",
    "\n",
    "            # re-compile the model with a lower learning rate for fine-tuning\n",
    "            fine_tune_lr = config['learning_rate'] * 0.01\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "                \n",
    "            print(f\"Fine-tuning for {config['fine_tune_epochs']} epochs...\")\n",
    "\n",
    "            # setup callbacks again for fine-tuning phase with a unique checkpoint\n",
    "            early_stopping, model_checkpoint = callbacks_setup(checkpoint_filepath)\n",
    "            \n",
    "            history_fine_tune = model.fit(\n",
    "                train_ds,                                       # dataset used for training\n",
    "                validation_data=val_ds,                         # dataset used for validation\n",
    "                epochs=config['fine_tune_epochs'],                        # epochs used for training\n",
    "                callbacks=[early_stopping, model_checkpoint],   # set early stopping to avoid overfitting\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # load weights after fine-tuning\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "\n",
    "        # evaluate on validation set after training\n",
    "        val_predictions = model.predict(val_ds)\n",
    "        avg_val_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "        avg_val_accuracy, avg_val_precision, avg_val_recall, avg_val_f1, avg_val_auc = calculate_metrics(\n",
    "            np.concatenate([y for x, y in val_ds]), val_predictions\n",
    "        )\n",
    "\n",
    "        print(f\"\\nValidation: \\tFold {fold} - Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accuracy:.4f}, Precision: {avg_val_precision:.4f}, Recall: {avg_val_recall:.4f}, F1 Score: {avg_val_f1:.4f}, AUC Score: {avg_val_auc:.4f}\")\n",
    "\n",
    "        test_predictions = model.predict(test_ds)\n",
    "        predicted_labels = np.argmax(test_predictions, axis=-1)\n",
    "        true_labels = np.concatenate([y for _, y in test_ds], axis=0)\n",
    "\n",
    "        avg_test_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "        # -------------------- Optional: Evaluation on Test Dataset --------------------\n",
    "        # If this fold produces the best F1 score so far, save the model and visuals\n",
    "        if avg_test_f1 > best_f1_score:\n",
    "            best_f1_score = avg_test_f1\n",
    "            # Save the best model (using model.export for TensorFlow SavedModel format)\n",
    "            model.export(best_f1_dir)\n",
    "            print(f\"Best model updated at Fold {fold} with F1 Score: {best_f1_score:.4f}\")\n",
    "            if config.get('save_metrics', False):\n",
    "                save_best_model_visuals(history_frozen, model, test_ds, class_names, model_subdir, fold)\n",
    "\n",
    "        fold += 1       # Move to the next fold\n",
    "\n",
    "# save metrics after training\n",
    "# np.save(os.path.join(save_dir, 'train_metrics.npy'), train_metrics)\n",
    "# np.save(os.path.join(save_dir, 'val_metrics.npy'), val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the best model on the held-out test set...\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step\n",
      "\n",
      "Test Set Evaluation - Loss: 0.2367, Accuracy: 0.9219, Precision: 0.9218, Recall: 0.9200, F1 Score: 0.9208, AUC Score: 0.9586\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('../saved_models/mobilenetv2_best_f1score_fold_1.h5')\n",
    "\n",
    "# once training is complete, evaluate on the held-out test set\n",
    "print(\"Evaluating the best model on the held-out test set...\")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).batch(batch_size)\n",
    "\n",
    "test_predictions = model.predict(test_ds)\n",
    "avg_test_loss = model.evaluate(test_ds, verbose=0)[0]\n",
    "avg_test_accuracy, avg_test_precision, avg_test_recall, avg_test_f1, avg_test_auc = calculate_metrics(\n",
    "    np.concatenate([y for x, y in test_ds]), test_predictions\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Set Evaluation - Loss: {avg_test_loss:.4f}, Accuracy: {avg_test_accuracy:.4f}, Precision: {avg_test_precision:.4f}, Recall: {avg_test_recall:.4f}, F1 Score: {avg_test_f1:.4f}, AUC Score: {avg_test_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
