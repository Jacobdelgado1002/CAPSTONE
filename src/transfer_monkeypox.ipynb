{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with MonkeyPox Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(\"../data/Augmented_Images\")    # points to the folder containing the images that will be used for training\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32         # size of the batch that will be fed to model\n",
    "img_height = 224        # input image height\n",
    "img_width = 224         # input image width\n",
    "NUM_MODELS = 1          # number of models that you wish to train\n",
    "\n",
    "# k-fold cross-validation parameters\n",
    "FOLDS = 5               # the amount of folds that will be created for cross-validation\n",
    "\n",
    "# Fine-tuning parameters\n",
    "FINE_TUNE_EPOCHS = 10   # number of epochs after which we start fine-tuning\n",
    "FINE_TUNE_AT = 150      # layer number where we start unfreezing layers\n",
    "\n",
    "# configurations that will be used in training\n",
    "configs = [\n",
    "    {\"learning_rate\": 0.001, \"optimizer\": \"adam\", \"epochs\": 14, \"save_metrics\": False},\n",
    "    # {\"learning_rate\": 0.0001, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": False},\n",
    "    # {\"learning_rate\": 0.001, \"optimizer\": \"sgd\", \"epochs\": 50, \"save_metrics\": False},\n",
    "    # {\"learning_rate\": 0.0001, \"optimizer\": \"sgd\", \"epochs\": 50, \"save_metrics\": False},\n",
    "]\n",
    "\n",
    "# Define the base path for saving models\n",
    "save_dir = \"../saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3192 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset without splitting\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_root,                                  # loads images from the data_root directory\n",
    "    image_size=(img_height, img_width),         # resizes all images to (224, 224) pixels\n",
    "    batch_size=batch_size,                      # set the batch size\n",
    "    shuffle=True                                # shufle data when loaded\n",
    ")\n",
    "\n",
    "class_names = np.array(dataset.class_names)     # get the class names for the data\n",
    "num_classes = len(class_names)                  # get the number of classes in the dataset\n",
    "\n",
    "# convert the dataset to a list of (image, label) pairs. This makes it easier to perform cross-validation\n",
    "image_paths, labels = [], []\n",
    "for image_batch, label_batch in dataset:\n",
    "    image_paths.extend(image_batch.numpy())\n",
    "    labels.extend(label_batch.numpy())\n",
    "\n",
    "image_paths = np.array(image_paths)             # convert to numpy array to facilitate training\n",
    "labels = np.array(labels)                       # convert to numpy array to facilitate training\n",
    "\n",
    "# Split the dataset into training/validation and test sets\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.05, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# K-fold Cross Validation\n",
    "kfold = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "best_val_f1score = -float('inf')  # Initialize best F1 score with a very low value\n",
    "\n",
    "# Define the base path for saving models\n",
    "checkpoint_filepath = \"../checkpoints\"\n",
    "os.makedirs(checkpoint_filepath, exist_ok=True)\n",
    "\n",
    "def callbacks_setup():\n",
    "    # EarlyStopping callback configuration\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # monitor validation loss\n",
    "        patience=3,                # number of epochs with no improvement to stop training\n",
    "        mode = 'min',              # want to minimize what it being monitored \n",
    "        restore_best_weights=False # restore model weights from the epoch with the best value of the monitored metric\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,   # oath to save weights\n",
    "        save_weights_only=True,         # only save weights instead of full model\n",
    "        monitor='val_loss',             # monitor validation loss\n",
    "        mode='min',                     # want to minimize what is being monitored\n",
    "        save_best_only=True             # save the best weights\n",
    "    )            \n",
    "\n",
    "    return early_stopping, model_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, classification_report, roc_auc_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# plot and save confusion matrix\n",
    "def save_confusion_matrix(true_labels, predicted_labels, class_names, save_path):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# plot and save loss curves\n",
    "def save_loss_curve(history, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# compute and plot evaluation metrics (accuracy, sensitivity, specificity, F1 score)\n",
    "def save_evaluation_metrics(true_labels, predicted_labels, history, cm, save_path):\n",
    "    accuracy = history['val_accuracy'][-1]\n",
    "    sensitivity = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    specificity = np.mean(np.diag(cm) / (np.diag(cm) + np.sum(cm, axis=0) - np.diag(cm)))\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensitivity (Recall)\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(metrics.keys(), metrics.values(), color=['darkturquoise', 'sandybrown', 'hotpink', 'limegreen'])\n",
    "    plt.title(\"Model Evaluation Metrics\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    return metrics\n",
    "\n",
    "# save classification report\n",
    "def save_classification_report(true_labels, predicted_labels, class_names, save_path):\n",
    "    class_report = classification_report(true_labels, predicted_labels, target_names=class_names, digits=4)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(class_report)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    accuracy = np.mean(np.argmax(predictions, axis=1) == true_labels)\n",
    "    precision = precision_score(true_labels, np.argmax(predictions, axis=1), average='macro')\n",
    "    recall = recall_score(true_labels, np.argmax(predictions, axis=1), average='macro')\n",
    "    f1 = f1_score(true_labels, np.argmax(predictions, axis=1), average='macro')\n",
    "    auc = roc_auc_score(tf.keras.utils.to_categorical(true_labels), predictions, multi_class='ovr')\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Function to save metrics, loss curve, and confusion matrix for the best model\n",
    "def save_best_model_visuals(history, model, val_ds, class_names, weights_path, fold):\n",
    "    # generate predictions for the validation set\n",
    "    val_predictions = model.predict(val_ds)\n",
    "    val_predicted_ids = np.argmax(val_predictions, axis=-1)\n",
    "    true_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "    # confusion Matrix\n",
    "    confusion_matrix_path = os.path.join(weights_path, f\"confusion_matrix_fold_{fold}.png\")\n",
    "    save_confusion_matrix(true_labels, val_predicted_ids, class_names, confusion_matrix_path)\n",
    "\n",
    "    # loss curve\n",
    "    loss_curve_path = os.path.join(weights_path, f\"loss_curve_fold_{fold}.png\")\n",
    "    save_loss_curve(history.history, loss_curve_path)\n",
    "\n",
    "    # evaluation Metrics (Accuracy, Sensitivity, Specificity, F1 Score)\n",
    "    cm = confusion_matrix(true_labels, val_predicted_ids)\n",
    "    metrics_bar_chart_path = os.path.join(weights_path, f\"evaluation_metrics_fold_{fold}.png\")\n",
    "    save_evaluation_metrics(true_labels, val_predicted_ids, history.history, cm, metrics_bar_chart_path)\n",
    "\n",
    "    # save classification report as a text file\n",
    "    classification_report_path = os.path.join(weights_path, f\"classification_report_fold_{fold}.txt\")\n",
    "    save_classification_report(true_labels, val_predicted_ids, class_names, classification_report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and compile the model\n",
    "def create_model(num_classes, config, fine_tune=None):\n",
    "    # if you are not fine tuning the model, instantiate a new model \n",
    "    if(fine_tune == False):         \n",
    "        # instantiate mobilenet (contains 154 layers)\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=(img_height, img_width, 3),     # set the input it will receive\n",
    "            include_top=False,                          # do not include top layer to perform transfer learning\n",
    "            weights='imagenet'                          # load weights from imagenet dataset\n",
    "        )\n",
    "        base_model.trainable = False                    # Freeze the base model\n",
    "        \n",
    "        # add a layer in order to perform classification on our dataset\n",
    "        model = Sequential([\n",
    "            base_model,                                 # use base_model as the start of your model\n",
    "            layers.GlobalAveragePooling2D(),            # add a final layer to perform classification\n",
    "            layers.Dense(num_classes)                   # set the number of possible prediction to the num of classes in dataset\n",
    "        ])\n",
    "        \n",
    "    # select optimizer and learning rate based on configuration\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {config['optimizer']}\")\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fine tune model by unfreezing the layers after the first fine_tune_at layers\n",
    "def fine_tune_model(base_model, fine_tune_at):\n",
    "    # Unfreeze the layers starting from fine_tune_at index\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/1 with config: {'learning_rate': 0.001, 'optimizer': 'adam', 'epochs': 14, 'save_metrics': False}\n",
      "\n",
      "Fold 1/5...\n",
      "Training with frozen base layers for 14 epochs...\n",
      "Epoch 1/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 294ms/step - accuracy: 0.6805 - loss: 0.6266 - val_accuracy: 0.8287 - val_loss: 0.3972\n",
      "Epoch 2/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 219ms/step - accuracy: 0.8594 - loss: 0.3491 - val_accuracy: 0.8616 - val_loss: 0.3441\n",
      "Epoch 3/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 221ms/step - accuracy: 0.8988 - loss: 0.2874 - val_accuracy: 0.8764 - val_loss: 0.3154\n",
      "Epoch 4/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 220ms/step - accuracy: 0.9107 - loss: 0.2520 - val_accuracy: 0.8896 - val_loss: 0.2962\n",
      "Epoch 5/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 216ms/step - accuracy: 0.9174 - loss: 0.2276 - val_accuracy: 0.8880 - val_loss: 0.2822\n",
      "Epoch 6/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 219ms/step - accuracy: 0.9242 - loss: 0.2091 - val_accuracy: 0.8946 - val_loss: 0.2716\n",
      "Epoch 7/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 219ms/step - accuracy: 0.9377 - loss: 0.1943 - val_accuracy: 0.8962 - val_loss: 0.2634\n",
      "Epoch 8/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 218ms/step - accuracy: 0.9412 - loss: 0.1822 - val_accuracy: 0.8962 - val_loss: 0.2569\n",
      "Epoch 9/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 221ms/step - accuracy: 0.9460 - loss: 0.1720 - val_accuracy: 0.8962 - val_loss: 0.2518\n",
      "Epoch 10/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 226ms/step - accuracy: 0.9518 - loss: 0.1631 - val_accuracy: 0.8995 - val_loss: 0.2477\n",
      "Epoch 11/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 216ms/step - accuracy: 0.9531 - loss: 0.1554 - val_accuracy: 0.9044 - val_loss: 0.2443\n",
      "Epoch 12/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 221ms/step - accuracy: 0.9581 - loss: 0.1486 - val_accuracy: 0.9094 - val_loss: 0.2416\n",
      "Epoch 13/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 216ms/step - accuracy: 0.9590 - loss: 0.1425 - val_accuracy: 0.9094 - val_loss: 0.2393\n",
      "Epoch 14/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 220ms/step - accuracy: 0.9592 - loss: 0.1369 - val_accuracy: 0.9094 - val_loss: 0.2374\n",
      "Unfreezing layers starting from layer 150 for fine-tuning...\n",
      "Fine-tuning for 10 epochs...\n",
      "Epoch 1/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 287ms/step - accuracy: 0.8616 - loss: 0.3400 - val_accuracy: 0.8962 - val_loss: 0.3017\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 256ms/step - accuracy: 0.9156 - loss: 0.1970 - val_accuracy: 0.8913 - val_loss: 0.3127\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 252ms/step - accuracy: 0.9408 - loss: 0.1582 - val_accuracy: 0.8913 - val_loss: 0.3029\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 254ms/step - accuracy: 0.9502 - loss: 0.1387 - val_accuracy: 0.8913 - val_loss: 0.2894\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - accuracy: 0.9549 - loss: 0.1263 - val_accuracy: 0.8962 - val_loss: 0.2768\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 538ms/step - accuracy: 0.9623 - loss: 0.1171 - val_accuracy: 0.8962 - val_loss: 0.2660\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 252ms/step - accuracy: 0.9637 - loss: 0.1096 - val_accuracy: 0.9012 - val_loss: 0.2566\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 256ms/step - accuracy: 0.9678 - loss: 0.1033 - val_accuracy: 0.9044 - val_loss: 0.2485\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 252ms/step - accuracy: 0.9743 - loss: 0.0976 - val_accuracy: 0.9061 - val_loss: 0.2414\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 255ms/step - accuracy: 0.9753 - loss: 0.0926 - val_accuracy: 0.9077 - val_loss: 0.2352\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: \tFold 1 - Loss: 0.2352, Accuracy: 0.9077, Precision: 0.9165, Recall: 0.9016, F1 Score: 0.9056, AUC Score: 0.9629\n",
      "Model with best F1 score during Validation saved at Fold 1 with F1 Score of 0.9056\n",
      "\n",
      "Fold 2/5...\n",
      "Training with frozen base layers for 14 epochs...\n",
      "Epoch 1/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 294ms/step - accuracy: 0.6194 - loss: 0.7491 - val_accuracy: 0.7677 - val_loss: 0.4499\n",
      "Epoch 2/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 229ms/step - accuracy: 0.8305 - loss: 0.3853 - val_accuracy: 0.8270 - val_loss: 0.3791\n",
      "Epoch 3/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 221ms/step - accuracy: 0.8863 - loss: 0.3139 - val_accuracy: 0.8386 - val_loss: 0.3413\n",
      "Epoch 4/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 568ms/step - accuracy: 0.9014 - loss: 0.2720 - val_accuracy: 0.8583 - val_loss: 0.3174\n",
      "Epoch 5/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9165 - loss: 0.2441 - val_accuracy: 0.8682 - val_loss: 0.3006\n",
      "Epoch 6/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.9233 - loss: 0.2236 - val_accuracy: 0.8699 - val_loss: 0.2881\n",
      "Epoch 7/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.9299 - loss: 0.2075 - val_accuracy: 0.8748 - val_loss: 0.2783\n",
      "Epoch 8/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 873ms/step - accuracy: 0.9350 - loss: 0.1942 - val_accuracy: 0.8764 - val_loss: 0.2705\n",
      "Epoch 9/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 527ms/step - accuracy: 0.9403 - loss: 0.1831 - val_accuracy: 0.8797 - val_loss: 0.2642\n",
      "Epoch 10/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 747ms/step - accuracy: 0.9430 - loss: 0.1734 - val_accuracy: 0.8863 - val_loss: 0.2590\n",
      "Epoch 11/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 743ms/step - accuracy: 0.9490 - loss: 0.1649 - val_accuracy: 0.8896 - val_loss: 0.2547\n",
      "Epoch 12/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 760ms/step - accuracy: 0.9482 - loss: 0.1574 - val_accuracy: 0.8929 - val_loss: 0.2512\n",
      "Epoch 13/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 750ms/step - accuracy: 0.9525 - loss: 0.1506 - val_accuracy: 0.8929 - val_loss: 0.2482\n",
      "Epoch 14/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 757ms/step - accuracy: 0.9549 - loss: 0.1445 - val_accuracy: 0.8929 - val_loss: 0.2457\n",
      "Unfreezing layers starting from layer 150 for fine-tuning...\n",
      "Fine-tuning for 10 epochs...\n",
      "Epoch 1/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 867ms/step - accuracy: 0.8833 - loss: 0.2598 - val_accuracy: 0.8731 - val_loss: 0.2885\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m995s\u001b[0m 13s/step - accuracy: 0.9348 - loss: 0.1753 - val_accuracy: 0.8649 - val_loss: 0.2956\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 251ms/step - accuracy: 0.9412 - loss: 0.1490 - val_accuracy: 0.8715 - val_loss: 0.2903\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9524 - loss: 0.1333 - val_accuracy: 0.8764 - val_loss: 0.2821\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 235ms/step - accuracy: 0.9599 - loss: 0.1222 - val_accuracy: 0.8814 - val_loss: 0.2737\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9658 - loss: 0.1137 - val_accuracy: 0.8847 - val_loss: 0.2658\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 231ms/step - accuracy: 0.9698 - loss: 0.1066 - val_accuracy: 0.8863 - val_loss: 0.2586\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 232ms/step - accuracy: 0.9714 - loss: 0.1005 - val_accuracy: 0.8962 - val_loss: 0.2520\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 233ms/step - accuracy: 0.9730 - loss: 0.0950 - val_accuracy: 0.8979 - val_loss: 0.2462\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 231ms/step - accuracy: 0.9760 - loss: 0.0901 - val_accuracy: 0.8979 - val_loss: 0.2408\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step\n",
      "\n",
      "Validation: \tFold 2 - Loss: 0.2408, Accuracy: 0.8979, Precision: 0.9070, Recall: 0.8916, F1 Score: 0.8955, AUC Score: 0.9556\n",
      "\n",
      "Fold 3/5...\n",
      "Training with frozen base layers for 14 epochs...\n",
      "Epoch 1/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 1s/step - accuracy: 0.6901 - loss: 0.6038 - val_accuracy: 0.7937 - val_loss: 0.4303\n",
      "Epoch 2/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 285ms/step - accuracy: 0.8526 - loss: 0.3683 - val_accuracy: 0.8564 - val_loss: 0.3544\n",
      "Epoch 3/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 224ms/step - accuracy: 0.8829 - loss: 0.3056 - val_accuracy: 0.8762 - val_loss: 0.3117\n",
      "Epoch 4/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 216ms/step - accuracy: 0.9028 - loss: 0.2682 - val_accuracy: 0.8861 - val_loss: 0.2842\n",
      "Epoch 5/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 218ms/step - accuracy: 0.9128 - loss: 0.2420 - val_accuracy: 0.8927 - val_loss: 0.2646\n",
      "Epoch 6/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 218ms/step - accuracy: 0.9228 - loss: 0.2220 - val_accuracy: 0.8927 - val_loss: 0.2499\n",
      "Epoch 7/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 219ms/step - accuracy: 0.9298 - loss: 0.2059 - val_accuracy: 0.9010 - val_loss: 0.2384\n",
      "Epoch 8/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 217ms/step - accuracy: 0.9365 - loss: 0.1926 - val_accuracy: 0.9076 - val_loss: 0.2293\n",
      "Epoch 9/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 220ms/step - accuracy: 0.9427 - loss: 0.1813 - val_accuracy: 0.9092 - val_loss: 0.2219\n",
      "Epoch 10/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 217ms/step - accuracy: 0.9473 - loss: 0.1715 - val_accuracy: 0.9125 - val_loss: 0.2159\n",
      "Epoch 11/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 217ms/step - accuracy: 0.9479 - loss: 0.1630 - val_accuracy: 0.9142 - val_loss: 0.2110\n",
      "Epoch 12/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 220ms/step - accuracy: 0.9533 - loss: 0.1555 - val_accuracy: 0.9158 - val_loss: 0.2069\n",
      "Epoch 13/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 221ms/step - accuracy: 0.9572 - loss: 0.1487 - val_accuracy: 0.9191 - val_loss: 0.2035\n",
      "Epoch 14/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 225ms/step - accuracy: 0.9588 - loss: 0.1426 - val_accuracy: 0.9191 - val_loss: 0.2006\n",
      "Unfreezing layers starting from layer 150 for fine-tuning...\n",
      "Fine-tuning for 10 epochs...\n",
      "Epoch 1/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 493ms/step - accuracy: 0.8811 - loss: 0.2795 - val_accuracy: 0.9257 - val_loss: 0.2466\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - accuracy: 0.9300 - loss: 0.1806 - val_accuracy: 0.9208 - val_loss: 0.2511\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 557ms/step - accuracy: 0.9415 - loss: 0.1525 - val_accuracy: 0.9241 - val_loss: 0.2433\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 874ms/step - accuracy: 0.9477 - loss: 0.1356 - val_accuracy: 0.9274 - val_loss: 0.2342\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 835ms/step - accuracy: 0.9590 - loss: 0.1237 - val_accuracy: 0.9307 - val_loss: 0.2261\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 858ms/step - accuracy: 0.9620 - loss: 0.1144 - val_accuracy: 0.9257 - val_loss: 0.2191\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 843ms/step - accuracy: 0.9671 - loss: 0.1068 - val_accuracy: 0.9274 - val_loss: 0.2131\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 828ms/step - accuracy: 0.9740 - loss: 0.1003 - val_accuracy: 0.9257 - val_loss: 0.2077\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 815ms/step - accuracy: 0.9783 - loss: 0.0945 - val_accuracy: 0.9274 - val_loss: 0.2030\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 818ms/step - accuracy: 0.9805 - loss: 0.0894 - val_accuracy: 0.9274 - val_loss: 0.1987\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 734ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: \tFold 3 - Loss: 0.1987, Accuracy: 0.9274, Precision: 0.9338, Recall: 0.9192, F1 Score: 0.9246, AUC Score: 0.9664\n",
      "Model with best F1 score during Validation saved at Fold 3 with F1 Score of 0.9246\n",
      "\n",
      "Fold 4/5...\n",
      "Training with frozen base layers for 14 epochs...\n",
      "Epoch 1/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 917ms/step - accuracy: 0.6693 - loss: 0.6394 - val_accuracy: 0.8152 - val_loss: 0.4080\n",
      "Epoch 2/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 854ms/step - accuracy: 0.8449 - loss: 0.3806 - val_accuracy: 0.8614 - val_loss: 0.3449\n",
      "Epoch 3/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 340ms/step - accuracy: 0.8806 - loss: 0.3146 - val_accuracy: 0.8713 - val_loss: 0.3116\n",
      "Epoch 4/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 542ms/step - accuracy: 0.8982 - loss: 0.2758 - val_accuracy: 0.8828 - val_loss: 0.2900\n",
      "Epoch 5/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.9135 - loss: 0.2486 - val_accuracy: 0.8861 - val_loss: 0.2742\n",
      "Epoch 6/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 994ms/step - accuracy: 0.9209 - loss: 0.2276 - val_accuracy: 0.8894 - val_loss: 0.2621\n",
      "Epoch 7/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 978ms/step - accuracy: 0.9271 - loss: 0.2105 - val_accuracy: 0.8944 - val_loss: 0.2527\n",
      "Epoch 8/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 940ms/step - accuracy: 0.9337 - loss: 0.1964 - val_accuracy: 0.9026 - val_loss: 0.2451\n",
      "Epoch 9/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 3s/step - accuracy: 0.9404 - loss: 0.1844 - val_accuracy: 0.9076 - val_loss: 0.2391\n",
      "Epoch 10/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.9434 - loss: 0.1741 - val_accuracy: 0.9109 - val_loss: 0.2342\n",
      "Epoch 11/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 402ms/step - accuracy: 0.9471 - loss: 0.1653 - val_accuracy: 0.9125 - val_loss: 0.2301\n",
      "Epoch 12/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 792ms/step - accuracy: 0.9499 - loss: 0.1575 - val_accuracy: 0.9142 - val_loss: 0.2267\n",
      "Epoch 13/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 893ms/step - accuracy: 0.9497 - loss: 0.1506 - val_accuracy: 0.9175 - val_loss: 0.2238\n",
      "Epoch 14/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 801ms/step - accuracy: 0.9528 - loss: 0.1444 - val_accuracy: 0.9158 - val_loss: 0.2213\n",
      "Unfreezing layers starting from layer 150 for fine-tuning...\n",
      "Fine-tuning for 10 epochs...\n",
      "Epoch 1/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 771ms/step - accuracy: 0.8878 - loss: 0.2670 - val_accuracy: 0.9142 - val_loss: 0.2572\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 392ms/step - accuracy: 0.9407 - loss: 0.1780 - val_accuracy: 0.9109 - val_loss: 0.2669\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 761ms/step - accuracy: 0.9493 - loss: 0.1516 - val_accuracy: 0.9109 - val_loss: 0.2633\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 874ms/step - accuracy: 0.9549 - loss: 0.1353 - val_accuracy: 0.9142 - val_loss: 0.2567\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 854ms/step - accuracy: 0.9626 - loss: 0.1238 - val_accuracy: 0.9158 - val_loss: 0.2493\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 850ms/step - accuracy: 0.9695 - loss: 0.1148 - val_accuracy: 0.9175 - val_loss: 0.2421\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 852ms/step - accuracy: 0.9736 - loss: 0.1073 - val_accuracy: 0.9191 - val_loss: 0.2353\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 873ms/step - accuracy: 0.9770 - loss: 0.1008 - val_accuracy: 0.9224 - val_loss: 0.2289\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 823ms/step - accuracy: 0.9787 - loss: 0.0951 - val_accuracy: 0.9241 - val_loss: 0.2230\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 795ms/step - accuracy: 0.9800 - loss: 0.0899 - val_accuracy: 0.9274 - val_loss: 0.2175\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 629ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: \tFold 4 - Loss: 0.2175, Accuracy: 0.9274, Precision: 0.9329, Recall: 0.9228, F1 Score: 0.9260, AUC Score: 0.9595\n",
      "Model with best F1 score during Validation saved at Fold 4 with F1 Score of 0.9260\n",
      "\n",
      "Fold 5/5...\n",
      "Training with frozen base layers for 14 epochs...\n",
      "Epoch 1/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.6850 - loss: 0.6015 - val_accuracy: 0.8399 - val_loss: 0.3706\n",
      "Epoch 2/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 253ms/step - accuracy: 0.8602 - loss: 0.3549 - val_accuracy: 0.8729 - val_loss: 0.3050\n",
      "Epoch 3/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 241ms/step - accuracy: 0.8928 - loss: 0.2941 - val_accuracy: 0.8944 - val_loss: 0.2730\n",
      "Epoch 4/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 241ms/step - accuracy: 0.9083 - loss: 0.2590 - val_accuracy: 0.8960 - val_loss: 0.2535\n",
      "Epoch 5/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.9190 - loss: 0.2345 - val_accuracy: 0.9043 - val_loss: 0.2398\n",
      "Epoch 6/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 241ms/step - accuracy: 0.9327 - loss: 0.2158 - val_accuracy: 0.9076 - val_loss: 0.2295\n",
      "Epoch 7/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 240ms/step - accuracy: 0.9383 - loss: 0.2005 - val_accuracy: 0.9076 - val_loss: 0.2213\n",
      "Epoch 8/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.9442 - loss: 0.1876 - val_accuracy: 0.9092 - val_loss: 0.2148\n",
      "Epoch 9/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9494 - loss: 0.1766 - val_accuracy: 0.9142 - val_loss: 0.2095\n",
      "Epoch 10/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 237ms/step - accuracy: 0.9523 - loss: 0.1668 - val_accuracy: 0.9191 - val_loss: 0.2051\n",
      "Epoch 11/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.9562 - loss: 0.1582 - val_accuracy: 0.9224 - val_loss: 0.2016\n",
      "Epoch 12/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 239ms/step - accuracy: 0.9576 - loss: 0.1505 - val_accuracy: 0.9224 - val_loss: 0.1987\n",
      "Epoch 13/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 242ms/step - accuracy: 0.9629 - loss: 0.1436 - val_accuracy: 0.9241 - val_loss: 0.1964\n",
      "Epoch 14/14\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 241ms/step - accuracy: 0.9635 - loss: 0.1374 - val_accuracy: 0.9241 - val_loss: 0.1945\n",
      "Unfreezing layers starting from layer 150 for fine-tuning...\n",
      "Fine-tuning for 10 epochs...\n",
      "Epoch 1/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 506ms/step - accuracy: 0.8789 - loss: 0.2744 - val_accuracy: 0.9026 - val_loss: 0.2247\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 328ms/step - accuracy: 0.9311 - loss: 0.1786 - val_accuracy: 0.8993 - val_loss: 0.2305\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 253ms/step - accuracy: 0.9459 - loss: 0.1494 - val_accuracy: 0.8944 - val_loss: 0.2254\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 256ms/step - accuracy: 0.9560 - loss: 0.1328 - val_accuracy: 0.9010 - val_loss: 0.2184\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 257ms/step - accuracy: 0.9596 - loss: 0.1214 - val_accuracy: 0.9059 - val_loss: 0.2114\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 502ms/step - accuracy: 0.9621 - loss: 0.1127 - val_accuracy: 0.9092 - val_loss: 0.2049\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 269ms/step - accuracy: 0.9691 - loss: 0.1055 - val_accuracy: 0.9076 - val_loss: 0.1989\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 263ms/step - accuracy: 0.9714 - loss: 0.0993 - val_accuracy: 0.9092 - val_loss: 0.1935\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 257ms/step - accuracy: 0.9742 - loss: 0.0938 - val_accuracy: 0.9092 - val_loss: 0.1886\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 256ms/step - accuracy: 0.9751 - loss: 0.0889 - val_accuracy: 0.9109 - val_loss: 0.1841\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step\n",
      "\n",
      "Validation: \tFold 5 - Loss: 0.1841, Accuracy: 0.9109, Precision: 0.9185, Recall: 0.9040, F1 Score: 0.9085, AUC Score: 0.9720\n"
     ]
    }
   ],
   "source": [
    "train_metrics = []      # list to save training metrics\n",
    "val_metrics = []        # list to save validation metrics\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"Training model {i + 1}/{len(configs)} with config: {config}\")\n",
    "\n",
    "    # Training and validation loop for each fold\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kfold.split(train_val_images):\n",
    "        print(f\"\\nFold {fold}/{FOLDS}...\")\n",
    "\n",
    "        # Create subset datasets for training and validation\n",
    "        train_images, train_labels = train_val_images[train_idx], train_val_labels[train_idx]\n",
    "        val_images, val_labels = train_val_images[val_idx], train_val_labels[val_idx]\n",
    "\n",
    "        # Convert NumPy arrays back to TensorFlow datasets\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "\n",
    "        # Normalize datasets and batch\n",
    "        normalization_layer = layers.Rescaling(1./255)\n",
    "        train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)).batch(batch_size)\n",
    "        val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)).batch(batch_size)\n",
    "\n",
    "        # prefetch data to improve performance by overlapping data preprocessing and model execution and cache the dataset in memor\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        # Step 1: Train model with frozen layers\n",
    "        print(f\"Training with frozen base layers for {config['epochs']} epochs...\")\n",
    "\n",
    "        # Create and compile model for each fold\n",
    "        model = create_model(num_classes, config, fine_tune=False)\n",
    "\n",
    "        # setup the callbacks that will be used\n",
    "        early_stopping, model_checkpoint = callbacks_setup()\n",
    "\n",
    "        # train the model on the training set until the epochs specified\n",
    "        history_frozen = model.fit(\n",
    "            train_ds,                                       # dataset used for training\n",
    "            validation_data=val_ds,                         # dataset used for validation\n",
    "            epochs=config['epochs'],                        # epochs used for training\n",
    "            callbacks=[early_stopping, model_checkpoint],   # set early stopping to avoid overfitting\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Step 2: Unfreeze layers and fine-tune\n",
    "        print(f\"Unfreezing layers starting from layer {FINE_TUNE_AT} for fine-tuning...\")\n",
    "        fine_tune_model(model.layers[0], FINE_TUNE_AT)      # fine tune model\n",
    "\n",
    "        # re-compile the model with a lower learning rate for fine-tuning\n",
    "        fine_tune_lr = config['learning_rate'] * 0.01\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(f\"Fine-tuning for {FINE_TUNE_EPOCHS} epochs...\")\n",
    "\n",
    "        early_stopping, model_checkpoint = callbacks_setup()\n",
    "        \n",
    "        history_fine_tune = model.fit(\n",
    "            train_ds,                                       # dataset used for training\n",
    "            validation_data=val_ds,                         # dataset used for validation\n",
    "            epochs=FINE_TUNE_EPOCHS,                        # epochs used for training\n",
    "            callbacks=[early_stopping, model_checkpoint],   # set early stopping to avoid overfitting\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # evaluate on validation set after training\n",
    "        val_predictions = model.predict(val_ds)\n",
    "        avg_val_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "        avg_val_accuracy, avg_val_precision, avg_val_recall, avg_val_f1, avg_val_auc = calculate_metrics(\n",
    "            np.concatenate([y for x, y in val_ds]), val_predictions\n",
    "        )\n",
    "\n",
    "        print(f\"\\nValidation: \\tFold {fold} - Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accuracy:.4f}, Precision: {avg_val_precision:.4f}, Recall: {avg_val_recall:.4f}, F1 Score: {avg_val_f1:.4f}, AUC Score: {avg_val_auc:.4f}\")\n",
    "\n",
    "        # save the best model based on validation F1 score\n",
    "        if avg_val_f1 > best_val_f1score:\n",
    "            best_val_f1score = avg_val_f1\n",
    "            model.save(os.path.join(save_dir, f'mobilenetv2_best_f1score_fold_{fold}.h5'))\n",
    "            print(f\"Model with best F1 score during Validation saved at Fold {fold} with F1 Score of {best_val_f1score:.4f}\")\n",
    "\n",
    "            if (config['save_metrics'] == True):\n",
    "                #save confusion matrix, loss curve, evaluation metrics for the best model\n",
    "                save_best_model_visuals(history_fine_tune, model, val_ds, class_names, save_dir, fold)\n",
    "\n",
    "        fold += 1       # Move to the next fold\n",
    "\n",
    "# save metrics after training\n",
    "# np.save(os.path.join(save_dir, 'train_metrics.npy'), train_metrics)\n",
    "# np.save(os.path.join(save_dir, 'val_metrics.npy'), val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the best model on the held-out test set...\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 208ms/step\n",
      "\n",
      "Test Set Evaluation - Loss: 0.1917, Accuracy: 0.9062, Precision: 0.9222, Recall: 0.8971, F1 Score: 0.9031, AUC Score: 0.9748\n"
     ]
    }
   ],
   "source": [
    "# once training is complete, evaluate on the held-out test set\n",
    "print(\"Evaluating the best model on the held-out test set...\")\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).batch(batch_size)\n",
    "\n",
    "test_predictions = model.predict(test_ds)\n",
    "avg_test_loss = model.evaluate(test_ds, verbose=0)[0]\n",
    "avg_test_accuracy, avg_test_precision, avg_test_recall, avg_test_f1, avg_test_auc = calculate_metrics(\n",
    "    np.concatenate([y for x, y in test_ds]), test_predictions\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Set Evaluation - Loss: {avg_test_loss:.4f}, Accuracy: {avg_test_accuracy:.4f}, Precision: {avg_test_precision:.4f}, Recall: {avg_test_recall:.4f}, F1 Score: {avg_test_f1:.4f}, AUC Score: {avg_test_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
