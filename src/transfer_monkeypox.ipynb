{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 17:42:26.817391: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-31 17:42:26.900256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 17:42:26.930681: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 17:42:26.941767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 17:42:26.995435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 17:42:27.570512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with MonkeyPox Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(\"../data/MSLD/Augmented_Images\")    # points to the folder containing the images that will be used for training\n",
    "\n",
    "# hyperparameters\n",
    "img_height = 224        # input image height\n",
    "img_width = 224         # input image width\n",
    "batch_size = 16         # size of the batch that will be fed to model\n",
    "\n",
    "# folds = the amount of folds that will be created for cross-validation\n",
    "# fine_tune_epochs = number of epochs after which we start fine-tuning\n",
    "# fine_tune_at = layer number where we start unfreezing layers\n",
    "\n",
    "# configurations that will be used in training\n",
    "# supported models: mobilenet_v2, efficientnet_v2, inception_v3, resnet_50\n",
    "# mobilenet (contains 154 layers)\n",
    "# efficientnet_v2 (contains 513 layers)\n",
    "# inception_v3 (contains 311 layers)\n",
    "# resnet_50 (contains 175 layers)\n",
    "configs = [\n",
    "    {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    # {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": True, \"fine_tune_epochs\": 25, \"fine_tune_at\": 148},\n",
    "    # {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    # {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": True, \"fine_tune_epochs\": 25, \"fine_tune_at\": 148},\n",
    "\n",
    "    # {\"model_name\": \"efficientnet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    \n",
    "    # {\"model_name\": \"densenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "\n",
    "    # {\"model_name\": \"inceptionv3\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "\n",
    "    # {\"model_name\": \"resnet50\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "\n",
    "    # {\"model_name\": \"vgg16\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 15\n",
    "\n",
    "    # {\"model_name\": \"xception\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "]\n",
    "\n",
    "# Define the base path for saving models\n",
    "save_dir = \"../saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory ../data/Augmented_Images",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset without splitting\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# loads images from the data_root directory\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# resizes all images to (224, 224) pixels\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# set the batch size\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m# shufle data when loaded\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m class_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(dataset\u001b[38;5;241m.\u001b[39mclass_names)     \u001b[38;5;66;03m# get the class names for the data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_names)                  \u001b[38;5;66;03m# get the number of classes in the dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:224\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.12/site-packages/keras/src/utils/dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.12/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory ../data/Augmented_Images"
     ]
    }
   ],
   "source": [
    "# Load dataset without splitting\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_root,                                  # loads images from the data_root directory\n",
    "    image_size=(img_height, img_width),         # resizes all images to (224, 224) pixels\n",
    "    batch_size=batch_size,                      # set the batch size\n",
    "    shuffle=True                                # shufle data when loaded\n",
    ")\n",
    "\n",
    "class_names = np.array(dataset.class_names)     # get the class names for the data\n",
    "num_classes = len(class_names)                  # get the number of classes in the dataset\n",
    "\n",
    "# convert the dataset to a list of (image, label) pairs. This makes it easier to perform cross-validation\n",
    "image_paths, labels = [], []\n",
    "for image_batch, label_batch in dataset:\n",
    "    image_paths.extend(image_batch.numpy())\n",
    "    labels.extend(label_batch.numpy())\n",
    "\n",
    "image_paths = np.array(image_paths)             # convert to numpy array to facilitate training\n",
    "labels = np.array(labels)                       # convert to numpy array to facilitate training\n",
    "\n",
    "# Split the dataset into training/validation and test sets\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.10, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "def callbacks_setup(checkpoint_filepath):\n",
    "    # EarlyStopping callback configuration\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # monitor validation loss\n",
    "        patience=10,               # number of epochs with no improvement to stop training\n",
    "        mode='min',                # want to minimize what it being monitored \n",
    "        restore_best_weights=False # don't restore in EarlyStopping, handled by ModelCheckpoint\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,   # path to save weights\n",
    "        save_weights_only=True,         # only save weights instead of full model\n",
    "        monitor='val_recall',           # monitor validation loss\n",
    "        mode='max',                     # want to maximize what is being monitored\n",
    "        save_best_only=True             # save the best weights\n",
    "    )            \n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',      # monitor validation loss \n",
    "        factor=0.5,              # factor by which the learning rate will be reduced \n",
    "        patience=4,              # number of epochs with no improvement to stop training \n",
    "        mode='min',              # want to minimize what it being monitored \n",
    "        min_lr=1e-6              # lower bound on the learning rate \n",
    "    )            \n",
    "\n",
    "    return early_stopping, model_checkpoint, reduce_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, classification_report, roc_auc_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# plot and save confusion matrix\n",
    "def save_confusion_matrix(true_labels, predicted_labels, class_names, save_path):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# plot and save loss curves\n",
    "def save_loss_curve(history, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# compute and plot evaluation metrics (accuracy, sensitivity, specificity, F1 score)\n",
    "def save_evaluation_metrics(true_labels, predicted_labels, cm, save_path): \n",
    "\n",
    "    accuracy = np.mean(predicted_labels == true_labels)  \n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')     \n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')   \n",
    "    f1 = f1_score(true_labels, predicted_labels, average='binary')                \n",
    "    roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "    # specificity = np.mean(np.diag(cm) / (np.diag(cm) + np.sum(cm, axis=0) - np.diag(cm)))   \n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Sensitivity (Recall)\": recall,\n",
    "        # \"Specificity: specificity,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Plot the metrics as bars\n",
    "    bars = plt.bar(metrics.keys(), metrics.values(), color=['darkturquoise', 'sandybrown', 'hotpink', 'limegreen', 'mediumpurple'])\n",
    "\n",
    "    # Annotate each bar with the corresponding value\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.4f}', ha='center', va='bottom')  # Add metric value at the top of the bar\n",
    "\n",
    "    plt.title(\"Model Evaluation Metrics\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    return metrics\n",
    "\n",
    "# save classification report\n",
    "def save_classification_report(true_labels, predicted_labels, class_names, save_path):\n",
    "    class_report = classification_report(true_labels, predicted_labels, target_names=class_names, digits=4)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(class_report)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    # Apply threshold of 0.5 to convert probabilities to binary predictions\n",
    "    predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')  \n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')        \n",
    "    f1 = f1_score(true_labels, predicted_labels, average='binary')                \n",
    "    auc = roc_auc_score(true_labels, predictions)                                 \n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Function to save metrics, loss curve, and confusion matrix for the best model\n",
    "def save_best_model_visuals(history, model, val_ds, class_names, weights_path, fold):\n",
    "    # Generate predictions for the validation set\n",
    "    val_predictions = model.predict(val_ds)\n",
    "    val_predicted_probs = val_predictions.flatten()  # Convert 2D predictions into 1D probabilities\n",
    "    val_predicted_ids = (val_predicted_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "    true_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    confusion_matrix_path = os.path.join(weights_path, \"confusion_matrix_fold.png\")\n",
    "    save_confusion_matrix(true_labels, val_predicted_ids, class_names, confusion_matrix_path)\n",
    "\n",
    "    # Loss Curve\n",
    "    loss_curve_path = os.path.join(weights_path, \"loss_curve_fold.png\")\n",
    "    save_loss_curve(history.history, loss_curve_path)\n",
    "\n",
    "    # Evaluation Metrics (Accuracy, Sensitivity, Specificity, F1 Score)\n",
    "    cm = confusion_matrix(true_labels, val_predicted_ids)\n",
    "    metrics_bar_chart_path = os.path.join(weights_path, \"evaluation_metrics_fold.png\")\n",
    "    save_evaluation_metrics(true_labels, val_predicted_ids, cm, metrics_bar_chart_path)\n",
    "\n",
    "    # Save classification report as a text file\n",
    "    classification_report_path = os.path.join(weights_path, \"classification_report_fold.txt\")\n",
    "    save_classification_report(true_labels, val_predicted_ids, class_names, classification_report_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training/validation set for hyperparameter tuning\n",
    "# train_images_tuning, val_images_tuning, train_labels_tuning, val_labels_tuning = train_test_split(\n",
    "#     image_paths, labels, test_size=0.1, random_state=42, stratify=labels\n",
    "# )\n",
    "\n",
    "# # Define the hypermodel for hyperparameter tuning\n",
    "# def build_model(hp):\n",
    "#     base_model = tf.keras.applications.MobileNetV2(\n",
    "#         input_shape=(img_height, img_width, 3),\n",
    "#         include_top=False,\n",
    "#         weights='imagenet'\n",
    "#     )\n",
    "#     base_model.trainable = False  # Freeze layers initially\n",
    "    \n",
    "#     model = Sequential([\n",
    "#         base_model,\n",
    "#         layers.GlobalAveragePooling2D(),\n",
    "#         layers.Dense(num_classes)\n",
    "#     ])\n",
    "\n",
    "#     # Tune hyperparameters\n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "#     optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "\n",
    "#     if optimizer == 'adam':\n",
    "#         opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     else:\n",
    "#         opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=opt,\n",
    "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Set up the tuner for hyperparameter tuning\n",
    "# tuner = kt.RandomSearch(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',  # Optimize for validation accuracy\n",
    "#     max_trials=10,             # Try 10 different hyperparameter combinations\n",
    "#     executions_per_trial=1,    # Run each combination once\n",
    "#     directory='hyperparameter_tuning',\n",
    "#     project_name='best_hyperparams_tuning'\n",
    "# )\n",
    "\n",
    "# # Prepare TensorFlow datasets for training and validation\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images_tuning, train_labels_tuning)).batch(batch_size)\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((val_images_tuning, val_labels_tuning)).batch(batch_size)\n",
    "\n",
    "# # Perform the hyperparameter search on the validation set\n",
    "# tuner.search(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "# # Get the best hyperparameters after the search\n",
    "# best_hyperparams = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(f\"Best Hyperparameters: {best_hyperparams.values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and compile the model\n",
    "def create_model(num_classes, config, fine_tune=None):\n",
    "    # if you are not fine tuning the model, instantiate a new model \n",
    "    if(fine_tune == False):         \n",
    "\n",
    "        model_name = config[\"model_name\"]\n",
    "\n",
    "        if model_name == \"mobilenet\":\n",
    "            base_model = tf.keras.applications.MobileNetV2\n",
    "        elif model_name == \"efficientnet\":\n",
    "            base_model = tf.keras.applications.EfficientNetB3\n",
    "        elif model_name == \"densenet\":\n",
    "            base_model = tf.keras.applications.DenseNet121\n",
    "        elif model_name == \"inceptionv3\":\n",
    "            base_model = tf.keras.applications.InceptionV3\n",
    "        elif model_name == \"resnet50\":\n",
    "            base_model = tf.keras.applications.ResNet50\n",
    "        elif model_name == \"vgg16\":\n",
    "            base_model = tf.keras.applications.VGG16\n",
    "        elif model_name == \"xception\":\n",
    "            base_model = tf.keras.applications.Xception\n",
    "        else:\n",
    "            raise ValueError(f\"Model name '{model_name}' is not supported.\")\n",
    "\n",
    "\n",
    "        # instantiate mobilenet (contains 154 layers)\n",
    "        base_model = base_model(\n",
    "            input_shape=(img_height, img_width, 3),     # set the input it will receive\n",
    "            include_top=False,                          # do not include top layer to perform transfer learning\n",
    "            weights='imagenet'                          # load weights from imagenet dataset\n",
    "        )\n",
    "        base_model.trainable = False                    # Freeze the base model\n",
    "        # print(len(base_model.layers))\n",
    "        # add a layer in order to perform classification on our dataset\n",
    "        model = Sequential([\n",
    "            base_model,                         # use base_model as the start of your model\n",
    "            layers.GlobalAveragePooling2D(),    # add a final layer to perform classification\n",
    "            layers.Dense(1)                     # set the number of possible prediction to the num of classes in dataset for multiclass classification and 1 for binary classification\n",
    "        ])\n",
    "        \n",
    "    # select optimizer and learning rate based on configuration\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {config['optimizer']}\")\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fine tune model by unfreezing the layers after the first fine_tune_at layers\n",
    "def fine_tune_model(base_model, fine_tune_at):\n",
    "    # Unfreeze the layers starting from fine_tune_at index\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/1 with config: {'learning_rate': 0.001, 'optimizer': 'adam', 'epochs': 50, 'save_metrics': True, 'folds': 5, 'fine_tune': False, 'fine_tune_epochs': 25, 'fine_tune_at': 150}\n",
      "\n",
      "Fold 1/5...\n",
      "Training with frozen base layers for 50 epochs...\n",
      "Epoch 1/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 304ms/step - accuracy: 0.6290 - loss: 0.6206 - precision: 0.7218 - val_accuracy: 0.7983 - val_loss: 0.4485 - val_precision: 0.8507 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 241ms/step - accuracy: 0.8073 - loss: 0.4042 - precision: 0.8754 - val_accuracy: 0.8330 - val_loss: 0.3846 - val_precision: 0.8725 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 247ms/step - accuracy: 0.8533 - loss: 0.3427 - precision: 0.9049 - val_accuracy: 0.8626 - val_loss: 0.3487 - val_precision: 0.8867 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 247ms/step - accuracy: 0.8824 - loss: 0.3054 - precision: 0.9161 - val_accuracy: 0.8730 - val_loss: 0.3241 - val_precision: 0.8914 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 231ms/step - accuracy: 0.8892 - loss: 0.2791 - precision: 0.9201 - val_accuracy: 0.8870 - val_loss: 0.3057 - val_precision: 0.9042 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 240ms/step - accuracy: 0.9011 - loss: 0.2591 - precision: 0.9285 - val_accuracy: 0.8887 - val_loss: 0.2912 - val_precision: 0.9019 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 237ms/step - accuracy: 0.9082 - loss: 0.2431 - precision: 0.9326 - val_accuracy: 0.8974 - val_loss: 0.2794 - val_precision: 0.9060 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 231ms/step - accuracy: 0.9155 - loss: 0.2299 - precision: 0.9354 - val_accuracy: 0.9043 - val_loss: 0.2696 - val_precision: 0.9097 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.9189 - loss: 0.2187 - precision: 0.9394 - val_accuracy: 0.9043 - val_loss: 0.2613 - val_precision: 0.9097 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 243ms/step - accuracy: 0.9221 - loss: 0.2091 - precision: 0.9412 - val_accuracy: 0.9078 - val_loss: 0.2541 - val_precision: 0.9102 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 237ms/step - accuracy: 0.9270 - loss: 0.2007 - precision: 0.9443 - val_accuracy: 0.9130 - val_loss: 0.2478 - val_precision: 0.9136 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 247ms/step - accuracy: 0.9277 - loss: 0.1932 - precision: 0.9446 - val_accuracy: 0.9200 - val_loss: 0.2422 - val_precision: 0.9224 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9300 - loss: 0.1862 - precision: 0.9457"
     ]
    }
   ],
   "source": [
    "train_metrics = []      # list to save training metrics\n",
    "val_metrics = []        # list to save validation metrics\n",
    "normalization_layer = layers.Rescaling(1.0 / 255)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).batch(32)\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"Training model {i + 1}/{len(configs)} with config: {config}\")\n",
    "\n",
    "    # K-fold Cross Validation\n",
    "    kfold = StratifiedKFold(n_splits=config['folds'], shuffle=True, random_state=42)\n",
    "    best_val_f1score = -float('inf')            # Initialize best F1 score with a very low value\n",
    "\n",
    "    # Define the base path for saving models\n",
    "    model_subdir = os.path.join(save_dir, f'model{i + 1}')\n",
    "    os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "    # Define the base path for saving checkpoints for model\n",
    "    checkpoint_folder = os.path.join(model_subdir, 'checkpoints')\n",
    "    os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "    # Define the base path for saving cthe model with the best f1-score\n",
    "    best_f1_dir = os.path.join(model_subdir, 'best_f1score_fold')\n",
    "    os.makedirs(best_f1_dir, exist_ok=True)\n",
    "    \n",
    "    # Training and validation loop for each fold\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kfold.split(train_val_images, train_val_labels):\n",
    "        print(f\"\\nFold {fold}/{config['folds']}...\")\n",
    "\n",
    "        checkpoint_filepath = os.path.join(checkpoint_folder, f'checkpoint_fold{fold}.weights.h5')\n",
    "\n",
    "        # Create subset datasets for training and validation\n",
    "        train_images, train_labels = train_val_images[train_idx], train_val_labels[train_idx]\n",
    "        val_images, val_labels = train_val_images[val_idx], train_val_labels[val_idx]\n",
    "\n",
    "        # Convert NumPy arrays back to TensorFlow datasets\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "\n",
    "        # Normalize datasets \n",
    "        normalization_layer = layers.Rescaling(1./255)\n",
    "        train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "        val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "        # prefetch data to improve performance by overlapping data preprocessing and model execution and cache the dataset in memory and batch\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        train_ds = train_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        val_ds = val_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        # Step 1: Train model with frozen layers\n",
    "        print(f\"Training with frozen base layers for {config['epochs']} epochs...\")\n",
    "\n",
    "        # Create and compile model for each fold\n",
    "        model = create_model(num_classes, config, fine_tune=False) \n",
    "\n",
    "        # setup callbacks \n",
    "        early_stopping, model_checkpoint, reduce_lr = callbacks_setup(checkpoint_filepath)\n",
    "\n",
    "        # train the model on the training set until the epochs specified\n",
    "        history_frozen = model.fit(\n",
    "            train_ds,                                       # dataset used for training\n",
    "            validation_data=val_ds,                         # dataset used for validation\n",
    "            epochs=config['epochs'],                        # epochs used for training\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],   # set early stopping to avoid overfitting\n",
    "            verbose=1\n",
    "        ) \n",
    "\n",
    "        # load the best weights from ModelCheckpoint after training\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "\n",
    "        if(config[\"fine_tune\"] == True):\n",
    "            # Step 2: Unfreeze layers and fine-tune\n",
    "            print(f\"Unfreezing layers starting from layer {config['fine_tune_at']} for fine-tuning...\")\n",
    "            fine_tune_model(model.layers[0], config['fine_tune_at'])      # fine tune model\n",
    "\n",
    "            # re-compile the model with a lower learning rate for fine-tuning\n",
    "            fine_tune_lr = config['learning_rate'] * 0.01\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    "            )\n",
    "                \n",
    "            print(f\"Fine-tuning for {config['fine_tune_epochs']} epochs...\")\n",
    "\n",
    "            # setup callbacks again for fine-tuning phase with a unique checkpoint\n",
    "            early_stopping, model_checkpoint, reduce_lr = callbacks_setup(checkpoint_filepath)\n",
    "            \n",
    "            history_fine_tune = model.fit(\n",
    "                train_ds,                                       # dataset used for training\n",
    "                validation_data=val_ds,                         # dataset used for validation\n",
    "                epochs=config['fine_tune_epochs'],                        # epochs used for training\n",
    "                callbacks=[early_stopping, model_checkpoint, reduce_lr],   # set early stopping to avoid overfitting\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # load weights after fine-tuning\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "\n",
    "        # evaluate on validation set after training\n",
    "        val_predictions = model.predict(val_ds)\n",
    "        avg_val_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "        avg_val_accuracy, avg_val_precision, avg_val_recall, avg_val_f1, avg_val_auc = calculate_metrics(\n",
    "            np.concatenate([y for x, y in val_ds]), val_predictions\n",
    "        )\n",
    "\n",
    "        print(f\"\\nValidation: \\tFold {fold} - Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_accuracy:.4f}, Precision: {avg_val_precision:.4f}, Recall: {avg_val_recall:.4f}, F1 Score: {avg_val_f1:.4f}, ROC AUC Score: {avg_val_auc:.4f}\")\n",
    "\n",
    "        test_predictions = model.predict(test_ds)\n",
    "        predicted_labels = np.argmax(test_predictions, axis=-1)\n",
    "        true_labels = np.concatenate([y for _, y in test_ds], axis=0)\n",
    "\n",
    "        avg_test_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "        # save the best model based on validation F1 score\n",
    "        if avg_test_f1 > best_val_f1score:\n",
    "            best_val_f1score = avg_test_f1\n",
    "\n",
    "            # Save model in TensorFlow SavedModel format (for TensorRT compatibility)\n",
    "            # model.save(best_f1_dir)\n",
    "            # tf.saved_model.save(model, best_f1_dir)\n",
    "            model.export(best_f1_dir)\n",
    "            print(f\"Model with best F1 score during Validation saved at Fold {fold} with F1 Score of {best_val_f1score:.4f}\")\n",
    "\n",
    "            if (config['save_metrics'] == True):\n",
    "                #save confusion matrix, loss curve, evaluation metrics for the best model\n",
    "                history = history_frozen\n",
    "                save_best_model_visuals(history, model, test_ds, class_names, model_subdir, fold)\n",
    "\n",
    "        fold += 1       # Move to the next fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the best model for model1 on the held-out test set...\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "\n",
      "Test Set Evaluation - Accuracy: 0.8906, Precision: 0.8944, Recall: 0.9096, F1 Score: 0.9020, AUC Score: 0.9681\n",
      "\n",
      "Evaluating the best model for model2 on the held-out test set...\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 464ms/step\n",
      "\n",
      "Test Set Evaluation - Accuracy: 0.7344, Precision: 0.7190, Recall: 0.8531, F1 Score: 0.7804, AUC Score: 0.7584\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(configs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# model = tf.keras.models.load_model(f'../saved_models/model{i+1}/best_f1score_fold')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the SavedModel using TFSMLayer, treating it as a Keras layer\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     model_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFSMLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../saved_models/model\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/best_f1score_fold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserving_default\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Wrap the TFSMLayer in a Sequential model for inference\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([model_layer])\n",
      "File \u001b[1;32mc:\\Users\\jacob.delgado\\.conda\\envs\\capstone\\Lib\\site-packages\\keras\\src\\export\\export_lib.py:735\u001b[0m, in \u001b[0;36mTFSMLayer.__init__\u001b[1;34m(self, filepath, call_endpoint, call_training_endpoint, trainable, name, dtype)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# Initialize an empty layer, then add_weight() etc. as needed.\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(trainable\u001b[38;5;241m=\u001b[39mtrainable, name\u001b[38;5;241m=\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reloaded_obj \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;241m=\u001b[39m filepath\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_endpoint \u001b[38;5;241m=\u001b[39m call_endpoint\n",
      "File \u001b[1;32mc:\\Users\\jacob.delgado\\.conda\\envs\\capstone\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:912\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m    911\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[1;32m--> 912\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\jacob.delgado\\.conda\\envs\\capstone\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:1016\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, \u001b[38;5;28mset\u001b[39m):\n\u001b[0;32m   1012\u001b[0m   \u001b[38;5;66;03m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m   \u001b[38;5;66;03m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m   tags \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(tags)\n\u001b[0;32m   1015\u001b[0m saved_model_proto, debug_info \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1016\u001b[0m     \u001b[43mloader_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_saved_model_with_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1018\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m     saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_graph_def\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\jacob.delgado\\.conda\\envs\\capstone\\Lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:62\u001b[0m, in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m  parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m parse_saved_model(export_dir)\n\u001b[0;32m     61\u001b[0m debug_info_path \u001b[38;5;241m=\u001b[39m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mpath_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_debug_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     63\u001b[0m     constants\u001b[38;5;241m.\u001b[39mDEBUG_INFO_FILENAME_PB)\n\u001b[0;32m     64\u001b[0m debug_info \u001b[38;5;241m=\u001b[39m graph_debug_info_pb2\u001b[38;5;241m.\u001b[39mGraphDebugInfo()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_io\u001b[38;5;241m.\u001b[39mfile_exists(debug_info_path):\n",
      "File \u001b[1;32mc:\\Users\\jacob.delgado\\.conda\\envs\\capstone\\Lib\\site-packages\\tensorflow\\python\\saved_model\\path_helpers.py:79\u001b[0m, in \u001b[0;36mget_debug_dir\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_saved_model_pb_path\u001b[39m(export_dir):\n\u001b[0;32m     74\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     75\u001b[0m       compat\u001b[38;5;241m.\u001b[39mas_bytes(compat\u001b[38;5;241m.\u001b[39mpath_to_str(export_dir)),\n\u001b[0;32m     76\u001b[0m       compat\u001b[38;5;241m.\u001b[39mas_bytes(constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB))\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_debug_dir\u001b[39m(export_dir):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns path to the debug sub-directory in the SavedModel.\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     82\u001b[0m       compat\u001b[38;5;241m.\u001b[39mas_text(export_dir), compat\u001b[38;5;241m.\u001b[39mas_text(constants\u001b[38;5;241m.\u001b[39mDEBUG_DIRECTORY))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(configs):\n",
    "    # model = tf.keras.models.load_model(f'../saved_models/model{i+1}/best_f1score_fold')\n",
    "\n",
    "    # Load the SavedModel using TFSMLayer, treating it as a Keras layer\n",
    "    model_layer = tf.keras.layers.TFSMLayer(f'../saved_models/model{i+1}/best_f1score_fold', call_endpoint='serving_default')\n",
    "    \n",
    "    # Wrap the TFSMLayer in a Sequential model for inference\n",
    "    model = tf.keras.Sequential([model_layer])\n",
    "\n",
    "    # once training is complete, evaluate on the held-out test set\n",
    "    print(f\"Evaluating the best model for model{i+1} on the held-out test set...\")\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "    test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y)).batch(batch_size)\n",
    "\n",
    "    test_predictions = model.predict(test_ds)\n",
    "\n",
    "    # Print the shape and type of predictions for debugging\n",
    "    # print(f\"Predictions shape: {len(test_predictions['output_0'])}, type: {type(test_predictions)}\")\n",
    "    # print(f\"First few predictions: {test_predictions[:5]}\")  # Check the first few predictions\n",
    "\n",
    "    # avg_test_loss = model.evaluate(test_ds, verbose=0)[0]\n",
    "    avg_test_accuracy, avg_test_precision, avg_test_recall, avg_test_f1, avg_test_auc = calculate_metrics(\n",
    "        np.concatenate([y for x, y in test_ds]), test_predictions['output_0']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest Set Evaluation - Accuracy: {avg_test_accuracy:.4f}, Precision: {avg_test_precision:.4f}, Recall: {avg_test_recall:.4f}, F1 Score: {avg_test_f1:.4f}, AUC Score: {avg_test_auc:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
