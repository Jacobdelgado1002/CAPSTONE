{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:23:41.389136: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-11 21:23:41.398180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-11 21:23:41.408545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-11 21:23:41.411659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 21:23:41.420158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 21:23:41.860713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2656019838846194818\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 842989568\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6296614559756178266\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 22745120768\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14871333033358220386\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:82:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 2144165316\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739323422.461500  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.461665  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.491781  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.491975  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.492091  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.492194  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.498110  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.498242  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.498369  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.498476  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.498584  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-11 21:23:42.498690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n",
      "I0000 00:00:1739323422.498973  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-11 21:23:42.499077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:1 with 21691 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:82:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with MonkeyPox Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path(\"../data/Augmented_Images\")    # points to the folder containing the images that will be used for training\n",
    "\n",
    "# hyperparameters\n",
    "img_height = 224        # input image height\n",
    "img_width = 224         # input image width\n",
    "batch_size = 32         # size of the batch that will be fed to model\n",
    "\n",
    "# folds = the amount of folds that will be created for cross-validation\n",
    "# fine_tune_epochs = number of epochs after which we start fine-tuning\n",
    "# fine_tune_at = layer number where we start unfreezing layers\n",
    "\n",
    "# configurations that will be used in training\n",
    "configs = [\n",
    "    {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    # {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": True, \"fine_tune_epochs\": 25, \"fine_tune_at\": 148},\n",
    "    # {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    # {\"model_name\": \"mobilenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": True, \"fine_tune_epochs\": 25, \"fine_tune_at\": 148},\n",
    "\n",
    "    # {\"model_name\": \"efficientnet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "    \n",
    "    # {\"model_name\": \"densenet\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "\n",
    "    # {\"model_name\": \"inceptionv3\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "\n",
    "    # {\"model_name\": \"resnet50\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "\n",
    "    # {\"model_name\": \"vgg16\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 15\n",
    "\n",
    "    # {\"model_name\": \"xception\", \"learning_rate\": 0.001, \"batch_size\": 32, \"image_size\" : 224, \"optimizer\": \"adam\", \"epochs\": 50, \"save_metrics\": True, \"folds\": 5, \"fine_tune\": False, \"fine_tune_epochs\": 25, \"fine_tune_at\": 150},\n",
    "]\n",
    "\n",
    "# Define the base path for saving models\n",
    "save_dir = \"../saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_dataset(directory: str, batch_size: int, image_size: tuple[int, int],\n",
    "                       label_mode, shuffle: bool = True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Loads an image dataset from the specified directory using Keras' image_dataset_from_directory.\n",
    "\n",
    "    The directory should contain one subdirectory per class.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the image data directory.\n",
    "        batch_size (int): Number of images per batch.\n",
    "        image_size (Tuple[int, int]): Target size (height, width) for the images.\n",
    "        label_mode (str): Type of label encoding ('binary', 'categorical', or 'int'). Defaults to 'binary'.\n",
    "        shuffle (bool): Whether to shuffle the data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A dataset yielding batches of (image, label) pairs with images normalized.\n",
    "    \"\"\"\n",
    "    # Load images and labels from the directory, inferring subdirectory names as class names.\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels='inferred',\n",
    "        label_mode=\"int\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=(image_size, image_size),\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    # print(dataset.class_names)\n",
    "    class_names = np.array(dataset.class_names)     # get the class names for the data\n",
    "    # num_classes = len(class_names)                  # get the number of classes in the dataset\n",
    "    # Normalize images to the [0, 1] range using a rescaling layer.\n",
    "    normalization_layer = layers.Rescaling(1.0 / 255)\n",
    "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "    # Cache and prefetch data to optimize performance.\n",
    "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset, class_names\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def create_augmentation_layer() -> tf.keras.Sequential:\n",
    "    \"\"\"\n",
    "    Creates a data augmentation pipeline using Keras layers.\n",
    "    Augmentations include:\n",
    "    - Random flipping (horizontal & vertical)\n",
    "    - Small rotations\n",
    "    - Zooming\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),  # Random rotation (up to 20% of 360 degrees)\n",
    "        tf.keras.layers.RandomZoom(0.1),  # Random zoom (up to 10% zoom)\n",
    "        tf.keras.layers.RandomTranslation(0.1, 0.1),  # Random translation (up to 10% in both directions)\n",
    "        # tf.keras.layers.GaussianNoise(0.1),  # Add Gaussian noise to the images\n",
    "    ])\n",
    "\n",
    "def oversample_minority_classes(images: np.ndarray, labels: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Oversamples minority classes by duplicating their samples up to the majority class count.\n",
    "\n",
    "    Args:\n",
    "        images (np.ndarray): Numpy array of images.\n",
    "        labels (np.ndarray): Numpy array of labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Balanced images and labels.\n",
    "    \"\"\"\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    max_samples = max(class_counts)  # Find the class with the most images\n",
    "    augmented_images, augmented_labels = [], []\n",
    "\n",
    "    augmentation_layer = create_augmentation_layer()\n",
    "\n",
    "    for class_label in unique_classes:\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "        class_images = images[class_indices]\n",
    "        class_labels = labels[class_indices]\n",
    "\n",
    "        # Oversample minority classes\n",
    "        while len(class_images) < max_samples:\n",
    "            oversampled_images = resample(class_images, n_samples=min(max_samples - len(class_images), len(class_images)), replace=True)\n",
    "            augmented_samples = augmentation_layer(oversampled_images, training=True)  # Apply augmentation\n",
    "            class_images = np.concatenate([class_images, augmented_samples.numpy()], axis=0)\n",
    "\n",
    "        augmented_images.append(class_images)\n",
    "        augmented_labels.append(np.full(len(class_images), class_label))\n",
    "\n",
    "    return np.concatenate(augmented_images, axis=0), np.concatenate(augmented_labels, axis=0)\n",
    "\n",
    "def create_balanced_dataset(images: np.ndarray, labels: np.ndarray, batch_size: int = 32) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from balanced images and labels, applying augmentation dynamically.\n",
    "\n",
    "    Args:\n",
    "        images (np.ndarray): Array of images.\n",
    "        labels (np.ndarray): Array of labels.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: TensorFlow dataset.\n",
    "    \"\"\"\n",
    "    images, labels = oversample_minority_classes(images, labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    dataset = dataset.shuffle(len(images))\n",
    "    # dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def callbacks_setup(checkpoint_filepath):\n",
    "    # EarlyStopping callback configuration\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',        # monitor validation loss\n",
    "        patience=6,                # number of epochs with no improvement to stop training\n",
    "        mode = 'min',              # want to minimize what it being monitored \n",
    "        restore_best_weights=False # don't restore in EarlyStopping, handled by ModelCheckpoint\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,   # path to save weights\n",
    "        save_weights_only=True,         # only save weights instead of full model\n",
    "        monitor='val_loss',             # monitor validation loss\n",
    "        mode='min',                     # want to maximize what is being monitored\n",
    "        save_best_only=True             # save the best weights\n",
    "    )            \n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',      # monitor validation loss \n",
    "        factor=0.5,              # factor by which the learning rate will be reduced \n",
    "        patience=4,              # number of epochs with no improvement to stop training \n",
    "        mode='min',              # want to minimize what it being monitored \n",
    "        min_lr=1e-6              # lower bound on the learning rate \n",
    "    )            \n",
    "\n",
    "    return early_stopping, model_checkpoint, reduce_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def save_confusion_matrix(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                          class_names: List[str], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the confusion matrix for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels.\n",
    "        class_names (List[str]): List of class names corresponding to class indices.\n",
    "        save_path (str): Path to save the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix using sklearn\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    # Plot with adjustments\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Adjust figure size\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=12)\n",
    "    ax.set_ylabel(\"True label\", fontsize=12)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # Prevent labels from being cut off\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and close plot\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_loss_curve(history: Dict[str, Any], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the training and validation loss curves.\n",
    "\n",
    "    Args:\n",
    "        history (Dict[str, Any]): Dictionary containing training history (loss values).\n",
    "        save_path (str): Path to save the loss curve plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_roc_auc(true_labels: np.ndarray, predicted_probs: np.ndarray, class_names: list, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Plots and saves the ROC AUC curve for multi-class classification.\n",
    "    \n",
    "    Args:\n",
    "        true_labels (np.ndarray): True class labels.\n",
    "        predicted_probs (np.ndarray): Predicted class probabilities.\n",
    "        class_names (list): List of class names.\n",
    "        save_path (str, optional): Path to save the ROC curve plot. Defaults to None.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        fpr, tpr, _ = roc_curve(true_labels == i, predicted_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC AUC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def save_evaluation_metrics(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                            predicted_probs: np.ndarray, save_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for multi-class classification and saves a bar chart.\n",
    "    The metrics include accuracy, precision, recall, F1 score, and ROC AUC.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels.\n",
    "        predicted_probs (np.ndarray): Array of predicted probabilities (shape: [n_samples, n_classes]).\n",
    "        save_path (str): Path to save the evaluation metrics bar chart.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing computed metrics.\n",
    "    \"\"\"\n",
    "    # Calculate accuracy by comparing predicted and true labels\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    # Compute macro-averaged metrics for multi-class classification\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    # For ROC AUC, first binarize the true labels to one-hot encoding\n",
    "    n_classes = predicted_probs.shape[1]\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=list(range(n_classes)))\n",
    "    # Compute ROC AUC with a one-vs-rest approach and macro average\n",
    "    roc_auc = roc_auc_score(true_labels_binarized, predicted_probs, multi_class='ovr', average='macro')\n",
    "\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Sensitivity (Recall)\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "    # Plot metrics as a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(metrics.keys(), metrics.values(), \n",
    "                   color=['darkturquoise', 'sandybrown', 'hotpink', 'limegreen', 'mediumpurple'])\n",
    "    # Annotate each bar with its value\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.4f}', ha='center', va='bottom')\n",
    "    plt.title(\"Model Evaluation Metrics\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_classification_report(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                               class_names: List[str], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the classification report to a text file for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels.\n",
    "        class_names (List[str]): List of class names.\n",
    "        save_path (str): Path to save the classification report.\n",
    "    \"\"\"\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=class_names, digits=4)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "def calculate_metrics(true_labels: np.ndarray, predictions: np.ndarray) -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculates evaluation metrics for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels.\n",
    "        predictions (np.ndarray): Array of predicted probabilities (shape: [n_samples, n_classes]).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float, float]: A tuple containing accuracy, precision, recall, \n",
    "            F1 score, and ROC AUC score.\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to predicted class labels using argmax\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    # Binarize true labels for ROC AUC calculation\n",
    "    n_classes = predictions.shape[1]\n",
    "    true_labels_binarized = label_binarize(true_labels, classes=list(range(n_classes)))\n",
    "    auc = roc_auc_score(true_labels_binarized, predictions, multi_class='ovr', average='macro')\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "def save_best_model_visuals(history: tf.keras.callbacks.History, model: tf.keras.Model, \n",
    "                              val_ds: tf.data.Dataset, class_names: List[str], \n",
    "                              weights_path: str, fold: int) -> None:\n",
    "    \"\"\"\n",
    "    Generates and saves evaluation visuals including confusion matrix, loss curve, evaluation \n",
    "    metrics bar chart, and classification report for the best performing model in a given fold.\n",
    "\n",
    "    Args:\n",
    "        history (tf.keras.callbacks.History): Training history object.\n",
    "        model (tf.keras.Model): Trained model.\n",
    "        val_ds (tf.data.Dataset): Validation dataset.\n",
    "        class_names (List[str]): List of class names.\n",
    "        weights_path (str): Directory path to save visuals.\n",
    "        fold (int): Current fold number.\n",
    "    \"\"\"\n",
    "    # Generate predictions (predicted probabilities) for the validation set\n",
    "    val_predictions = model.predict(val_ds)\n",
    "    # Convert predicted probabilities to class labels using argmax\n",
    "    val_predicted_ids = np.argmax(val_predictions, axis=1)\n",
    "    # Concatenate true labels from the validation dataset\n",
    "    true_labels = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    confusion_matrix_path = os.path.join(weights_path, f\"confusion_matrix_fold{fold}.png\")\n",
    "    save_confusion_matrix(true_labels, val_predicted_ids, class_names, confusion_matrix_path)\n",
    "\n",
    "    # Save the loss curve using the training history\n",
    "    loss_curve_path = os.path.join(weights_path, f\"loss_curve_fold{fold}.png\")\n",
    "    save_loss_curve(history.history, loss_curve_path)\n",
    "\n",
    "    # Save the roc auc curve using the training history\n",
    "    roc_auc_curve_path = os.path.join(weights_path, f\"roc_auc_curve_fold{fold}.png\")\n",
    "    save_roc_auc(true_labels, val_predictions, class_names, roc_auc_curve_path)\n",
    "\n",
    "    # Save evaluation metrics bar chart (passing predicted probabilities for ROC AUC calculation)\n",
    "    metrics_bar_chart_path = os.path.join(weights_path, f\"evaluation_metrics_fold{fold}.png\")\n",
    "    save_evaluation_metrics(true_labels, val_predicted_ids, val_predictions, metrics_bar_chart_path)\n",
    "\n",
    "    # Save the classification report as a text file\n",
    "    classification_report_path = os.path.join(weights_path, f\"classification_report_fold{fold}.txt\")\n",
    "    save_classification_report(true_labels, val_predicted_ids, class_names, classification_report_path)\n",
    "\n",
    "def compute_cv_statistics(accuracies, precisions, recalls, f1_scores, use_std_error=False, output_filepath=None):\n",
    "    \"\"\"\n",
    "    Compute the mean and variation (std dev or standard error) for each metric and optionally save the results to a file.\n",
    "    \n",
    "    Parameters:\n",
    "        accuracies (list of float): Accuracy values for each fold.\n",
    "        precisions (list of float): Precision values for each fold.\n",
    "        recalls (list of float): Recall values for each fold.\n",
    "        f1_scores (list of float): F1 score values for each fold.\n",
    "        use_std_error (bool): If True, compute standard error (std / sqrt(n)); otherwise, compute standard deviation.\n",
    "        output_filepath (str): If provided, the path to a text file where the metrics will be saved.\n",
    "    \n",
    "    The function prints each metric in the format:\n",
    "        Metric: mean ± variation\n",
    "    (Metrics are multiplied by 100 to display percentages.)\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Accuracy\": np.array(accuracies),\n",
    "        \"Precision\": np.array(precisions),\n",
    "        \"Recall\": np.array(recalls),\n",
    "        \"F1 Score\": np.array(f1_scores)\n",
    "    }\n",
    "    \n",
    "    output_lines = []\n",
    "    for metric_name, values in metrics.items():\n",
    "        mean_val = np.mean(values)\n",
    "        # Use ddof=1 for sample standard deviation\n",
    "        variation = np.std(values, ddof=1)\n",
    "        if use_std_error:\n",
    "            variation /= np.sqrt(len(values))\n",
    "        # Format the output as percentages; adjust if your metrics are already in percentage\n",
    "        output_lines.append(f\"{metric_name}: {mean_val*100:.2f} ± {variation*100:.2f}\\n\")\n",
    "    \n",
    "    metrics_str = \"\".join(output_lines)\n",
    "    \n",
    "    # Print the overall metrics to the console\n",
    "    print(\"\\nOverall Cross-Validation Metrics:\")\n",
    "    print(metrics_str)\n",
    "    \n",
    "    # Save the metrics to a text file if an output file path is provided\n",
    "    if output_filepath:\n",
    "        with open(output_filepath, \"w\") as f:\n",
    "            f.write(\"Overall Cross-Validation Metrics:\\n\")\n",
    "            f.write(metrics_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix_binary(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                          save_path: str, mpox_index: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Converts multi-class labels to binary (Mpox vs Other) and plots/saves the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels (multi-class integers).\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels (multi-class integers).\n",
    "        save_path (str): Path to save the confusion matrix plot.\n",
    "        mpox_index (int): The index corresponding to Mpox. All other labels are considered \"Other\".\n",
    "    \"\"\"\n",
    "    # Convert multi-class labels to binary: 1 if label equals mpox_index, else 0.\n",
    "    binary_true = (true_labels == mpox_index).astype(int)\n",
    "    binary_pred = (predicted_labels == mpox_index).astype(int)\n",
    "    cm = confusion_matrix(binary_true, binary_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Other\", \"Mpox\"])\n",
    "    \n",
    "    # Plot with adjustments\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  # Adjust figure size\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=12)\n",
    "    ax.set_ylabel(\"True label\", fontsize=12)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "    # Prevent labels from being cut off\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and close plot\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_loss_curve_binary(history: Dict[str, Any], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots and saves the training and validation loss curves.\n",
    "\n",
    "    Args:\n",
    "        history (Dict[str, Any]): Dictionary containing training history (loss values).\n",
    "        save_path (str): Path to save the loss curve plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_evaluation_metrics_binary(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                            predicted_probs: np.ndarray, save_path: str, \n",
    "                            mpox_index: int = 0) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes and plots evaluation metrics for binary classification (Mpox vs Other).\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels (multi-class integers).\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels (multi-class integers).\n",
    "        predicted_probs (np.ndarray): Array of predicted probabilities for each class \n",
    "                                      (shape: [n_samples, n_classes]).\n",
    "        save_path (str): Path to save the evaluation metrics bar chart.\n",
    "        mpox_index (int): The index corresponding to Mpox.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing computed metrics.\n",
    "    \"\"\"\n",
    "    # Convert to binary labels\n",
    "    binary_true = (true_labels == mpox_index).astype(int)\n",
    "    binary_pred = (predicted_labels == mpox_index).astype(int)\n",
    "    # Use the probability for the Mpox class as the positive probability.\n",
    "    mpox_probs = predicted_probs[:, mpox_index]\n",
    "\n",
    "    accuracy = np.mean(binary_true == binary_pred)\n",
    "    precision = precision_score(binary_true, binary_pred)\n",
    "    recall = recall_score(binary_true, binary_pred)\n",
    "    f1 = f1_score(binary_true, binary_pred)\n",
    "    roc_auc = roc_auc_score(binary_true, mpox_probs)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Sensitivity (Recall)\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    }\n",
    "\n",
    "    # Plot metrics as a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(metrics.keys(), metrics.values(), \n",
    "                   color=['darkturquoise', 'sandybrown', 'hotpink', 'limegreen', 'mediumpurple'])\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.4f}', ha='center', va='bottom')\n",
    "    plt.title(\"Evaluation Metrics (Mpox vs Other)\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_classification_report_binary(true_labels: np.ndarray, predicted_labels: np.ndarray, \n",
    "                               save_path: str, mpox_index: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Saves the classification report for binary classification (Mpox vs Other).\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels (multi-class integers).\n",
    "        predicted_labels (np.ndarray): Array of predicted class labels (multi-class integers).\n",
    "        save_path (str): Path to save the classification report.\n",
    "        mpox_index (int): The index corresponding to Mpox.\n",
    "    \"\"\"\n",
    "    binary_true = (true_labels == mpox_index).astype(int)\n",
    "    binary_pred = (predicted_labels == mpox_index).astype(int)\n",
    "    report = classification_report(binary_true, binary_pred, target_names=[\"Other\", \"Mpox\"], digits=4)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "def calculate_metrics_binary(true_labels: np.ndarray, predictions: np.ndarray, \n",
    "                      mpox_index: int = 0) -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculates binary evaluation metrics for Mpox vs Other.\n",
    "    The multi-class predictions are converted into binary predictions where the positive class \n",
    "    is Mpox (identified by mpox_index) and all other classes are negative.\n",
    "\n",
    "    Args:\n",
    "        true_labels (np.ndarray): Array of true class labels (multi-class integers).\n",
    "        predictions (np.ndarray): Array of predicted probabilities (shape: [n_samples, n_classes]).\n",
    "        mpox_index (int): The index corresponding to Mpox.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float, float]:\n",
    "            Accuracy, Precision, Recall, F1 Score, and ROC AUC.\n",
    "    \"\"\"\n",
    "    # Convert multi-class predictions to class indices\n",
    "    predicted_labels_multi = np.argmax(predictions, axis=1)\n",
    "    # Convert to binary: 1 if Mpox, 0 otherwise.\n",
    "    binary_true = (true_labels == mpox_index).astype(int)\n",
    "    binary_pred = (predicted_labels_multi == mpox_index).astype(int)\n",
    "    mpox_probs = predictions[:, mpox_index]\n",
    "\n",
    "    accuracy = np.mean(binary_true == binary_pred)\n",
    "    precision = precision_score(binary_true, binary_pred)\n",
    "    recall = recall_score(binary_true, binary_pred)\n",
    "    f1 = f1_score(binary_true, binary_pred)\n",
    "    auc = roc_auc_score(binary_true, mpox_probs)\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "def save_best_model_visuals_binary(history: tf.keras.callbacks.History, model: tf.keras.Model, \n",
    "                              val_ds: tf.data.Dataset, weights_path: str, \n",
    "                              fold: int, mpox_index: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    Generates and saves evaluation visuals (confusion matrix, loss curve, metrics bar chart,\n",
    "    and classification report) for binary classification (Mpox vs Other) for the best performing model.\n",
    "\n",
    "    Args:\n",
    "        history (tf.keras.callbacks.History): Training history object.\n",
    "        model (tf.keras.Model): Trained model.\n",
    "        val_ds (tf.data.Dataset): Validation dataset.\n",
    "        weights_path (str): Directory path to save visuals.\n",
    "        fold (int): Current fold number.\n",
    "        mpox_index (int): The index corresponding to Mpox.\n",
    "    \"\"\"\n",
    "    # Generate predictions for the validation set\n",
    "    val_predictions = model.predict(val_ds)\n",
    "    predicted_ids_multi = np.argmax(val_predictions, axis=1)\n",
    "    true_labels = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "    \n",
    "    # Save the confusion matrix\n",
    "    confusion_matrix_path = os.path.join(weights_path, f\"confusion_matrix_binary_fold{fold}.png\")\n",
    "    save_confusion_matrix_binary(true_labels, predicted_ids_multi, confusion_matrix_path, mpox_index)\n",
    "    \n",
    "    # Save the loss curve\n",
    "    loss_curve_path = os.path.join(weights_path, f\"loss_curve_binary_fold{fold}.png\")\n",
    "    save_loss_curve_binary(history.history, loss_curve_path)\n",
    "    \n",
    "    # Save evaluation metrics bar chart\n",
    "    metrics_bar_chart_path = os.path.join(weights_path, f\"evaluation_metrics_binary_fold{fold}.png\")\n",
    "    save_evaluation_metrics_binary(true_labels, predicted_ids_multi, val_predictions, metrics_bar_chart_path, mpox_index)\n",
    "    \n",
    "    # Save the classification report\n",
    "    classification_report_path = os.path.join(weights_path, f\"classification_report_binary_fold{fold}.txt\")\n",
    "    save_classification_report_binary(true_labels, predicted_ids_multi, classification_report_path, mpox_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and compile the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "def create_model(num_classes: int, config: dict, fine_tune: bool = False) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Creates a MobileNetV2-based model for multi-class classification.\n",
    "\n",
    "    The model uses a pre-trained MobileNetV2 as a feature extractor (with ImageNet weights) \n",
    "    and adds a global average pooling layer followed by a dense layer that outputs logits \n",
    "    for each class. The model is compiled using SparseCategoricalCrossentropy (from_logits=True)\n",
    "    which is appropriate for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes for classification.\n",
    "        config (dict): Configuration dictionary containing:\n",
    "            - \"image_size\": Tuple[int, int] representing the target (height, width) for images.\n",
    "            - \"optimizer\": Optimizer name (\"adam\" or \"sgd\").\n",
    "            - \"learning_rate\": Learning rate for the optimizer.\n",
    "        fine_tune (bool, optional): If True, the base model will be set as trainable for fine-tuning.\n",
    "                                    Otherwise, the base model is frozen. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate the MobileNetV2 base model with pre-trained ImageNet weights.\n",
    "    # Exclude the top layer to allow for transfer learning.\n",
    "\n",
    "    model_name = config[\"model_name\"]\n",
    "\n",
    "    if model_name == \"mobilenet\":\n",
    "        base_model = tf.keras.applications.MobileNetV2\n",
    "    elif model_name == \"efficientnet\":\n",
    "        base_model = tf.keras.applications.EfficientNetB3\n",
    "    elif model_name == \"densenet\":\n",
    "        base_model = tf.keras.applications.DenseNet121\n",
    "    elif model_name == \"inceptionv3\":\n",
    "        base_model = tf.keras.applications.InceptionV3\n",
    "    elif model_name == \"resnet50\":\n",
    "        base_model = tf.keras.applications.ResNet50\n",
    "    elif model_name == \"vgg16\":\n",
    "        base_model = tf.keras.applications.VGG16\n",
    "    elif model_name == \"xception\":\n",
    "        base_model = tf.keras.applications.Xception\n",
    "    else:\n",
    "        raise ValueError(f\"Model name '{model_name}' is not supported.\")\n",
    "\n",
    "\n",
    "    base_model = base_model(\n",
    "        input_shape=(config[\"image_size\"], config[\"image_size\"], 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Set the trainability of the base model based on the fine_tune flag.\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # data_augmentation = tf.keras.Sequential([\n",
    "    #     layers.RandomFlip(\"horizontal\"),\n",
    "    #     layers.RandomRotation(0.1),\n",
    "    #     layers.RandomZoom(0.1)\n",
    "    # ])\n",
    "\n",
    "    # Build the model using the Keras Sequential API.\n",
    "    # For multi-class classification, the final Dense layer outputs 'num_classes' logits.\n",
    "    model = Sequential([\n",
    "        # data_augmentation,\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        # Dropout layer to randomly drop 50% of the neurons during training\n",
    "        # layers.Dropout(0.25),\n",
    "        # Dense layer with L2 regularization\n",
    "        # layers.Dense(num_classes, kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    # Select the optimizer based on configuration.\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {config['optimizer']}\")\n",
    "\n",
    "    # Compile the model with SparseCategoricalCrossentropy loss which expects logits.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# fine tune model by unfreezing the layers after the first fine_tune_at layers\n",
    "def fine_tune_model(base_model, fine_tune_at):\n",
    "    # Unfreeze the layers starting from fine_tune_at index\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_f1s = []\n",
    "\n",
    "def train_fold(fold: int, config: dict, best_f1_score: float, best_f1_dir: str, model_subdir: str) -> float:\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model for one fold of the dataset using images from the specified folders.\n",
    "    \n",
    "    Training images are loaded from 'Augmented Images/Fold{fold}/train',\n",
    "    validation images from 'Original Images/Fold{fold}/val', and test images from 'Original Images/Fold{fold}/test'.\n",
    "    \n",
    "    Args:\n",
    "        fold (int): The fold number to process.\n",
    "        config (dict): Configuration parameters including hyperparameters and paths.\n",
    "        best_f1_score (float): The best F1 score recorded so far.\n",
    "        best_f1_dir (str): Directory where the best model is saved.\n",
    "        model_subdir (str): Subdirectory for saving additional visuals and metrics.\n",
    "    \n",
    "    Returns:\n",
    "        float: The updated best F1 score after processing the current fold.\n",
    "    \"\"\"\n",
    "    # Define directory paths for the current fold\n",
    "    augmented_train_dir = os.path.join(\"../data\", \"MSLDV2\", \"Augmented Images\", \"Augmented Images\", \"FOLDS_AUG\", f\"fold{fold}_AUG\", \"Train\")\n",
    "    # augmented_train_dir = os.path.join(\"../data\", \"MSLDV2\", \"Original Images\", \"Original Images\", \"FOLDS\", f\"fold{fold}\", \"Train\")\n",
    "    original_val_dir = os.path.join(\"../data\", \"MSLDV2\", \"Original Images\", \"Original Images\", \"FOLDS\", f\"fold{fold}\", \"Valid\")\n",
    "    original_test_dir = os.path.join(\"../data\", \"MSLDV2\", \"Original Images\", \"Original Images\", \"FOLDS\", f\"fold{fold}\", \"Test\")\n",
    "\n",
    "    print(f\"\\nProcessing Fold {fold}...\")\n",
    "\n",
    "    # Load the training, validation, and test datasets\n",
    "    train_ds, class_names = load_image_dataset(augmented_train_dir, config['batch_size'],\n",
    "                                  config['image_size'], label_mode=config.get('label_mode', 'int'), shuffle=True)\n",
    "    val_ds, _ = load_image_dataset(original_val_dir, config['batch_size'],\n",
    "                                config['image_size'], label_mode=config.get('label_mode', 'int'), shuffle=False)\n",
    "    test_ds, _ = load_image_dataset(original_test_dir, config['batch_size'],\n",
    "                                 config['image_size'], label_mode=config.get('label_mode', 'int'), shuffle=False)\n",
    "    \n",
    "    # --- Compute class weights using sklearn ---\n",
    "    # Extract the labels from the training dataset.\n",
    "    # Note: train_ds is a tf.data.Dataset yielding (images, labels)\n",
    "    train_labels = np.concatenate([y.numpy() for _, y in train_ds], axis=0)\n",
    "    # Compute the class weights\n",
    "    classes = np.unique(train_labels)\n",
    "    computed_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "    # Create a dictionary mapping each class index to its weight.\n",
    "    class_weight_dict = {int(cls): weight for cls, weight in zip(classes, computed_weights)}\n",
    "    print(\"Class Weight Dictionary:\", class_weight_dict)\n",
    "    \n",
    "    # images, labels = [], []\n",
    "    # # Iterate over the dataset\n",
    "    # for batch_images, batch_labels in train_ds:\n",
    "    #     for img, lbl in zip(batch_images, batch_labels):  # Extract each image-label pair\n",
    "    #         images.append(img)\n",
    "    #         labels.append(lbl)\n",
    "\n",
    "    # # Ensure all images are stacked correctly\n",
    "    # images = np.array(tf.stack(images))  # Now all images should have the same shape\n",
    "    # labels = np.array(labels)  # Convert labels to a NumPy array\n",
    "\n",
    "    # train_ds = create_balanced_dataset(images, labels)\n",
    "\n",
    "    # Initialize dictionary to count samples per class\n",
    "    class_counts = Counter()\n",
    "\n",
    "    # Iterate through dataset and update class counts\n",
    "    for _, labels in train_ds:\n",
    "        class_counts.update(labels.numpy())  # Convert tensor to NumPy array and count occurrences\n",
    "\n",
    "    # Print the class distribution\n",
    "    print(\"Sample count per class in balanced dataset:\")\n",
    "    for class_label, count in sorted(class_counts.items()):\n",
    "        print(f\"Class {class_label}: {count} samples\")\n",
    "\n",
    "    print(\"______________________________________________\")\n",
    "\n",
    "\n",
    "    # Set up a filepath for model checkpointing for this fold\n",
    "    checkpoint_filepath = os.path.join(config['checkpoint_folder'], f'checkpoint_fold{fold}.weights.h5')\n",
    "\n",
    "    # -------------------- Step 1: Train with Frozen Base Layers --------------------\n",
    "    print(f\"Training with frozen base layers for {config['epochs']} epochs on Fold {fold}...\")\n",
    "\n",
    "    # Create the model for this fold (replace create_model with your actual model creation function)\n",
    "    model = create_model(len(class_names), config, fine_tune=False)\n",
    "    \n",
    "    # Setup callbacks (replace callbacks_setup with your callback setup function)\n",
    "    early_stopping, model_checkpoint, reduce_lr = callbacks_setup(checkpoint_filepath)\n",
    "    \n",
    "    # Train the model using the training and validation datasets, including class weights\n",
    "    history_frozen = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Load the best weights saved during training\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    # -------------------- Step 2: Fine-Tuning (Optional) --------------------\n",
    "    if config.get(\"fine_tune\", False):\n",
    "        print(f\"Unfreezing layers starting from layer {config['fine_tune_at']} for fine-tuning on Fold {fold}...\")\n",
    "        fine_tune_model(model.layers[0], config['fine_tune_at'])\n",
    "        # Set a lower learning rate for fine-tuning\n",
    "        fine_tune_lr = config['learning_rate'] * 0.01\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(f\"Fine-tuning for {config['fine_tune_epochs']} epochs on Fold {fold}...\")\n",
    "        early_stopping, model_checkpoint, reduce_lr = callbacks_setup(checkpoint_filepath)\n",
    "        history_fine_tune = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=config['fine_tune_epochs'],\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        # Load the best weights after fine-tuning\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    # -------------------- Evaluation on Validation Dataset --------------------\n",
    "    # Get predictions and compute metrics on the validation dataset\n",
    "    val_predictions = model.predict(val_ds)\n",
    "    avg_val_loss = model.evaluate(val_ds, verbose=0)[0]\n",
    "    avg_val_accuracy, avg_val_precision, avg_val_recall, avg_val_f1, avg_val_auc = calculate_metrics(\n",
    "        np.concatenate([y for _, y in val_ds]),  # combine labels from val_ds\n",
    "        val_predictions\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {fold} Validation Metrics:\")\n",
    "    print(f\"  Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {avg_val_precision:.4f}\")\n",
    "    print(f\"  Recall: {avg_val_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {avg_val_f1:.4f}\")\n",
    "    print(f\"  ROC AUC Score: {avg_val_auc:.4f}\")\n",
    "\n",
    "    # test_loss, test_accuracy = model.evaluate(test_ds, verbose=1)\n",
    "    # print(f\"Fold {fold} Test Metrics: Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_predictions = model.predict(test_ds)\n",
    "    avg_test_accuracy, avg_test_precision, avg_test_recall, avg_test_f1, _ = calculate_metrics(\n",
    "        np.concatenate([y for _, y in test_ds]), test_predictions\n",
    "        )\n",
    "\n",
    "    # Append the fold metrics to the lists\n",
    "    fold_accuracies.append(avg_test_accuracy)\n",
    "    fold_precisions.append(avg_test_precision)\n",
    "    fold_recalls.append(avg_test_recall)\n",
    "    fold_f1s.append(avg_test_f1)\n",
    "\n",
    "    # -------------------- Optional: Evaluation on Test Dataset --------------------\n",
    "    # If this fold produces the best F1 score so far, save the model and visuals\n",
    "    if avg_test_f1 > best_f1_score:\n",
    "        best_f1_score = avg_test_f1\n",
    "        # Save the best model (using model.export for TensorFlow SavedModel format)\n",
    "        model.export(best_f1_dir)\n",
    "        print(f\"Best model updated at Fold {fold} with F1 Score: {best_f1_score:.4f}\")\n",
    "        if config.get('save_metrics', False):\n",
    "            save_best_model_visuals(history_frozen, model, test_ds, class_names, model_subdir, fold)\n",
    "            save_best_model_visuals_binary(history_frozen, model, test_ds, model_subdir, fold, 5)\n",
    "    return best_f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop(config: dict, best_f1_dir: str, model_subdir: str) -> None:\n",
    "    \"\"\"\n",
    "    Main loop that iterates over each fold, training and evaluating the model.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration parameters including hyperparameters and training settings.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "        best_f1_dir (str): Directory to save the best performing model.\n",
    "        model_subdir (str): Subdirectory for saving additional metrics and visuals.\n",
    "        class_names (List[str]): List of class names.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    best_val_f1score = 0.0\n",
    "    total_folds = config.get('folds', 5)  # default to 5 folds if not specified\n",
    "\n",
    "    # Iterate through each fold and update the best F1 score if improved.\n",
    "    for fold in range(1, total_folds + 1):\n",
    "        best_val_f1score = train_fold(fold, config, best_val_f1score,\n",
    "                                      best_f1_dir, model_subdir)\n",
    "    print(f\"\\nTraining complete. Best Validation F1 Score: {best_val_f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Fold 1...\n",
      "Found 7518 files belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739323422.776674  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.776875  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777000  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777114  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777223  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777340  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777563  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777684  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777796  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.777906  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.778016  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.778125  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.778288  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.778402  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1739323422.778514  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-11 21:23:42.778624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n",
      "I0000 00:00:1739323422.778658  317533 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-11 21:23:42.778761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21691 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:82:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 files belonging to 6 classes.\n",
      "Found 74 files belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:23:46.901327: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-02-11 21:23:46.922122: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weight Dictionary: {0: 1.79, 1: 1.8265306122448979, 2: 0.771551724137931, 3: 1.0783132530120483, 4: 2.418918918918919, 5: 0.4430693069306931}\n",
      "Sample count per class in balanced dataset:\n",
      "Class 0: 700 samples\n",
      "Class 1: 686 samples\n",
      "Class 2: 1624 samples\n",
      "Class 3: 1162 samples\n",
      "Class 4: 518 samples\n",
      "Class 5: 2828 samples\n",
      "______________________________________________\n",
      "Training with frozen base layers for 50 epochs on Fold 1...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739323428.978691  317792 service.cc:146] XLA service 0x744d10003000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739323428.978712  317792 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1739323428.978714  317792 service.cc:154]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-02-11 21:23:49.018961: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-11 21:23:49.303217: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2025-02-11 21:23:49.630977: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-02-11 21:23:49.674088: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2025-02-11 21:23:50.104904: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 406.01MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-02-11 21:23:50.122992: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 761.60MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 31/235\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3436 - loss: 1.6241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739323431.177856  317792 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5555 - loss: 1.0872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:23:52.910895: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:23:52.977504: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-02-11 21:23:53.238246: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 715.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5588 - loss: 1.0788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:23:55.923231: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1167', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.5593 - loss: 1.0776 - val_accuracy: 0.7153 - val_loss: 0.9687 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7968 - loss: 0.4794 - val_accuracy: 0.7083 - val_loss: 0.9109 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8448 - loss: 0.3695 - val_accuracy: 0.7153 - val_loss: 0.8886 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8720 - loss: 0.3088 - val_accuracy: 0.7014 - val_loss: 0.8815 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8925 - loss: 0.2680 - val_accuracy: 0.6944 - val_loss: 0.8827 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9079 - loss: 0.2379 - val_accuracy: 0.6944 - val_loss: 0.8888 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9175 - loss: 0.2144 - val_accuracy: 0.6944 - val_loss: 0.8978 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9255 - loss: 0.1955 - val_accuracy: 0.6875 - val_loss: 0.9087 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9348 - loss: 0.1743 - val_accuracy: 0.6736 - val_loss: 0.9377 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9383 - loss: 0.1671 - val_accuracy: 0.6736 - val_loss: 0.9429 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 286ms/step\n",
      "\n",
      "Fold 1 Validation Metrics:\n",
      "  Loss: 0.8815\n",
      "  Accuracy: 0.7014\n",
      "  Precision: 0.6901\n",
      "  Recall: 0.6802\n",
      "  F1 Score: 0.6775\n",
      "  ROC AUC Score: 0.8939\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 708ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:12.351967: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/model1/best_f1score_fold/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/model1/best_f1score_fold/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../saved_models/model1/best_f1score_fold'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_154')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127884223132496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223131920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223132688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223133456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223131728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223133264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223133840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223133072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223134416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223134800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884223135376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212863824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212864208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212863632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212863248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212865744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212866512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212864400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212867472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212867088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212868240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212869200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212866704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212867856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212869008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212870544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212871888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212872080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212871696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212870928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212872464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212873808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212874000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212873616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212872848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212874384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212875728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212875920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212875536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212874768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212873232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212877840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212878032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212877648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212870160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212876688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213109008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213109584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212878416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212878800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213109968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213111312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213111504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213111120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213110352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213111888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213113232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213113424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213113040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213112272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213113808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213115152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213115344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213114960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213114192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213115728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213117072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213117264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213116880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213116112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213117648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213118992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213119184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213118800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213118032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213119568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213120912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213121104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213120720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213119952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213121488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213122832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213123024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213122640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213121872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213123408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213124752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213122256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213124560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213123792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213486608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213487184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213487376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213486992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213486224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213487760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213489104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213489296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213488912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213488144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213489680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213491024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213491216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213490832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213490064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213491600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213492944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213493136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213492752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213491984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213493520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213494864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213495056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213494672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213493904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213495440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213496784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213496976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213496592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213495824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213497360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213498704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213498896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213498512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213497744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213499280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213500624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213500816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213500432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213499664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213498128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202393680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202394448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213501200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213501584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202394832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202396176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202396368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202395984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202395216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202396752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202398096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202398288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202397904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202397136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202398672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202400016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202400208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202399824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202399056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202400592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202401936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202402128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202401744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202400976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202402512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202403856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202404048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202403664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202402896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202404432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202405776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202405968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202405584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202404816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202406352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202407696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202407888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202407504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202406736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202408272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202409616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202407120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202409424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202408656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202770704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202772048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202772240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202771856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202771280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202772624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202773968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202774160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202773776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202773008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202774544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202775888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202776080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202775696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202774928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202776464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202777808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202778000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202777616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202776848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202778384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202779728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202779920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202779536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202778768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202780304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202781648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202781840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202781456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202780688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202782224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202783568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202783760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202783376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202782608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202784144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202785488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202785680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202785296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202784528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202782992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191646352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191645776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202786064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202786448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191646928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191648272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191648464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191648080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191647312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191648848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191650192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191650384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191650000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191649232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191650768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191652112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191652304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191651152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191652688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191654032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191654224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191653840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191653072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191654608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191655952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191656144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191655760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191654992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191656528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191657872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191658064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191657680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191656912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191658448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191659792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191659984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191659600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191658832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191660368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191661712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191659216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191661520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884191660752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884192301520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884192302864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Best model updated at Fold 1 with F1 Score: 0.6917\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Processing Fold 2...\n",
      "Found 7126 files belonging to 6 classes.\n",
      "Found 164 files belonging to 6 classes.\n",
      "Found 82 files belonging to 6 classes.\n",
      "Class Weight Dictionary: {0: 1.6633986928104576, 1: 1.8049645390070923, 2: 0.7782874617737003, 3: 1.0604166666666666, 4: 2.120833333333333, 5: 0.4661172161172161}\n",
      "Sample count per class in balanced dataset:\n",
      "Class 0: 714 samples\n",
      "Class 1: 658 samples\n",
      "Class 2: 1526 samples\n",
      "Class 3: 1120 samples\n",
      "Class 4: 560 samples\n",
      "Class 5: 2548 samples\n",
      "______________________________________________\n",
      "Training with frozen base layers for 50 epochs on Fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:15.336568: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m213/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5409 - loss: 1.1741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:19.379462: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:24:19.433467: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5461 - loss: 1.1605"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:22.188766: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1167', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.5466 - loss: 1.1592 - val_accuracy: 0.6646 - val_loss: 0.9023 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7831 - loss: 0.5255 - val_accuracy: 0.6768 - val_loss: 0.8529 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8358 - loss: 0.4077 - val_accuracy: 0.7012 - val_loss: 0.8328 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8630 - loss: 0.3425 - val_accuracy: 0.7012 - val_loss: 0.8230 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8807 - loss: 0.2985 - val_accuracy: 0.7073 - val_loss: 0.8196 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8957 - loss: 0.2660 - val_accuracy: 0.7073 - val_loss: 0.8201 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9074 - loss: 0.2407 - val_accuracy: 0.7195 - val_loss: 0.8230 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9155 - loss: 0.2201 - val_accuracy: 0.7256 - val_loss: 0.8277 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9206 - loss: 0.2029 - val_accuracy: 0.7256 - val_loss: 0.8336 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9304 - loss: 0.1840 - val_accuracy: 0.7256 - val_loss: 0.8185 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9336 - loss: 0.1751 - val_accuracy: 0.7256 - val_loss: 0.8243 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9382 - loss: 0.1684 - val_accuracy: 0.7256 - val_loss: 0.8300 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.1622 - val_accuracy: 0.7256 - val_loss: 0.8358 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9463 - loss: 0.1564 - val_accuracy: 0.7256 - val_loss: 0.8417 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9514 - loss: 0.1481 - val_accuracy: 0.7195 - val_loss: 0.8581 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.1444 - val_accuracy: 0.7195 - val_loss: 0.8613 - learning_rate: 2.5000e-04\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step\n",
      "\n",
      "Fold 2 Validation Metrics:\n",
      "  Loss: 0.8185\n",
      "  Accuracy: 0.7256\n",
      "  Precision: 0.7350\n",
      "  Recall: 0.7516\n",
      "  F1 Score: 0.7267\n",
      "  ROC AUC Score: 0.9291\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:43.773498: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1150', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:24:43.810354: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1150', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 791ms/step\n",
      "\n",
      "Processing Fold 3...\n",
      "Found 7532 files belonging to 6 classes.\n",
      "Found 153 files belonging to 6 classes.\n",
      "Found 64 files belonging to 6 classes.\n",
      "Class Weight Dictionary: {0: 1.7581699346405228, 1: 1.9078014184397163, 2: 0.7865497076023392, 3: 1.1955555555555555, 4: 2.490740740740741, 5: 0.4170542635658915}\n",
      "Sample count per class in balanced dataset:\n",
      "Class 0: 714 samples\n",
      "Class 1: 658 samples\n",
      "Class 2: 1596 samples\n",
      "Class 3: 1050 samples\n",
      "Class 4: 504 samples\n",
      "Class 5: 3010 samples\n",
      "______________________________________________\n",
      "Training with frozen base layers for 50 epochs on Fold 3...\n",
      "Epoch 1/50\n",
      "\u001b[1m234/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5477 - loss: 1.1017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:49.717995: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4046', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5488 - loss: 1.0992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:24:52.367194: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1167', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:24:52.401917: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1167', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2025-02-11 21:24:52.632407: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 598.51MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.5493 - loss: 1.0980 - val_accuracy: 0.6013 - val_loss: 1.0294 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8074 - loss: 0.4691 - val_accuracy: 0.6471 - val_loss: 1.0507 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8482 - loss: 0.3606 - val_accuracy: 0.6405 - val_loss: 1.0889 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8712 - loss: 0.3016 - val_accuracy: 0.6275 - val_loss: 1.1259 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8894 - loss: 0.2621 - val_accuracy: 0.6275 - val_loss: 1.1610 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8989 - loss: 0.2311 - val_accuracy: 0.6209 - val_loss: 1.1722 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9107 - loss: 0.2168 - val_accuracy: 0.6209 - val_loss: 1.1890 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 257ms/step\n",
      "\n",
      "Fold 3 Validation Metrics:\n",
      "  Loss: 1.0294\n",
      "  Accuracy: 0.6013\n",
      "  Precision: 0.5851\n",
      "  Recall: 0.6270\n",
      "  F1 Score: 0.5972\n",
      "  ROC AUC Score: 0.8886\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "INFO:tensorflow:Assets written to: ../saved_models/model1/best_f1score_fold/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/model1/best_f1score_fold/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../saved_models/model1/best_f1score_fold'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_470')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127883805784784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805783440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805783248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805777104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805782096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805784016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805783824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805785168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805783632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805784592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805784976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658789136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658788944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805769808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658789904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658790288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658791632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658791824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658791440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658790672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658792208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658793552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658793744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658793360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658792592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658794128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658795472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658795664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658795280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658794512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658796048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658797392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658797584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658797200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658796432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658797968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658799312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658799504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658799120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658798352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658799888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658801232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658801424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658801040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658800272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658801808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658803152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658803344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658802960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658802192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658803728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658805072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658804880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658804688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883658802576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826889168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816960656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816961040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826890320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816961808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816962576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816967568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816967952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816966416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816964112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816966800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816971408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816971792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816970256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816969488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816970640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816975248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816975632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816974096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816973328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816974480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805770960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805770384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816960464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816974288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805772880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805775184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805776336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805773648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805773264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805771536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805780176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805778256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805780560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805779024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805776528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827266192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827280592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805782480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805773456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827280784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827278288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827273104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827277904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827280208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827276752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827270608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827265808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827270224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827276368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827270992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827272528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827267344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827272144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827274064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827268304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827267152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827659216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827281744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827269264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827663632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827669008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827662480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827665168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827670160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827659408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827668240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827668624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827670928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827666320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827667472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827672080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827672464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827667088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827660944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827671312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827673040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827659792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827674768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827674000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827671120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816528080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816528464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816532880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816525200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816527312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816517712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816529040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816533840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816532304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816532688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816524624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816519440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816524240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816526160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816518864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816520400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816520784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816521168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816522320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816530000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858973456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858972304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816521360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858973840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858974224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858975760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858976528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858971536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858975376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858979792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858980944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858979216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858978064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858977680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858970384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234472720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234473104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858979408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858969616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234471376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234469264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234468496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234469648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234473296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234463696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234465424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234464656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234465808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234467344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234459856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234461968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234458896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234457168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234463504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234457936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234460048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213110544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234472336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234462736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213111696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213109200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213112848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213114000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213112080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213115536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213112656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213116688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213117840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213110736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213119376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213116496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213120528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213121680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213114576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213123216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213120336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213120144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213108816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213118416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213124368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213485840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213488720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213489872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213486800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213491408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213488528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213492560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213493712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213486416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213495248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213492368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213496400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213497552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213490448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213499088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213496208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213500240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213501392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213494288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213487952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202396944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202395408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213496016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202395792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202398480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202395600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202399632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202400784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202394064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202402320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202399440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202403472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202404624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202397520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202406160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202403280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202407312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202408464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202401360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202409808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212870736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212866896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202394640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202405200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212869776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212874192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Best model updated at Fold 3 with F1 Score: 0.7216\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Processing Fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:25:05.461189: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7336 files belonging to 6 classes.\n",
      "Found 150 files belonging to 6 classes.\n",
      "Found 81 files belonging to 6 classes.\n",
      "Class Weight Dictionary: {0: 1.6794871794871795, 1: 1.9848484848484849, 2: 0.7594202898550725, 3: 1.0522088353413654, 4: 2.425925925925926, 5: 0.45017182130584193}\n",
      "Sample count per class in balanced dataset:\n",
      "Class 0: 728 samples\n",
      "Class 1: 616 samples\n",
      "Class 2: 1610 samples\n",
      "Class 3: 1162 samples\n",
      "Class 4: 504 samples\n",
      "Class 5: 2716 samples\n",
      "______________________________________________\n",
      "Training with frozen base layers for 50 epochs on Fold 4...\n",
      "Epoch 1/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5376 - loss: 1.1522 - val_accuracy: 0.5667 - val_loss: 1.1119 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7783 - loss: 0.5220 - val_accuracy: 0.6267 - val_loss: 0.9766 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8273 - loss: 0.4063 - val_accuracy: 0.6733 - val_loss: 0.9196 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8544 - loss: 0.3419 - val_accuracy: 0.6667 - val_loss: 0.8939 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8724 - loss: 0.2981 - val_accuracy: 0.6600 - val_loss: 0.8846 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8857 - loss: 0.2654 - val_accuracy: 0.6733 - val_loss: 0.8841 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8972 - loss: 0.2396 - val_accuracy: 0.6667 - val_loss: 0.8885 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9072 - loss: 0.2185 - val_accuracy: 0.6733 - val_loss: 0.8957 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9176 - loss: 0.2009 - val_accuracy: 0.6667 - val_loss: 0.9050 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9241 - loss: 0.1858 - val_accuracy: 0.6733 - val_loss: 0.9157 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9304 - loss: 0.1692 - val_accuracy: 0.6933 - val_loss: 0.8974 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9346 - loss: 0.1617 - val_accuracy: 0.6933 - val_loss: 0.9035 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 258ms/step\n",
      "\n",
      "Fold 4 Validation Metrics:\n",
      "  Loss: 0.8841\n",
      "  Accuracy: 0.6733\n",
      "  Precision: 0.6547\n",
      "  Recall: 0.7230\n",
      "  F1 Score: 0.6729\n",
      "  ROC AUC Score: 0.9135\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:25:29.574221: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1150', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:25:29.626032: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1150', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 774ms/step\n",
      "\n",
      "Processing Fold 5...\n",
      "Found 7532 files belonging to 6 classes.\n",
      "Found 154 files belonging to 6 classes.\n",
      "Found 63 files belonging to 6 classes.\n",
      "Class Weight Dictionary: {0: 1.6918238993710693, 1: 2.0852713178294575, 2: 0.8226299694189603, 3: 1.0803212851405624, 4: 2.3596491228070176, 5: 0.4229559748427673}\n",
      "Sample count per class in balanced dataset:\n",
      "Class 0: 742 samples\n",
      "Class 1: 602 samples\n",
      "Class 2: 1526 samples\n",
      "Class 3: 1162 samples\n",
      "Class 4: 532 samples\n",
      "Class 5: 2968 samples\n",
      "______________________________________________\n",
      "Training with frozen base layers for 50 epochs on Fold 5...\n",
      "Epoch 1/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5266 - loss: 1.1752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:25:36.888245: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1167', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:25:36.926439: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1167', 224 bytes spill stores, 224 bytes spill loads\n",
      "\n",
      "2025-02-11 21:25:37.165816: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 621.81MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5271 - loss: 1.1739 - val_accuracy: 0.7273 - val_loss: 0.7668 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7842 - loss: 0.5238 - val_accuracy: 0.7338 - val_loss: 0.7260 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8298 - loss: 0.4050 - val_accuracy: 0.7403 - val_loss: 0.7103 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8568 - loss: 0.3396 - val_accuracy: 0.7532 - val_loss: 0.7042 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8727 - loss: 0.2953 - val_accuracy: 0.7792 - val_loss: 0.7037 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8891 - loss: 0.2625 - val_accuracy: 0.7857 - val_loss: 0.7071 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9005 - loss: 0.2370 - val_accuracy: 0.7792 - val_loss: 0.7131 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9122 - loss: 0.2164 - val_accuracy: 0.7597 - val_loss: 0.7212 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9200 - loss: 0.1993 - val_accuracy: 0.7597 - val_loss: 0.7307 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9292 - loss: 0.1791 - val_accuracy: 0.7403 - val_loss: 0.7760 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9324 - loss: 0.1707 - val_accuracy: 0.7468 - val_loss: 0.7799 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 261ms/step\n",
      "\n",
      "Fold 5 Validation Metrics:\n",
      "  Loss: 0.7037\n",
      "  Accuracy: 0.7792\n",
      "  Precision: 0.7679\n",
      "  Recall: 0.8136\n",
      "  F1 Score: 0.7841\n",
      "  ROC AUC Score: 0.9382\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 21:25:53.482737: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1150', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-11 21:25:53.506370: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1150', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-02-11 21:25:53.751500: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 393.82MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-02-11 21:25:53.768439: W external/local_tsl/tsl/framework/bfc_allocator.cc:291] Allocator (GPU_0_bfc) ran out of memory trying to allocate 738.30MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step \n",
      "INFO:tensorflow:Assets written to: ../saved_models/model1/best_f1score_fold/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saved_models/model1/best_f1score_fold/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '../saved_models/model1/best_f1score_fold'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_786')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127883826894160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826895504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826895888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826895696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826894928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826896272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826897424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826897616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826897808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826897232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826898192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826899728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826901264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826899536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826899152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826900112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826902608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826901456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826901072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826902224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826902800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826890128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826889360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826888784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883826900880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816962960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816972176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816964496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816966608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816968336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816973712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816964688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816964880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816968528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816976016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805784400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805785744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805782864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805785360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805784208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805785552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805772496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805770192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805775568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805783056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805776912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805780944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805774800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805781328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883805774416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827275024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827276944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827274832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827269072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827280976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827271376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827275216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827271184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827272912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827279056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827666704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827664784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827665552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827660560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827664400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827667280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827661520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827663248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827670544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827669392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883827673232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816530384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816526928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816525008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816530768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816523472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816531152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816521552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816523088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816528848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883816523280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858977296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858981136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858976144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858972688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858970000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858980368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858969808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858980176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883858977488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234469456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234463888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234467536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234465040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234467728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234461584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234461200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234465616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234459664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234458128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884234466960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213119760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213110160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213118224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213109776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213118608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213109392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213116304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213124176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213123600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213110928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213495632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213488336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213494096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213485648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213494480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213486032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213492176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213501776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213499472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213490256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202401168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202396560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884213489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202395024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202401552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202393872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202399248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202408848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202406544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202402704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212878224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212872656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202409232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202403088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212865360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212877456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212871120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212869584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212878608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884212868432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526032464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526032848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526032656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526029584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526033232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526034576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526034768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526034384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526033616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526035152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526036496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526036688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526036304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526035536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526037072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526038416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526038608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526038224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526037456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526038992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526040336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526040528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526040144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526039376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526040912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526042256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526042448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526042064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526041296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526042832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526044176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526044368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526043984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526043216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526044752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202328336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202329104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526030928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127883526045136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202328912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202330448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202330640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202330256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202329488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202331024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202332368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202332560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202332176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202331408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202332944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202334288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202334480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202334096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202333328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202334864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202336208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202336400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202336016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202335248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202336784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202338128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202338320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202337936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202337168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202338704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202340048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202340240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202339856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202339088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202340624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202341968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202342160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202341776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202341008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202342544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202343888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202341392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202343696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202342928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884202344080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203099536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203099728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203099344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203098192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203100112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203101456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203101648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203101264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203100496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203102032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203103376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203103568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203103184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203102416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203103952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203105296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203105488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203105104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203104336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203105872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203107216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203107408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203107024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203106256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203107792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203109136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203109328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203108944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203108176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203109712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203111056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203111248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203110864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203110096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203111632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203112976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203113168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203112784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203112016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203114320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127884203098960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Best model updated at Fold 5 with F1 Score: 0.8066\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training complete. Best Validation F1 Score: 0.8066\n",
      "\n",
      "Overall Cross-Validation Metrics:\n",
      "Accuracy: 72.87 ± 7.01\n",
      "Precision: 72.38 ± 5.63\n",
      "Recall: 74.06 ± 4.56\n",
      "F1 Score: 71.66 ± 5.39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(configs):\n",
    "    # Define the base path for saving models\n",
    "    model_subdir = os.path.join(save_dir, f'model{i + 1}')\n",
    "    os.makedirs(model_subdir, exist_ok=True)\n",
    "\n",
    "    # Define the base path for saving cthe model with the best f1-score\n",
    "    best_f1_dir = os.path.join(model_subdir, 'best_f1score_fold')\n",
    "    os.makedirs(best_f1_dir, exist_ok=True)\n",
    "\n",
    "    # Define the base path for saving checkpoints for model\n",
    "    checkpoint_folder = os.path.join(model_subdir, 'checkpoints')\n",
    "    os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "    \n",
    "    config['checkpoint_folder'] = checkpoint_folder\n",
    "\n",
    "    main_training_loop(configs[i], best_f1_dir, model_subdir)\n",
    "\n",
    "    output_file_path = os.path.join(model_subdir, \"cv_metrics.txt\")\n",
    "    compute_cv_statistics(fold_accuracies, fold_precisions, fold_recalls, fold_f1s,\n",
    "                        use_std_error=False, output_filepath=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(fold_dirs: list):\n",
    "    \"\"\"\n",
    "    Prints and plots the class distribution for training, validation, and test sets across folds.\n",
    "    \n",
    "    Args:\n",
    "        fold_dirs (list): List of directory paths for each fold.\n",
    "    \"\"\"\n",
    "    for fold, fold_dir in enumerate(fold_dirs, 1):\n",
    "        for split in ['Train', 'Valid', 'Test']:\n",
    "            split_dir = os.path.join(fold_dir, split)\n",
    "            dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                split_dir,\n",
    "                labels='inferred',\n",
    "                label_mode='int',\n",
    "                batch_size=32,\n",
    "                image_size=(224, 224),\n",
    "                shuffle=False\n",
    "            )\n",
    "            labels = np.concatenate([y.numpy() for _, y in dataset], axis=0)\n",
    "            unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "            \n",
    "            print(f\"Fold {fold} - {split} Set Class Distribution:\")\n",
    "            for cls, count in zip(unique_classes, class_counts):\n",
    "                print(f\"Class {cls}: {count} samples\")\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.bar(unique_classes, class_counts)\n",
    "            plt.xlabel(\"Class\")\n",
    "            plt.ylabel(\"Number of Samples\")\n",
    "            plt.title(f\"Fold {fold} - {split} Set Class Distribution\")\n",
    "            plt.show()\n",
    "\n",
    "print_class_distribution([\"../data/archive/Original Images/Original Images/FOLDS/fold1\",\n",
    "                        #   \"../data/archive/Original Images/Original Images/FOLDS/fold2\",\n",
    "                        #   \"../data/archive/Original Images/Original Images/FOLDS/fold3\",\n",
    "                        #   \"../data/archive/Original Images/Original Images/FOLDS/fold4\",\n",
    "                        #   \"../data/archive/Original Images/Original Images/FOLDS/fold5\"\n",
    "                        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
