{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b205aa9-a98b-4183-96f6-4cb9ce7ce94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78adc05e-67c2-4a1a-8473-8b51282d5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ONNX_MODEL_PATH = \"../converted_models/ONNX/fp16_quantized_model.onnx\"  # Path to your ONNX model\n",
    "TRT_ENGINE_PATH = \"../converted_models/ONNX/fp16_quantized_model.trt\"   # Output TensorRT engine path\n",
    "MAX_BATCH_SIZE = 32              # Set max batch size\n",
    "MODE = \"fp16\"                # Enable FP16 for optimization (if supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad8b3f18-e317-45bf-9908-d0184806bc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_default_keras_tensor_312:0 dim {\n",
      "  dim_param: \"unk__751\"\n",
      "}\n",
      "dim {\n",
      "  dim_value: 224\n",
      "}\n",
      "dim {\n",
      "  dim_value: 224\n",
      "}\n",
      "dim {\n",
      "  dim_value: 3\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(ONNX_MODEL_PATH)\n",
    "for input_tensor in model.graph.input:\n",
    "    print(input_tensor.name, input_tensor.type.tensor_type.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8642fe60-5194-4510-85aa-d4d698541ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input: serving_default_keras_tensor_312:0, shape: (-1, 224, 224, 3), dtype: DataType.FLOAT\n",
      "Model output: StatefulPartitionedCall_1:0, shape: (-1, 1), dtype: DataType.FLOAT\n",
      "Building TensorRT engine. This may take a while...\n",
      "[12/07/2024-14:21:15] [TRT] [W] DLA requests all profiles have same min, max, and opt value. All dla layers are falling back to GPU\n",
      "TensorRT engine saved to: ../converted_models/ONNX/fp16_quantized_model.trt\n"
     ]
    }
   ],
   "source": [
    "def build_engine(onnx_file_path, engine_file_path, max_batch_size=1, mode=\"fp16\"):\n",
    "    \"\"\"Converts ONNX model to TensorRT engine.\"\"\"\n",
    "    logger = trt.Logger(trt.Logger.WARNING)\n",
    "    builder = trt.Builder(logger)\n",
    "    \n",
    "    # Configure the builder\n",
    "    config = builder.create_builder_config()\n",
    "    \n",
    "    # Set cache\n",
    "    cache = config.create_timing_cache(b\"\")\n",
    "    config.set_timing_cache(cache, ignore_mismatch=False)\n",
    "    \n",
    "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    network = builder.create_network(flag)\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "    # Read ONNX file\n",
    "    if not os.path.exists(onnx_file_path):\n",
    "        raise FileNotFoundError(f\"ONNX file not found: {onnx_file_path}\")\n",
    "    with open(onnx_file_path, \"rb\") as model_file:\n",
    "        if not parser.parse(model_file.read()):\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            raise RuntimeError(f\"Failed to parse ONNX file: {onnx_file_path}\")\n",
    "\n",
    "    # Inspect inputs and outputs\n",
    "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
    "\n",
    "    for input_tensor in inputs:\n",
    "        print(f\"Model input: {input_tensor.name}, shape: {input_tensor.shape}, dtype: {input_tensor.dtype}\")\n",
    "    for output_tensor in outputs:\n",
    "        print(f\"Model output: {output_tensor.name}, shape: {output_tensor.shape}, dtype: {output_tensor.dtype}\")\n",
    "\n",
    "    # Define optimization profile for dynamic input shapes (NHWC format)\n",
    "    input_tensor = network.get_input(0)  # Assuming single input model\n",
    "    profile = builder.create_optimization_profile()\n",
    "\n",
    "    # Define shapes for NHWC format (Batch size dynamic)\n",
    "    min_shape = (1, 224, 224, 3)         # Minimum input size (batch_size=1)\n",
    "    opt_shape = (max_batch_size // 2, 224, 224, 3)  # Optimum input size\n",
    "    max_shape = (max_batch_size, 224, 224, 3)       # Maximum input size\n",
    "    profile.set_shape(input_tensor.name, min=min_shape, opt=opt_shape, max=max_shape)\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "    # Set precision mode\n",
    "    if mode == \"fp16\":\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "    elif mode == \"int8\":\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Precision mode {mode} not supported\")\n",
    "\n",
    "    # Build engine\n",
    "    print(\"Building TensorRT engine. This may take a while...\")\n",
    "    serialized_engine = builder.build_serialized_network(network, config)\n",
    "    if serialized_engine is None:\n",
    "        raise RuntimeError(\"Failed to build TensorRT engine\")\n",
    "\n",
    "    # Save the serialized engine to a file\n",
    "    with open(engine_file_path, \"wb\") as engine_file:\n",
    "        engine_file.write(serialized_engine)\n",
    "    print(f\"TensorRT engine saved to: {engine_file_path}\")\n",
    "\n",
    "\n",
    "# Convert ONNX to TensorRT\n",
    "build_engine(ONNX_MODEL_PATH, TRT_ENGINE_PATH, MAX_BATCH_SIZE, mode=\"fp16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578079cc-3215-49d0-a7b0-4490bb9ef159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
