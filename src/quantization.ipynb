{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../converted_models/ONNX\", exist_ok=True)\n",
    "os.makedirs(\"../converted_models/TFLITE\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Convert the TensorFlow SavedModel directly to ONNX\n",
    "# saved_model_dir = \"../best_model/model1/best_f1score_fold\"\n",
    "# tflife_model_dir = \"../converted_models/TFLITE/fp16_quantized_model.tflite\"\n",
    "# onnx_model_path = \"../converted_models/ONNX/fp16_converted_model.onnx\"\n",
    "\n",
    "# # Convert the model\n",
    "# # !python -m tflite2onnx.convert --saved-model {tflife_model_dir} --output {onnx_model_path} --opset 13\n",
    "# !python -m tf2onnx.convert --tflite {tflife_model_dir} --output {onnx_model_path} --opset 13\n",
    "\n",
    "# print(f\"ONNX model saved at: {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3192 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "def convert_savedmodel_to_onnx(saved_model_dir: str):\n",
    "    os.makedirs(\"../converted_models\", exist_ok=True)\n",
    "    onnx_model_dir = \"../converted_models/converted_model.onnx\"\n",
    "    \n",
    "    tflite2onnx_command = [\n",
    "        \"python\", \"-m\", \"tf2onnx.convert\",\n",
    "        \"--saved-model\", saved_model_dir,\n",
    "        \"--output\", onnx_model_dir,\n",
    "        \"--opset\", \"13\"\n",
    "    ]\n",
    "\n",
    "    print(tflite2onnx_command)\n",
    "\n",
    "    # Run the command\n",
    "    try:\n",
    "        subprocess.run(tflite2onnx_command, check=True)\n",
    "        print(f\"ONNX model successfully saved at: {onnx_model_dir}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred during ONNX conversion: {e}\")\n",
    "        raise\n",
    "\n",
    "def convert_savedmodel_to_tflite(\n",
    "        saved_model_dir: str,\n",
    "        representative_dataset_gen=None,\n",
    "        quantization_type: str = \"dynamic\",\n",
    "        tflite_model_path: str = \"quantized_model.tflite\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Converts a TensorFlow SavedModel to a quantized TensorFlow Lite model, and then to ONNX format using CLI tools.\n",
    "    \n",
    "    Parameters:\n",
    "        saved_model_dir (str): Path to the TensorFlow SavedModel directory.\n",
    "        representative_dataset_gen (callable): A generator function providing representative data for calibration.\n",
    "                                               Required for integer quantization.\n",
    "        quantization_type (str): Type of post-training quantization. Options: \"dynamic\", \"integer\", \"fp16\".\n",
    "        tflite_model_path (str): Path to save the quantized TFLite model.\n",
    "    \"\"\"\n",
    "    # Load the TensorFlow model\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "    # Apply the selected quantization method\n",
    "    if quantization_type == \"dynamic\":\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Dynamic range quantization\n",
    "    elif quantization_type == \"fp16\":\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    elif quantization_type == \"integer\":\n",
    "        if representative_dataset_gen is None:\n",
    "            raise ValueError(\"A representative dataset generator must be provided for integer quantization.\")\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported quantization type: {quantization_type}\")\n",
    "\n",
    "    # Convert the model to TensorFlow Lite format\n",
    "    tflite_model = converter.convert()\n",
    "    print(f\"{quantization_type} quantization complete!\")\n",
    "\n",
    "    # Save the TFLite model\n",
    "    tflite_model_path = f\"../converted_models/TFLITE/{quantization_type}_quantized_model.tflite\"\n",
    "    with open(tflite_model_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(f\"Quantized TFLite model saved at: {tflite_model_path}\")\n",
    "\n",
    "def convert_tflite_to_onnx(\n",
    "    tflite_model_path: str,\n",
    "    quantization_type: str = \"dynamic\",\n",
    "    onnx_model_path: str = \"converted_model.onnx\",\n",
    "    opset: str = \"13\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Converts a TensorFlow Lite model to ONNX format using CLI tools.\n",
    "    \n",
    "    Parameters:\n",
    "        tflite_model_path (str): Path to save the quantized TFLite model.\n",
    "        quantization_type (str): Type of post-training quantization. Options: \"dynamic\", \"integer\", \"fp16\".\n",
    "        onnx_model_path (str): Path to save the converted ONNX model.\n",
    "        opset: TODO\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert TFLite to ONNX using command-line tool\n",
    "    # Construct the command\n",
    "    onnx_model_path = f\"../converted_models/ONNX/{quantization_type}_quantized_model.onnx\"\n",
    "    # python_env = sys.executable  # This gets the same Python interpreter used by the script\n",
    "    tflite2onnx_command = [\n",
    "        \"python\", \"-m\", \"tf2onnx.convert\",\n",
    "        \"--tflite\", tflite_model_path,\n",
    "        \"--output\", onnx_model_path,\n",
    "        \"--opset\", \"13\"\n",
    "    ]\n",
    "\n",
    "    print(tflite2onnx_command)\n",
    "\n",
    "    # Run the command\n",
    "    try:\n",
    "        result = subprocess.run(tflite2onnx_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"ONNX model successfully saved at: {onnx_model_path}\")\n",
    "        print(\"Output:\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred during ONNX conversion: {e}\")\n",
    "        print(\"stderr:\", e.stderr)\n",
    "        raise\n",
    "\n",
    "# representative dataset generator (for integer quantization)\n",
    "def create_data_generator(data_dir, batch_size=32, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Creates a data generator that yields batches of data for quantization.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the data directory containing 'MonkeyPox' and 'Others' subdirectories.\n",
    "        batch_size (int): The size of the batches to return. Default is 32.\n",
    "        img_size (tuple): Tuple representing the target image size (height, width). Default is (224, 224).\n",
    "\n",
    "    Returns:\n",
    "        generator: A Python generator that yields batches of image data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an ImageDataGenerator for preprocessing\n",
    "    datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1]\n",
    "    \n",
    "    # Create a flow from the directory\n",
    "    # The target_size should match the input size of your model (e.g., 224x224 for many pre-trained models)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=img_size,  # Resize all images to this size\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,  # We don't need the labels for quantization\n",
    "        # shuffle=True,  # Shuffle the images\n",
    "        seed=42,  # Set a seed for reproducibility\n",
    "    )\n",
    "    \n",
    "    return generator\n",
    "\n",
    "generator = create_data_generator(\"../data/Augmented_Images\")\n",
    "# Define a representative dataset function for quantization\n",
    "def representative_dataset_gen():\n",
    "    \"\"\"\n",
    "    A generator function that yields batches of data for the INT8 calibration.\n",
    "    \"\"\"\n",
    "    for batch in generator:\n",
    "        # Yield a batch of images from the generator as a numpy array\n",
    "        yield [batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer quantization complete!\n"
     ]
    }
   ],
   "source": [
    "# convert_savedmodel_to_onnx(\"../best_model/model1/best_f1score_fold\")\n",
    "quantization_types = [\"integer\"]\n",
    "# quantization_types = [\"dynamic\", \"fp16\"]\n",
    "# quantization_types = [\"dynamic\", \"fp16\", \"integer\"]\n",
    "saved_model_dir = \"../best_model/model1/best_f1score_fold\"\n",
    "\n",
    "for quant_type in quantization_types:\n",
    "    convert_savedmodel_to_tflite(saved_model_dir, quantization_type=quant_type, representative_dataset_gen=representative_dataset_gen)\n",
    "    tltite_model_dir = f\"../converted_models/TFLITE/{quant_type}_quantized_model.tflite\"\n",
    "    # convert_tflite_to_onnx(tltite_model_dir, quantization_type=quant_type)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
